{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kbojja\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "import keras\n",
    "\n",
    "# import postprocessing as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement attention_keras (from versions: none)\n",
      "ERROR: No matching distribution found for attention_keras\n"
     ]
    }
   ],
   "source": [
    "pip install attention_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import layers\n",
    "# from tensorflow.keras.layers import Layer\n",
    "from attention import AttentionLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open('sample-1M.jsonl') as f:\n",
    "#     d = json.load(f)\n",
    "#     print(d)\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.read_json('sample-1M.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\kbojja\\anaconda3\\lib\\site-packages (20200104)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\kbojja\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.1.0)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in c:\\users\\kbojja\\anaconda3\\lib\\site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\kbojja\\anaconda3\\lib\\site-packages (from pdfminer.six) (3.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Available online at www.sciencedirect.com\\nAvailable online at www.sciencedirect.com\\n\\nProcedia Computer Science 00 (2019) 000–000\\nProcedia Computer Science 00 (2019) 000–000\\n\\nProcedia Computer Science 147 (2019) 400–406\\n\\nwww.elsevier.com/locate/procedia\\nwww.elsevier.com/locate/procedia\\n\\n2018 International Conference on Identiﬁcation, Information and Knowledge\\n2018 International Conference on Identiﬁcation, Information and Knowledge\\n\\nin the Internet of Things, IIKI 2018\\nin the Internet of Things, IIKI 2018\\n\\nStock Market Prediction Based on Generative Adversarial Network\\nStock Market Prediction Based on Generative Adversarial Network\\n\\nKang Zhanga, Guoqiang Zhonga,∗, Junyu Donga, Shengke Wanga, Yong Wanga\\nKang Zhanga, Guoqiang Zhonga,∗, Junyu Donga, Shengke Wanga, Yong Wanga\\n\\naDepartment of Computer Science and Technology, Ocean University of China, Qingdao, 266100, China\\naDepartment of Computer Science and Technology, Ocean University of China, Qingdao, 266100, China\\n\\nAbstract\\nAbstract\\nDeep learning has recently achieved great success in many areas due to its strong capacity in data process. For instance, it has been\\nDeep learning has recently achieved great success in many areas due to its strong capacity in data process. For instance, it has been\\nwidely used in ﬁnancial areas such as stock market prediction, portfolio optimization, ﬁnancial information processing and trade\\nwidely used in ﬁnancial areas such as stock market prediction, portfolio optimization, ﬁnancial information processing and trade\\nexecution strategies. Stock market prediction is one of the most popular and valuable area in ﬁnance. In this paper, we propose a\\nexecution strategies. Stock market prediction is one of the most popular and valuable area in ﬁnance. In this paper, we propose a\\nnovel architecture of Generative Adversarial Network (GAN) with the Multi-Layer Perceptron (MLP) as the discriminator and the\\nnovel architecture of Generative Adversarial Network (GAN) with the Multi-Layer Perceptron (MLP) as the discriminator and the\\nLong Short-Term Memory (LSTM) as the generator for forecasting the closing price of stocks. The generator is built by LSTM\\nLong Short-Term Memory (LSTM) as the generator for forecasting the closing price of stocks. The generator is built by LSTM\\nto mine the data distributions of stocks from given data in stock market and generate data in the same distributions, whereas the\\nto mine the data distributions of stocks from given data in stock market and generate data in the same distributions, whereas the\\ndiscriminator designed by MLP aims to discriminate the real stock data and generated data. We choose the daily data on S&P 500\\ndiscriminator designed by MLP aims to discriminate the real stock data and generated data. We choose the daily data on S&P 500\\nIndex and several stocks in a wide range of trading days and try to predict the daily closing price. Experimental results show that\\nIndex and several stocks in a wide range of trading days and try to predict the daily closing price. Experimental results show that\\nour novel GAN can get a promising performance in the closing price prediction on the real data compared with other models in\\nour novel GAN can get a promising performance in the closing price prediction on the real data compared with other models in\\nmachine learning and deep learning.\\nmachine learning and deep learning.\\nc(cid:30) 2019 The Authors. Published by Elsevier B.V.\\n© 2019 The Authors. Published by Elsevier B.V.\\nc(cid:30) 2019 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nPeer-review under responsibility of the scientiﬁc committee of the 2018 International Conference on Identiﬁcation, Information\\nPeer-review under responsibility of the scientific committee of the 2018 International Conference on Identification, Information and \\nPeer-review under responsibility of the scientiﬁc committee of the 2018 International Conference on Identiﬁcation, Information\\nand Knowledge in the Internet of Things.\\nKnowledge in the Internet of Things.\\nand Knowledge in the Internet of Things.\\nKeywords: Deep Learning; Stock Prediction; Generative Adversarial Networks; Data Mining.\\nKeywords: Deep Learning; Stock Prediction; Generative Adversarial Networks; Data Mining.\\n\\n1. Introduction\\n1. Introduction\\n\\nThe prediction of stock market returns is one of the most important and challenging issues in this domain. Many\\nThe prediction of stock market returns is one of the most important and challenging issues in this domain. Many\\nanalyses and assumptions in ﬁnancial area show that stock market is predictable. Technical analysis in stock invest-\\nanalyses and assumptions in ﬁnancial area show that stock market is predictable. Technical analysis in stock invest-\\nment theory is an analysis methodology for forecasting the direction of prices through the research on past market\\nment theory is an analysis methodology for forecasting the direction of prices through the research on past market\\ndata. A meaningful assumption named Mean Reversion states that the stock price is temporary and tends to move to\\ndata. A meaningful assumption named Mean Reversion states that the stock price is temporary and tends to move to\\nthe average price over time. Moreover, this assumption has a further development called Moving Average Reversion\\nthe average price over time. Moreover, this assumption has a further development called Moving Average Reversion\\n(MAR), which supposes that the average of price is the mean of price in a past window of time, e.g. ﬁve days [7].\\n(MAR), which supposes that the average of price is the mean of price in a past window of time, e.g. ﬁve days [7].\\nBased on the views mentioned above, we propose a new deep learning model to forecast the daily closing price.\\nBased on the views mentioned above, we propose a new deep learning model to forecast the daily closing price.\\n\\nThe main contributions of this paper can be summarized in the followings:\\nThe main contributions of this paper can be summarized in the followings:\\n\\n∗ Guoqiang Zhong Tel.: +86-159-0639-8365.\\n∗ Guoqiang Zhong Tel.: +86-159-0639-8365.\\n\\nE-mail address: gqzhong@ouc.edu.cn\\nE-mail address: gqzhong@ouc.edu.cn\\n\\n1877-0509 c(cid:30) 2019 The Authors. Published by Elsevier B.V.\\n1877-0509 © 2019 The Authors. Published by Elsevier B.V.\\n1877-0509 c(cid:30) 2019 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nPeer-review under responsibility of the scientiﬁc committee of the 2018 International Conference on Identiﬁcation, Information and Knowledge in\\nPeer-review under responsibility of the scientific committee of the 2018 International Conference on Identification, Information and \\nPeer-review under responsibility of the scientiﬁc committee of the 2018 International Conference on Identiﬁcation, Information and Knowledge in\\nthe Internet of Things.\\nKnowledge in the Internet of Things.\\nthe Internet of Things.\\n10.1016/j.procs.2019.01.256\\n\\nScienceDirectAvailable online at www.sciencedirect.com\\x0c \\n2\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\n401\\n\\n• A novel Generative Adversarial Network (GAN) architecture with Long-Short Term Memory (LSTM) network\\nas the generator and Multi-Layer Perceptron (MLP) as the discriminator is proposed. The model trained in and\\nend-to-end way to predict the daily closing price by giving the stock data in several past days.\\n\\n• We try to generate the same distributions of the stock daily data through the adversarial learning system, instead\\n\\nof only utilizing traditional regression methods for the price forecasting.\\n\\n2. Related Work\\n\\nThe stock market prediction can be seen as a time series forecasting issue and one of the classical algorithms is\\nthe Autoregressive Integrated Moving Average (ARIMA) [2]. ARIMA performs well in linear and stationary time\\nseries, but it doesnt perform well on the nonlinear and non-stationary data in stock market. In order to solve this\\nproblem, one approach [9] combines ARIMA with SVM. The idea is that the forecasting is constituted by a linear\\npart and a nonlinear part, so that they can predict the linear part with ARIMA and the nonlinear part with SVM.\\nMoreover, another approach [6] combines the wavelet basis with SVM, which decomposes the stock data with wavelet\\ntransformation and uses SVM for forecasting. Subsequently, the Artiﬁcial Neural Network (ANN) were combined\\nwith ARIMA to predict the nonlinear part of the stock price data [1]. The hybrid of wavelet transformation and ANN\\ndemonstrated that eﬀective features should be extracted for the training of ANN [3]. Convolutional Neural Network\\n(CNN) was also used in forecasting stock prices from the limit order book [12]. The number of orders and the price of\\n10 bid/ask orders were transformed into a 2D array. In addition, some designed RNNs had been applied to forecasting\\nthe stock data [11] [10]. News and events in ﬁnancial area were extracted and represented as dense vectors to realize\\nstock prediction [4]. Besides, reinforcement learning is another popular method to improve the trading strategies\\nthrough fusing Q-learning and dynamic programming [8].\\n\\n3. Our Methodology\\n\\n3.1. Principle\\n\\n3.2. The Generator\\n\\nGAN is a new framework which trains two models like a zero-sum game [5]. In the adversarial process, the\\ngenerator can be seen as a cheater to generate the similar data as the real data, while the discriminator plays the role\\nof judge to distinguish the real data and generated data. They can reach an ideal point that the discriminator is unable\\nto diﬀerentiate the two types of data. At this point, the generator can capture the data distributions from this game.\\nBased on this principle, we propose our GAN architecture for the prediction of stock closing price.\\n\\nThe generator of our model is designed by LSTM with its strong ability in processing time series data. We choose\\nthe daily data in the last 20 years with 7 ﬁnancial factors to predict the future closing price. The 7 factors of the stock\\ndata in one day are High Price, Low Price, Open Price, Close Price, Volume, Turnover Rate and Ma5 (the average of\\nclosing price in past 5 days). The 7 factors are valuable and signiﬁcant in price prediction with the theory of technical\\nanalysis, Mean Reversion, or MAR. Therefore, these factors can be used as 7 features of the stock data for the price\\nprediction. Suppose our input is X = {x1, ..., xt}, which consists of the daily stock data of t days. Each xk in X is a\\nvector, which is composed of 7 features as follows:\\n\\n[xk,i]7\\n\\ni=1 = [xk,High, xk,Low, xk,Open, xk,Close, xk,TurnoverRate, xk,Volume, xk,Ma5].\\n\\nThe architecture of the generator is shown in Fig. 1. For simplicity, we have omitted the details of the LSTM. With\\nthe generator, we extract the output ht of the LSTM and put it into a fully connected layer with 7 neurons to generate\\nthe ˆxt+1. ˆxt+1 aims to approximate xt+1 and we can get ˆxt+1,Close from ˆxt+1 as the prediction of closing price on the t + 1\\nday.\\n\\nThe output of generator G(X) is deﬁned as follows:\\n\\n(1)\\n\\n(2)\\n\\nht = g(X),\\n\\n\\x0c402 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406\\n\\n3\\n\\nx^\\n\\nt+1\\n\\nh\\n\\nt\\n\\nX\\n\\n...\\n\\nLSTM\\n\\nFig. 1. The generator designed with an LSTM.\\n\\nG(X) = ˆxt+1 = δ(WT\\n\\nhht + bh),\\n\\nwhere g(·) denotes the output of LSTM and ht is the output of the LSTM with X = {x1, ..., xt} as the input. δ stands\\nfor the Leaky Rectiﬁed Linear Unit (ReLU) activation function. Wh and bh denote the weight and bias in the fully\\nconnected layer. We also use dropout as a regularization method to avoid overﬁtting. Furthermore, we can continue to\\npredict ˆxt+2 with ˆxt+1 and X.\\n\\n3.3. The Discriminator\\n\\nThe purpose of the discriminator is to constitute a diﬀerentiable function D to classify the input data. The discrim-\\ninator is expected to output 0 when inputting a fake data and output 1 when inputting a real data. Here, we choose an\\nMLP as our discriminator with three hidden layers h1,h2,h3 including 72, 100, 10 neurons, respectively. The Leaky\\nReLU is used as the activation function among the hidden layers and the sigmoid function is used in the output layer.\\nIn addition, the cross entropy loss is chosen as the loss function to optimize the MLP. In particular, we concatenate\\nthe X = {x1, ..., xt} and ˆxt+1 to get {x1, ..., xt, ˆxt+1} as the fake data Xfake. Similarly, we concatenate the X = {x1, ..., xt}\\nand xt+1 to get {x1, ..., xt, xt+1} as the real data Xreal. The output of the discriminator is deﬁned as follows:\\n\\nD(Xfake) = σ(d(Xfake)),\\n\\nD(Xreal) = σ(d(Xreal)),\\n\\n3.4. The Architecture of GAN\\n\\nwhere d(·) denotes the output of MLP and denotes the sigmoid activation function. Both Xfake and Xreal output a\\nsingle scalar. Fig. 2 shows the architecture of the discriminator.\\n\\nWith the two models mentioned above, we propose our GAN architecture. According to [5], in the two-player\\nminimax game, both G and D try to optimize a value function. Similarly, we can deﬁne the optimization of our value\\nfunction V(G,D) as follows:\\n\\nmin\\nG\\n\\nmax\\nD\\n\\nV(G, D) = E(cid:31)logD (Xreal)(cid:30) + E(cid:31)log (1 − D (Xfake))(cid:30) .\\n\\n(3)\\n\\n(4)\\n\\n(5)\\n\\n(6)\\n\\n\\x0c4\\n \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406 \\n\\n403\\n\\n0\\n\\n1\\n\\nh3\\n\\nh2\\n\\nh1\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\nXfake\\n\\nX₀₀ X₁₁\\n\\n. . .\\n\\nXt\\n\\n^\\nX\\n\\nt+1\\n\\nX₀₀ X₁₁\\n\\n. . .\\n\\nXt\\n\\nXt+1\\n\\nXreal\\n\\nFig. 2. Discriminator designed using an MLP with Xfake and Xreal as the inputs.\\n\\nWe deﬁne the generator loss Gloss and discriminator loss Dloss to optimize the value function. Particularly, we\\ncombine the Mean Square Error (MSE) with the generator loss of a classical GAN to constitute the Gloss of our\\narchitecture. The Gloss and Dloss are as follows:\\n\\n1\\nm\\n\\nm(cid:31)i=1\\n\\nlog(1 − D(Xi\\n\\nfake)),\\n\\nDloss = −\\n\\ngMSE =\\n\\ngloss =\\n\\nreal) −\\n\\n1\\nm\\n\\nlogD(Xi\\n\\nm(cid:31)i=1\\nm(cid:31)i=1\\nm(cid:31)i=1\\nlog(1 − D(Xi\\n\\nt+1 − xi\\n(ˆxi\\n\\n1\\nm\\n\\n1\\nm\\n\\nt+1)2,\\n\\nfake)),\\n\\nGloss = λ1gMSE + λ2gloss.\\n\\nThe loss function Gloss is composed by gMSE and gloss with λ1 and λ2, respectively. λ1 and λ2 are hyper-parameters\\nthat we set manually. Fig. 3 shows the architecture of our GAN. The reason why we put Xfake and Xreal rather than ˆxt+1\\nand xt+1 in the discriminator is that we expect the discriminator to capture the correlation and time series information\\nbetween xt+1 and X.\\n\\n4. Experiments\\n\\n4.1. Dataset Descriptions\\n\\nWe evaluate our model on the real stock data, including the Standard & Poor’s 500 (S&P 500 Index), Shanghai\\nComposite Index in China, International Business Machine (IBM) from New York Stock Exchange (NYSE), Mi-\\ncrosoft Corporation (MSFT) from National Association of Securities Dealers Automated Quotation (NASDAQ), Ping\\nAn Insurance Company of China (PAICC). All the stock data can be downloaded in Yahoo Finance. We select the\\ntrade date within the last 20 years (almost 5000 pieces of data in each stock). For instance, some examples of the\\nstock features are shown in Tab. 1 . The trade date is not continuous due to the limitation of trading on weekends and\\nholidays.\\n\\nNote that the normalization is necessary and a key point to achieve competitive results. With the assumption of\\n\\nMAR mentioned above, we normalize the data as follows:\\n\\nxi =\\n\\nxi − µt\\nτt\\n\\n,\\n\\n(7)\\n\\n(8)\\n\\n(9)\\n\\n(10)\\n\\n(11)\\n\\n\\x0c404 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406\\n\\n5\\n\\nReal Data\\n\\nX\\n\\nX₀₀\\n\\nX₁₁\\n\\n.\\n.\\n.\\n\\nXt\\n\\nG\\n\\nGenerator\\n\\nX₀₀\\n\\nX₁₁\\n\\n.\\n.\\n.\\n\\nXt\\n\\nt+1X\\n\\nX₀₀\\n\\nX₁₁\\n\\n.\\n.\\n.\\n\\nXt\\n\\n^\\nt+1X\\n\\n^\\nt+1X\\n\\nD\\n\\nDiscriminator\\n\\nIs D correct\\n\\n?\\n\\nFine Tune\\n\\nFig. 3. The architecture of our GAN.\\n\\nTable 1. Raw stock data from PAICC.\\n\\nName\\n\\nTrade Date\\n\\nOpen Price\\n\\nHighest Price\\n\\nLowest Price\\n\\nClose Price\\n\\nTurnover Volume\\n\\nTurnover Rate\\n\\nMa5\\n\\nPAICC\\nPAICC\\nPAICC\\nPAICC\\nPAICC\\n\\n2017/12/20\\n2017/12/21\\n2017/12/22\\n2017/12/25\\n2017/12/26\\n\\n73.49\\n73.64\\n74.8\\n74.06\\n74.11\\n\\n74.59\\n75.58\\n75.2\\n76.17\\n74.56\\n\\n73.08\\n73.21\\n73.63\\n73.37\\n72.88\\n\\n73.94\\n74.91\\n74.02\\n74.16\\n73.94\\n\\n93938288\\n93470766\\n7195658\\n110997896\\n83228950\\n\\n0.0087\\n0.0086\\n0.0066\\n0.0102\\n0.0077\\n\\n71.792\\n72.608\\n73.482\\n74.201\\n74.136\\n\\nwhere µt and τt are the mean and standard deviation of X. We select t = 5 empirically because we attempt to predict\\nthe data in the next day by data in the past one week (trade is limited on weekends). For instance, we compute the\\nmean and standard deviation of the data of 5 days to normalize the data. Afterwards, the normalized data are used to\\npredict the data on 6th day. The data in both training and testing periods are processed in the same way.\\n\\n4.2. Training of our model\\n\\nOur purpose is to predict these 7 factors and get the closing price in the next day through the data in the past t days.\\nThe reason why predicting 7 factors in the next day is that the generator aims to mining the distributions of the real\\ndata and we can get the closing price from the generated data. The data are separated into two parts for training and\\ntesting. We choose the ﬁrst 90%-95% of the stock data for training and the remaining 5%-10% (about 250-500 pieces\\nof data) for testing.\\n\\nThe loss in the training period can be seen in Fig. 4. There is a signiﬁcant adversarial process between the discrim-\\ninator and generator during training. Both the discriminator and generator have been optimized during the adversarial\\nprocess.\\n\\n4.3. Experimental results\\n\\nWe evaluate the forecasting performance of our model by the following statistical indicators: Mean Absolute Error\\n(MAE), Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE) and Average Return (AR).\\n\\n\\x0c \\n6\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\n405\\n\\nFig. 4. Losses of the discriminator and generator during training.\\n\\nSuppose the real closing price and the prediction of closing price on the k-th day as yk and ˆyk. Then the indicators are\\ngiven as follows:\\n\\nMAE =\\n\\nRMAE = (cid:29)(cid:28)(cid:27) 1\\n\\nMAE =\\n\\nAR =\\n\\n1\\nN − 1\\n\\n1\\nN\\n\\n1\\nN\\n\\nN\\n\\n(ˆyk − yk)2,\\n\\nN(cid:31)k=1(cid:30)(cid:30)(cid:30)ˆyk − yk(cid:30)(cid:30)(cid:30),\\nN(cid:31)k=1\\nN(cid:31)k=1 (cid:30)(cid:30)(cid:30)ˆyk − yk(cid:30)(cid:30)(cid:30)\\nN−1(cid:31)k=1(cid:26)yk+1 − yk(cid:25) , if ˆyk+1 > ˆyk.\\n\\nyk\\n\\n,\\n\\nWe compute the mean of RMSE on our ﬁve datasets as the average evaluation. MAE and HR are also calculated\\nin this way. Support Vector Regression (SVR), ANN and LSTM are classical methods for stock market prediction\\nand we choose them as the baselines to compare with our model. The prediction results are shown in Tab. 2 with the\\nboldface as the best results. Low MAE, RMSE and MAPE indicate that the prediction of closing price is approximate\\nto the real data. AR shows the daily average return of these stocks based on four prediction methods. We can see our\\nmethod achieves a competitive performance compared with other methods.\\n\\nTable 2. The average evaluation on ﬁve stock data sets.\\n\\nMethod\\n\\nOur GAN\\nLSTM\\nANN\\nSVR\\n\\nMAE\\n\\n3.0401\\n4.1228\\n7.3029\\n4.9285\\n\\nRMSE\\n\\n4.1026\\n5.4131\\n9.1757\\n8.2261\\n\\nMAPE\\n\\n0.0137\\n0.0145\\n0.0808\\n0.0452\\n\\nFig. 5 shows an example of prediction by four methods on PAICC with the same training steps. From Fig. 5 we\\n\\ncan see that the best performance in matching the trend line of the real price is achieved by our method.\\n\\n5. Conclusion\\n\\nWe have made an exploration in stock market prediction and attempt to catch the distributions of the real stock data\\nby our proposed GAN. For the future work, we plan to explore how to extract more valuable and inﬂuential ﬁnancial\\n\\n(12)\\n\\n(13)\\n\\n(14)\\n\\n(15)\\n\\nAR\\n\\n0.7554\\n0.6859\\n0.5249\\n0.7266\\n\\n\\x0c406 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406\\n\\n7\\n\\nFig. 5. Illustration of price prediction by our GAN and some compared models on PAICC.\\n\\nfactors from stock markets and optimize our model to learn the data distributions more accurately, so that we can\\nobtain a higher precision of trend or price prediction in stock market by our method.\\n\\nThis work was supported by the National Key R&D Program of China under Grant 2016YFC1401004, the Na-\\ntional Natural Science Foundation of China (NSFC) under Grant No. 61170312 and 61633021, the Science and\\nTechnology Program of Qingdao under Grant No. 17-3-3-20-nsh, the CERNET Innovation Project under Grant No.\\nNGII20170416, the State Key Laboratory of Software Engineering under Grant No. SKLSE2012-09-14, and the Fun-\\ndamental Research Funds for the Central Universities of China.\\n\\nAcknowledgements\\n\\nReferences\\n\\n[1] Areekul, P., Senjyu, T., Toyama, H., Yona, A., 2010. A hybrid arima and neural network model for short-term price forecasting in deregulated\\n\\nmarket. IEEE Transactions on Power Systems Pwrs .\\n\\n[2] Box, G.E.P., Jenkins, G.M., 1976. Time series analysis: Forecasting and control. Journal of Time 31, 238–242.\\n[3] Chandar, S.K., Sumathi, M., Sivanandam, S.N., 2016. Prediction of stock market price using hybrid of wavelet transform and artiﬁcial neural\\n\\nnetwork. Indian Journal of Science & Technology 9.\\n\\n[4] Ding, X., Zhang, Y., Liu, T., Duan, J., 2015. Deep learning for event-driven stock prediction, in: Proceedings of the Twenty-Fourth International\\n\\nJoint Conference on Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 2327–2333.\\n\\n[5] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y., 2014. Generative adversar-\\nial nets, in: Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014,\\nDecember 8-13 2014, Montreal, Quebec, Canada, pp. 2672–2680.\\n\\n[6] Huang, S., Wang, H., 2006. Combining time-scale feature extractions with svms for stock index forecasting, in: Neural Information Processing,\\n\\n13th International Conference, ICONIP 2006, Hong Kong, China, October 3-6, 2006, Proceedings, Part III, pp. 390–399.\\n\\n[7] Li, B., Hoi, S.C.H., 2012. On-line portfolio selection with moving average reversion, in: Proceedings of the 29th International Conference on\\n\\nMachine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012.\\n\\n[8] Nevmyvaka, Y., Feng, Y., Kearns, M.J., 2006. Reinforcement learning for optimized trade execution, in: Machine Learning, Proceedings of\\n\\nthe Twenty-Third International Conference (ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006, pp. 673–680.\\n\\n[9] Pai, P.F., Lin, C.S., 2005. A hybrid arima and support vector machines model in stock price forecasting. Omega 33, 497–505.\\n[10] Rather, A.M., Agarwal, A., Sastry, V.N., 2015. Recurrent neural network and a hybrid model for prediction of stock returns. Expert Syst. Appl.\\n\\n42, 3234–3241.\\n\\n[11] Saad, E.W., Prokhorov, D.V., II, D.C.W., 1998. Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural\\n\\nnetworks. IEEE Trans. Neural Networks 9, 1456–1470.\\n\\n[12] Tsantekidis, A., Passalis, N., Tefas, A., Kanniainen, J., Gabbouj, M., Iosiﬁdis, A., 2017. Forecasting stock prices from the limit order book\\nusing convolutional neural networks, in: 19th IEEE Conference on Business Informatics, CBI 2017, Thessaloniki, Greece, July 24-27, 2017,\\nVolume 1: Conference Papers, pp. 7–12.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "text = extract_text('gan_prediction.pdf')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the function\n",
    "cleaned_text = []\n",
    "cleaned_text.append(text_cleaner(text,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['available online www sciencedirect com available online www sciencedirect com procedia computer science procedia computer science procedia computer science www elsevier com locate procedia www elsevier com locate procedia international conference identi cation information knowledge international conference identi cation information knowledge internet things iiki internet things iiki stock market prediction based generative adversarial network stock market prediction based generative adversarial network kang zhanga guoqiang zhonga junyu donga shengke wanga yong wanga kang zhanga guoqiang zhonga junyu donga shengke wanga yong wanga adepartment computer science technology ocean university china qingdao china adepartment computer science technology ocean university china qingdao china abstract abstract deep learning recently achieved great success many areas due strong capacity data process instance deep learning recently achieved great success many areas due strong capacity data process instance widely used nancial areas stock market prediction portfolio optimization nancial information processing trade widely used nancial areas stock market prediction portfolio optimization nancial information processing trade execution strategies stock market prediction one popular valuable area nance paper propose execution strategies stock market prediction one popular valuable area nance paper propose novel architecture generative adversarial network multi layer perceptron discriminator novel architecture generative adversarial network multi layer perceptron discriminator long short term memory generator forecasting closing price stocks generator built lstm long short term memory generator forecasting closing price stocks generator built lstm mine data distributions stocks given data stock market generate data distributions whereas mine data distributions stocks given data stock market generate data distributions whereas discriminator designed mlp aims discriminate real stock data generated data choose daily data discriminator designed mlp aims discriminate real stock data generated data choose daily data index several stocks wide range trading days try predict daily closing price experimental results show index several stocks wide range trading days try predict daily closing price experimental results show novel gan get promising performance closing price prediction real data compared models novel gan get promising performance closing price prediction real data compared models machine learning deep learning machine learning deep learning authors published elsevier authors published elsevier authors published elsevier open access article cc nc nd license open access article cc nc nd license open access article cc nc nd license peer review responsibility scienti committee international conference identi cation information peer review responsibility scientific committee international conference identification information peer review responsibility scienti committee international conference identi cation information knowledge internet things knowledge internet things knowledge internet things keywords deep learning stock prediction generative adversarial networks data mining keywords deep learning stock prediction generative adversarial networks data mining introduction introduction prediction stock market returns one important challenging issues domain many prediction stock market returns one important challenging issues domain many analyses assumptions nancial area show stock market predictable technical analysis stock invest analyses assumptions nancial area show stock market predictable technical analysis stock invest ment theory analysis methodology forecasting direction prices research past market ment theory analysis methodology forecasting direction prices research past market data meaningful assumption named mean reversion states stock price temporary tends move data meaningful assumption named mean reversion states stock price temporary tends move average price time moreover assumption development called moving average reversion average price time moreover assumption development called moving average reversion supposes average price mean price past window time days supposes average price mean price past window time days based views mentioned propose new deep learning model forecast daily closing price based views mentioned propose new deep learning model forecast daily closing price main contributions paper summarized followings main contributions paper summarized followings guoqiang zhong tel guoqiang zhong tel mail address gqzhong ouc edu cn mail address gqzhong ouc edu cn authors published elsevier authors published elsevier authors published elsevier open access article cc nc nd license open access article cc nc nd license open access article cc nc nd license peer review responsibility scienti committee international conference identi cation information knowledge peer review responsibility scientific committee international conference identification information peer review responsibility scienti committee international conference identi cation information knowledge internet things knowledge internet things internet things procs sciencedirectavailable online www sciencedirect com kang zhang et al procedia computer science kang zhang procedia computer science novel generative adversarial network architecture long short term memory network generator multi layer perceptron discriminator proposed model trained end end way predict daily closing price giving stock data several past days try generate distributions stock daily data adversarial learning system instead utilizing traditional regression methods price forecasting related work stock market prediction seen time series forecasting issue one classical algorithms autoregressive integrated moving average arima performs well linear stationary time series doesnt perform well nonlinear non stationary data stock market order solve problem one approach combines arima svm idea forecasting constituted linear part nonlinear part predict linear part arima nonlinear part svm moreover another approach combines wavelet basis svm decomposes stock data wavelet transformation uses svm forecasting subsequently arti cial neural network combined arima predict nonlinear part stock price data hybrid wavelet transformation ann demonstrated ective features extracted training ann convolutional neural network also used forecasting stock prices limit order book number orders price bid ask orders transformed array addition designed rnns applied forecasting stock data news events nancial area extracted represented dense vectors realize stock prediction besides reinforcement learning another popular method improve trading strategies fusing learning dynamic programming methodology principle generator gan new framework trains two models like zero sum game adversarial process generator seen cheater generate similar data real data discriminator plays role judge distinguish real data generated data reach ideal point discriminator unable di erentiate two types data point generator capture data distributions game based principle propose gan architecture prediction stock closing price generator model designed lstm strong ability processing time series data choose daily data last years nancial factors predict future closing price factors stock data one day high price low price open price close price volume turnover rate factors valuable signi cant price prediction theory technical analysis mean reversion mar therefore factors used features stock data price prediction suppose input xt consists daily stock data days xk vector composed features follows xk xk high xk low xk open xk close xk turnoverrate xk volume xk architecture generator shown fig simplicity omitted details lstm generator extract output ht lstm put fully connected layer neurons generate xt xt aims approximate xt get xt close xt prediction closing price day output generator de ned follows ht kang zhang procedia computer science kang zhang et al procedia computer science lstm fig generator designed lstm xt denotes output lstm ht output lstm xt input stands leaky recti ed linear unit activation function wh bh denote weight bias fully connected layer also use dropout regularization method avoid tting furthermore continue predict xt xt discriminator purpose discriminator constitute di erentiable function classify input data discrim inator expected output inputting fake data output inputting real data choose mlp discriminator three hidden layers including neurons respectively leaky relu used activation function among hidden layers sigmoid function used output layer addition cross entropy loss chosen loss function optimize mlp particular concatenate xt xt get xt xt fake data xfake similarly concatenate xt xt get xt xt real data xreal output discriminator de ned follows architecture gan denotes output mlp denotes sigmoid activation function xfake xreal output single scalar fig shows architecture discriminator two models mentioned propose gan architecture according two player minimax game try optimize value function similarly de ne optimization value function follows min max elogd elog kang zhang procedia computer science kang zhang et al procedia computer science xfake xt xt xt xreal fig discriminator designed using mlp xfake xreal inputs de ne generator loss gloss discriminator loss dloss optimize value function particularly combine mean square error generator loss classical gan constitute gloss architecture gloss dloss follows mi log dloss gmse gloss real logdi mi mi log fake gloss gmse gloss loss function gloss composed gmse gloss respectively hyper parameters set manually fig shows architecture gan reason put xfake xreal rather xt xt discriminator expect discriminator capture correlation time series information xt experiments dataset descriptions evaluate model real stock data including standard poor shanghai composite index china international business machine new york stock exchange mi crosoft corporation national association securities dealers automated quotation ping insurance company china stock data downloaded yahoo finance select trade date within last years instance examples stock features shown tab trade date continuous due limitation trading weekends holidays note normalization necessary key point achieve competitive results assumption mar mentioned normalize data follows xi xi kang zhang procedia computer science kang zhang et al procedia computer science real data xt generator xt xt discriminator correct fine tune fig architecture gan table raw stock data paicc name trade date open price highest price lowest price close price turnover volume turnover rate paicc paicc paicc paicc paicc mean standard deviation select empirically attempt predict data next day data past one week instance compute mean standard deviation data days normalize data afterwards normalized data used predict data th day data training testing periods processed way training model purpose predict factors get closing price next day data past days reason predicting factors next day generator aims mining distributions real data get closing price generated data data separated two parts training testing choose rst stock data training remaining testing loss training period seen fig signi cant adversarial process discrim inator generator training discriminator generator optimized adversarial process experimental results evaluate forecasting performance model following statistical indicators mean absolute error root mean square error mean absolute percentage error average return kang zhang et al procedia computer science kang zhang procedia computer science fig losses discriminator generator training suppose real closing price prediction closing price th day yk yk indicators given follows mae rmae mae ar nk yk yk nk nk yk yk yk yk yk yk yk compute mean rmse datasets average evaluation mae hr also calculated way support vector regression ann lstm classical methods stock market prediction choose baselines compare model prediction results shown tab boldface best results low mae rmse mape indicate prediction closing price approximate real data ar shows daily average return stocks based four prediction methods see method achieves competitive performance compared methods table average evaluation stock data sets method gan lstm ann svr mae rmse mape fig shows example prediction four methods paicc training steps fig see best performance matching trend line real price achieved method conclusion made exploration stock market prediction attempt catch distributions real stock data proposed gan future work plan explore extract valuable uential nancial ar kang zhang procedia computer science kang zhang et al procedia computer science fig illustration price prediction gan compared models paicc factors stock markets optimize model learn data distributions accurately obtain higher precision trend price prediction stock market method work supported national key program china grant yfc na tional natural science foundation china grant science technology program qingdao grant nsh cernet innovation project grant ngii state key laboratory software engineering grant sklse fun damental research funds central universities china acknowledgements references areekul senjyu toyama yona hybrid arima neural network model short term price forecasting deregulated market ieee transactions power systems pwrs box jenkins time series analysis forecasting control journal time chandar sumathi sivanandam prediction stock market price using hybrid wavelet transform arti cial neural network indian journal science technology ding zhang liu duan deep learning event driven stock prediction proceedings twenty fourth international joint conference arti cial intelligence ijcai buenos aires argentina july pp goodfellow pouget abadie mirza xu warde farley ozair courville bengio generative adversar ial nets advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pp huang wang combining time scale feature extractions svms stock index forecasting neural information processing th international conference iconip hong kong china october proceedings part iii pp li hoi line portfolio selection moving average reversion proceedings th international conference machine learning icml edinburgh scotland uk june july nevmyvaka feng kearns reinforcement learning optimized trade execution machine learning proceedings twenty third international conference pittsburgh pennsylvania usa june pp pai lin hybrid arima support vector machines model stock price forecasting omega rather agarwal sastry recurrent neural network hybrid model prediction stock returns expert syst appl saad prokhorov ii comparative study stock trend prediction using time delay recurrent probabilistic neural networks ieee trans neural networks tsantekidis passalis tefas kanniainen gabbouj iosi dis forecasting stock prices limit order book using convolutional neural networks th ieee conference business informatics cbi thessaloniki greece july volume conference papers pp']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_series = pd.Series(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text_series = cleaned_text_series.apply(lambda x : '_START_ '+ x + ' _END_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    _START_ available online www sciencedirect com available online www sciencedirect com procedia computer science procedia computer science procedia computer science www elsevier com locate procedia...\n",
       "dtype: object"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer \n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(cleaned_text_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 78.10650887573965\n",
      "Total Coverage of rare words: 38.45780795344326\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)\n",
    "\n",
    "## maxlen=max_text_len,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare a tokenizer \n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(cleaned_text_series))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(cleaned_text_series) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "# x_voc   =  x_tokenizer.num_words + 1\n",
    "x_voc = 1300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tokenizer.num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x000001FAB4872D08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x000001FAB4872D08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x000001FAB4872D08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x000001FAB4872D08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 100)    130000      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, None, 300),  481200      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 300),  721200      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    130000      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 300),  721200      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 300),  481200      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 300),  180300      lstm_2[0][0]                     \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 600)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 1300)   781300      concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,626,400\n",
      "Trainable params: 3,626,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "max_text_len = 1265\n",
    "latent_dim = 300\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(x_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "# attn_layer = Attention(name='attention_layer')\n",
    "# attn_out, attn_states = tf.map_fn([encoder_outputs, decoder_outputs])\n",
    "\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(x_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'input_2' with dtype float and shape [?,?]\n\t [[{{node input_2}}]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-126-b1d4c9f451be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m           \u001b[1;31m# `ins` can be callable in tf.distribute.Strategy + eager case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m           \u001b[0mactual_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mins\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'input_2' with dtype float and shape [?,?]\n\t [[{{node input_2}}]]"
     ]
    }
   ],
   "source": [
    "# history=model.fit(epochs=50,steps_per_epoch=10,callbacks=[es],batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x000001FAB4872D08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x000001FAB4872D08>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x000001FAB4872D08>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method AttentionLayer.call of <attention.AttentionLayer object at 0x000001FAB4872D08>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(None,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reverse_target_word_index=x_tokenizer.index_word \n",
    "target_word_index=x_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = target_word_index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='end'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "            # Exit condition: either hit max length or find stop word.\n",
    "            if (sampled_token == 'end' or len(decoded_sentence.split()) >= (max_text_len-1)):\n",
    "                stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1265)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "726",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-6aa9dbc63817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdecode_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-177-1eb2ff363ab5>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Sample a token\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msampled_token_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0msampled_token\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreverse_target_word_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msampled_token_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampled_token\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;34m'end'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 726"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "decode_sequence(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
