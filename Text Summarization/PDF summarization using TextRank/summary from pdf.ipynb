{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kbojja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Available online at www.sciencedirect.com\\nAvailable online at www.sciencedirect.com\\n\\nProcedia Computer Science 00 (2019) 000–000\\nProcedia Computer Science 00 (2019) 000–000\\n\\nProcedia Computer Science 147 (2019) 400–406\\n\\nwww.elsevier.com/locate/procedia\\nwww.elsevier.com/locate/procedia\\n\\n2018 International Conference on Identiﬁcation, Information and Knowledge\\n2018 International Conference on Identiﬁcation, Information and Knowledge\\n\\nin the Internet of Things, IIKI 2018\\nin the Internet of Things, IIKI 2018\\n\\nStock Market Prediction Based on Generative Adversarial Network\\nStock Market Prediction Based on Generative Adversarial Network\\n\\nKang Zhanga, Guoqiang Zhonga,∗, Junyu Donga, Shengke Wanga, Yong Wanga\\nKang Zhanga, Guoqiang Zhonga,∗, Junyu Donga, Shengke Wanga, Yong Wanga\\n\\naDepartment of Computer Science and Technology, Ocean University of China, Qingdao, 266100, China\\naDepartment of Computer Science and Technology, Ocean University of China, Qingdao, 266100, China\\n\\nAbstract\\nAbstract\\nDeep learning has recently achieved great success in many areas due to its strong capacity in data process. For instance, it has been\\nDeep learning has recently achieved great success in many areas due to its strong capacity in data process. For instance, it has been\\nwidely used in ﬁnancial areas such as stock market prediction, portfolio optimization, ﬁnancial information processing and trade\\nwidely used in ﬁnancial areas such as stock market prediction, portfolio optimization, ﬁnancial information processing and trade\\nexecution strategies. Stock market prediction is one of the most popular and valuable area in ﬁnance. In this paper, we propose a\\nexecution strategies. Stock market prediction is one of the most popular and valuable area in ﬁnance. In this paper, we propose a\\nnovel architecture of Generative Adversarial Network (GAN) with the Multi-Layer Perceptron (MLP) as the discriminator and the\\nnovel architecture of Generative Adversarial Network (GAN) with the Multi-Layer Perceptron (MLP) as the discriminator and the\\nLong Short-Term Memory (LSTM) as the generator for forecasting the closing price of stocks. The generator is built by LSTM\\nLong Short-Term Memory (LSTM) as the generator for forecasting the closing price of stocks. The generator is built by LSTM\\nto mine the data distributions of stocks from given data in stock market and generate data in the same distributions, whereas the\\nto mine the data distributions of stocks from given data in stock market and generate data in the same distributions, whereas the\\ndiscriminator designed by MLP aims to discriminate the real stock data and generated data. We choose the daily data on S&P 500\\ndiscriminator designed by MLP aims to discriminate the real stock data and generated data. We choose the daily data on S&P 500\\nIndex and several stocks in a wide range of trading days and try to predict the daily closing price. Experimental results show that\\nIndex and several stocks in a wide range of trading days and try to predict the daily closing price. Experimental results show that\\nour novel GAN can get a promising performance in the closing price prediction on the real data compared with other models in\\nour novel GAN can get a promising performance in the closing price prediction on the real data compared with other models in\\nmachine learning and deep learning.\\nmachine learning and deep learning.\\nc(cid:30) 2019 The Authors. Published by Elsevier B.V.\\n© 2019 The Authors. Published by Elsevier B.V.\\nc(cid:30) 2019 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nPeer-review under responsibility of the scientiﬁc committee of the 2018 International Conference on Identiﬁcation, Information\\nPeer-review under responsibility of the scientific committee of the 2018 International Conference on Identification, Information and \\nPeer-review under responsibility of the scientiﬁc committee of the 2018 International Conference on Identiﬁcation, Information\\nand Knowledge in the Internet of Things.\\nKnowledge in the Internet of Things.\\nand Knowledge in the Internet of Things.\\nKeywords: Deep Learning; Stock Prediction; Generative Adversarial Networks; Data Mining.\\nKeywords: Deep Learning; Stock Prediction; Generative Adversarial Networks; Data Mining.\\n\\n1. Introduction\\n1. Introduction\\n\\nThe prediction of stock market returns is one of the most important and challenging issues in this domain. Many\\nThe prediction of stock market returns is one of the most important and challenging issues in this domain. Many\\nanalyses and assumptions in ﬁnancial area show that stock market is predictable. Technical analysis in stock invest-\\nanalyses and assumptions in ﬁnancial area show that stock market is predictable. Technical analysis in stock invest-\\nment theory is an analysis methodology for forecasting the direction of prices through the research on past market\\nment theory is an analysis methodology for forecasting the direction of prices through the research on past market\\ndata. A meaningful assumption named Mean Reversion states that the stock price is temporary and tends to move to\\ndata. A meaningful assumption named Mean Reversion states that the stock price is temporary and tends to move to\\nthe average price over time. Moreover, this assumption has a further development called Moving Average Reversion\\nthe average price over time. Moreover, this assumption has a further development called Moving Average Reversion\\n(MAR), which supposes that the average of price is the mean of price in a past window of time, e.g. ﬁve days [7].\\n(MAR), which supposes that the average of price is the mean of price in a past window of time, e.g. ﬁve days [7].\\nBased on the views mentioned above, we propose a new deep learning model to forecast the daily closing price.\\nBased on the views mentioned above, we propose a new deep learning model to forecast the daily closing price.\\n\\nThe main contributions of this paper can be summarized in the followings:\\nThe main contributions of this paper can be summarized in the followings:\\n\\n∗ Guoqiang Zhong Tel.: +86-159-0639-8365.\\n∗ Guoqiang Zhong Tel.: +86-159-0639-8365.\\n\\nE-mail address: gqzhong@ouc.edu.cn\\nE-mail address: gqzhong@ouc.edu.cn\\n\\n1877-0509 c(cid:30) 2019 The Authors. Published by Elsevier B.V.\\n1877-0509 © 2019 The Authors. Published by Elsevier B.V.\\n1877-0509 c(cid:30) 2019 The Authors. Published by Elsevier B.V.\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nThis is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/)\\nPeer-review under responsibility of the scientiﬁc committee of the 2018 International Conference on Identiﬁcation, Information and Knowledge in\\nPeer-review under responsibility of the scientific committee of the 2018 International Conference on Identification, Information and \\nPeer-review under responsibility of the scientiﬁc committee of the 2018 International Conference on Identiﬁcation, Information and Knowledge in\\nthe Internet of Things.\\nKnowledge in the Internet of Things.\\nthe Internet of Things.\\n10.1016/j.procs.2019.01.256\\n\\nScienceDirectAvailable online at www.sciencedirect.com\\x0c \\n2\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\n401\\n\\n• A novel Generative Adversarial Network (GAN) architecture with Long-Short Term Memory (LSTM) network\\nas the generator and Multi-Layer Perceptron (MLP) as the discriminator is proposed. The model trained in and\\nend-to-end way to predict the daily closing price by giving the stock data in several past days.\\n\\n• We try to generate the same distributions of the stock daily data through the adversarial learning system, instead\\n\\nof only utilizing traditional regression methods for the price forecasting.\\n\\n2. Related Work\\n\\nThe stock market prediction can be seen as a time series forecasting issue and one of the classical algorithms is\\nthe Autoregressive Integrated Moving Average (ARIMA) [2]. ARIMA performs well in linear and stationary time\\nseries, but it doesnt perform well on the nonlinear and non-stationary data in stock market. In order to solve this\\nproblem, one approach [9] combines ARIMA with SVM. The idea is that the forecasting is constituted by a linear\\npart and a nonlinear part, so that they can predict the linear part with ARIMA and the nonlinear part with SVM.\\nMoreover, another approach [6] combines the wavelet basis with SVM, which decomposes the stock data with wavelet\\ntransformation and uses SVM for forecasting. Subsequently, the Artiﬁcial Neural Network (ANN) were combined\\nwith ARIMA to predict the nonlinear part of the stock price data [1]. The hybrid of wavelet transformation and ANN\\ndemonstrated that eﬀective features should be extracted for the training of ANN [3]. Convolutional Neural Network\\n(CNN) was also used in forecasting stock prices from the limit order book [12]. The number of orders and the price of\\n10 bid/ask orders were transformed into a 2D array. In addition, some designed RNNs had been applied to forecasting\\nthe stock data [11] [10]. News and events in ﬁnancial area were extracted and represented as dense vectors to realize\\nstock prediction [4]. Besides, reinforcement learning is another popular method to improve the trading strategies\\nthrough fusing Q-learning and dynamic programming [8].\\n\\n3. Our Methodology\\n\\n3.1. Principle\\n\\n3.2. The Generator\\n\\nGAN is a new framework which trains two models like a zero-sum game [5]. In the adversarial process, the\\ngenerator can be seen as a cheater to generate the similar data as the real data, while the discriminator plays the role\\nof judge to distinguish the real data and generated data. They can reach an ideal point that the discriminator is unable\\nto diﬀerentiate the two types of data. At this point, the generator can capture the data distributions from this game.\\nBased on this principle, we propose our GAN architecture for the prediction of stock closing price.\\n\\nThe generator of our model is designed by LSTM with its strong ability in processing time series data. We choose\\nthe daily data in the last 20 years with 7 ﬁnancial factors to predict the future closing price. The 7 factors of the stock\\ndata in one day are High Price, Low Price, Open Price, Close Price, Volume, Turnover Rate and Ma5 (the average of\\nclosing price in past 5 days). The 7 factors are valuable and signiﬁcant in price prediction with the theory of technical\\nanalysis, Mean Reversion, or MAR. Therefore, these factors can be used as 7 features of the stock data for the price\\nprediction. Suppose our input is X = {x1, ..., xt}, which consists of the daily stock data of t days. Each xk in X is a\\nvector, which is composed of 7 features as follows:\\n\\n[xk,i]7\\n\\ni=1 = [xk,High, xk,Low, xk,Open, xk,Close, xk,TurnoverRate, xk,Volume, xk,Ma5].\\n\\nThe architecture of the generator is shown in Fig. 1. For simplicity, we have omitted the details of the LSTM. With\\nthe generator, we extract the output ht of the LSTM and put it into a fully connected layer with 7 neurons to generate\\nthe ˆxt+1. ˆxt+1 aims to approximate xt+1 and we can get ˆxt+1,Close from ˆxt+1 as the prediction of closing price on the t + 1\\nday.\\n\\nThe output of generator G(X) is deﬁned as follows:\\n\\n(1)\\n\\n(2)\\n\\nht = g(X),\\n\\n\\x0c402 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406\\n\\n3\\n\\nx^\\n\\nt+1\\n\\nh\\n\\nt\\n\\nX\\n\\n...\\n\\nLSTM\\n\\nFig. 1. The generator designed with an LSTM.\\n\\nG(X) = ˆxt+1 = δ(WT\\n\\nhht + bh),\\n\\nwhere g(·) denotes the output of LSTM and ht is the output of the LSTM with X = {x1, ..., xt} as the input. δ stands\\nfor the Leaky Rectiﬁed Linear Unit (ReLU) activation function. Wh and bh denote the weight and bias in the fully\\nconnected layer. We also use dropout as a regularization method to avoid overﬁtting. Furthermore, we can continue to\\npredict ˆxt+2 with ˆxt+1 and X.\\n\\n3.3. The Discriminator\\n\\nThe purpose of the discriminator is to constitute a diﬀerentiable function D to classify the input data. The discrim-\\ninator is expected to output 0 when inputting a fake data and output 1 when inputting a real data. Here, we choose an\\nMLP as our discriminator with three hidden layers h1,h2,h3 including 72, 100, 10 neurons, respectively. The Leaky\\nReLU is used as the activation function among the hidden layers and the sigmoid function is used in the output layer.\\nIn addition, the cross entropy loss is chosen as the loss function to optimize the MLP. In particular, we concatenate\\nthe X = {x1, ..., xt} and ˆxt+1 to get {x1, ..., xt, ˆxt+1} as the fake data Xfake. Similarly, we concatenate the X = {x1, ..., xt}\\nand xt+1 to get {x1, ..., xt, xt+1} as the real data Xreal. The output of the discriminator is deﬁned as follows:\\n\\nD(Xfake) = σ(d(Xfake)),\\n\\nD(Xreal) = σ(d(Xreal)),\\n\\n3.4. The Architecture of GAN\\n\\nwhere d(·) denotes the output of MLP and denotes the sigmoid activation function. Both Xfake and Xreal output a\\nsingle scalar. Fig. 2 shows the architecture of the discriminator.\\n\\nWith the two models mentioned above, we propose our GAN architecture. According to [5], in the two-player\\nminimax game, both G and D try to optimize a value function. Similarly, we can deﬁne the optimization of our value\\nfunction V(G,D) as follows:\\n\\nmin\\nG\\n\\nmax\\nD\\n\\nV(G, D) = E(cid:31)logD (Xreal)(cid:30) + E(cid:31)log (1 − D (Xfake))(cid:30) .\\n\\n(3)\\n\\n(4)\\n\\n(5)\\n\\n(6)\\n\\n\\x0c4\\n \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406 \\n\\n403\\n\\n0\\n\\n1\\n\\nh3\\n\\nh2\\n\\nh1\\n\\n. . .\\n\\n. . .\\n\\n. . .\\n\\nXfake\\n\\nX₀₀ X₁₁\\n\\n. . .\\n\\nXt\\n\\n^\\nX\\n\\nt+1\\n\\nX₀₀ X₁₁\\n\\n. . .\\n\\nXt\\n\\nXt+1\\n\\nXreal\\n\\nFig. 2. Discriminator designed using an MLP with Xfake and Xreal as the inputs.\\n\\nWe deﬁne the generator loss Gloss and discriminator loss Dloss to optimize the value function. Particularly, we\\ncombine the Mean Square Error (MSE) with the generator loss of a classical GAN to constitute the Gloss of our\\narchitecture. The Gloss and Dloss are as follows:\\n\\n1\\nm\\n\\nm(cid:31)i=1\\n\\nlog(1 − D(Xi\\n\\nfake)),\\n\\nDloss = −\\n\\ngMSE =\\n\\ngloss =\\n\\nreal) −\\n\\n1\\nm\\n\\nlogD(Xi\\n\\nm(cid:31)i=1\\nm(cid:31)i=1\\nm(cid:31)i=1\\nlog(1 − D(Xi\\n\\nt+1 − xi\\n(ˆxi\\n\\n1\\nm\\n\\n1\\nm\\n\\nt+1)2,\\n\\nfake)),\\n\\nGloss = λ1gMSE + λ2gloss.\\n\\nThe loss function Gloss is composed by gMSE and gloss with λ1 and λ2, respectively. λ1 and λ2 are hyper-parameters\\nthat we set manually. Fig. 3 shows the architecture of our GAN. The reason why we put Xfake and Xreal rather than ˆxt+1\\nand xt+1 in the discriminator is that we expect the discriminator to capture the correlation and time series information\\nbetween xt+1 and X.\\n\\n4. Experiments\\n\\n4.1. Dataset Descriptions\\n\\nWe evaluate our model on the real stock data, including the Standard & Poor’s 500 (S&P 500 Index), Shanghai\\nComposite Index in China, International Business Machine (IBM) from New York Stock Exchange (NYSE), Mi-\\ncrosoft Corporation (MSFT) from National Association of Securities Dealers Automated Quotation (NASDAQ), Ping\\nAn Insurance Company of China (PAICC). All the stock data can be downloaded in Yahoo Finance. We select the\\ntrade date within the last 20 years (almost 5000 pieces of data in each stock). For instance, some examples of the\\nstock features are shown in Tab. 1 . The trade date is not continuous due to the limitation of trading on weekends and\\nholidays.\\n\\nNote that the normalization is necessary and a key point to achieve competitive results. With the assumption of\\n\\nMAR mentioned above, we normalize the data as follows:\\n\\nxi =\\n\\nxi − µt\\nτt\\n\\n,\\n\\n(7)\\n\\n(8)\\n\\n(9)\\n\\n(10)\\n\\n(11)\\n\\n\\x0c404 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406\\n\\n5\\n\\nReal Data\\n\\nX\\n\\nX₀₀\\n\\nX₁₁\\n\\n.\\n.\\n.\\n\\nXt\\n\\nG\\n\\nGenerator\\n\\nX₀₀\\n\\nX₁₁\\n\\n.\\n.\\n.\\n\\nXt\\n\\nt+1X\\n\\nX₀₀\\n\\nX₁₁\\n\\n.\\n.\\n.\\n\\nXt\\n\\n^\\nt+1X\\n\\n^\\nt+1X\\n\\nD\\n\\nDiscriminator\\n\\nIs D correct\\n\\n?\\n\\nFine Tune\\n\\nFig. 3. The architecture of our GAN.\\n\\nTable 1. Raw stock data from PAICC.\\n\\nName\\n\\nTrade Date\\n\\nOpen Price\\n\\nHighest Price\\n\\nLowest Price\\n\\nClose Price\\n\\nTurnover Volume\\n\\nTurnover Rate\\n\\nMa5\\n\\nPAICC\\nPAICC\\nPAICC\\nPAICC\\nPAICC\\n\\n2017/12/20\\n2017/12/21\\n2017/12/22\\n2017/12/25\\n2017/12/26\\n\\n73.49\\n73.64\\n74.8\\n74.06\\n74.11\\n\\n74.59\\n75.58\\n75.2\\n76.17\\n74.56\\n\\n73.08\\n73.21\\n73.63\\n73.37\\n72.88\\n\\n73.94\\n74.91\\n74.02\\n74.16\\n73.94\\n\\n93938288\\n93470766\\n7195658\\n110997896\\n83228950\\n\\n0.0087\\n0.0086\\n0.0066\\n0.0102\\n0.0077\\n\\n71.792\\n72.608\\n73.482\\n74.201\\n74.136\\n\\nwhere µt and τt are the mean and standard deviation of X. We select t = 5 empirically because we attempt to predict\\nthe data in the next day by data in the past one week (trade is limited on weekends). For instance, we compute the\\nmean and standard deviation of the data of 5 days to normalize the data. Afterwards, the normalized data are used to\\npredict the data on 6th day. The data in both training and testing periods are processed in the same way.\\n\\n4.2. Training of our model\\n\\nOur purpose is to predict these 7 factors and get the closing price in the next day through the data in the past t days.\\nThe reason why predicting 7 factors in the next day is that the generator aims to mining the distributions of the real\\ndata and we can get the closing price from the generated data. The data are separated into two parts for training and\\ntesting. We choose the ﬁrst 90%-95% of the stock data for training and the remaining 5%-10% (about 250-500 pieces\\nof data) for testing.\\n\\nThe loss in the training period can be seen in Fig. 4. There is a signiﬁcant adversarial process between the discrim-\\ninator and generator during training. Both the discriminator and generator have been optimized during the adversarial\\nprocess.\\n\\n4.3. Experimental results\\n\\nWe evaluate the forecasting performance of our model by the following statistical indicators: Mean Absolute Error\\n(MAE), Root Mean Square Error (RMSE), Mean Absolute Percentage Error (MAPE) and Average Return (AR).\\n\\n\\x0c \\n6\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\n405\\n\\nFig. 4. Losses of the discriminator and generator during training.\\n\\nSuppose the real closing price and the prediction of closing price on the k-th day as yk and ˆyk. Then the indicators are\\ngiven as follows:\\n\\nMAE =\\n\\nRMAE = (cid:29)(cid:28)(cid:27) 1\\n\\nMAE =\\n\\nAR =\\n\\n1\\nN − 1\\n\\n1\\nN\\n\\n1\\nN\\n\\nN\\n\\n(ˆyk − yk)2,\\n\\nN(cid:31)k=1(cid:30)(cid:30)(cid:30)ˆyk − yk(cid:30)(cid:30)(cid:30),\\nN(cid:31)k=1\\nN(cid:31)k=1 (cid:30)(cid:30)(cid:30)ˆyk − yk(cid:30)(cid:30)(cid:30)\\nN−1(cid:31)k=1(cid:26)yk+1 − yk(cid:25) , if ˆyk+1 > ˆyk.\\n\\nyk\\n\\n,\\n\\nWe compute the mean of RMSE on our ﬁve datasets as the average evaluation. MAE and HR are also calculated\\nin this way. Support Vector Regression (SVR), ANN and LSTM are classical methods for stock market prediction\\nand we choose them as the baselines to compare with our model. The prediction results are shown in Tab. 2 with the\\nboldface as the best results. Low MAE, RMSE and MAPE indicate that the prediction of closing price is approximate\\nto the real data. AR shows the daily average return of these stocks based on four prediction methods. We can see our\\nmethod achieves a competitive performance compared with other methods.\\n\\nTable 2. The average evaluation on ﬁve stock data sets.\\n\\nMethod\\n\\nOur GAN\\nLSTM\\nANN\\nSVR\\n\\nMAE\\n\\n3.0401\\n4.1228\\n7.3029\\n4.9285\\n\\nRMSE\\n\\n4.1026\\n5.4131\\n9.1757\\n8.2261\\n\\nMAPE\\n\\n0.0137\\n0.0145\\n0.0808\\n0.0452\\n\\nFig. 5 shows an example of prediction by four methods on PAICC with the same training steps. From Fig. 5 we\\n\\ncan see that the best performance in matching the trend line of the real price is achieved by our method.\\n\\n5. Conclusion\\n\\nWe have made an exploration in stock market prediction and attempt to catch the distributions of the real stock data\\nby our proposed GAN. For the future work, we plan to explore how to extract more valuable and inﬂuential ﬁnancial\\n\\n(12)\\n\\n(13)\\n\\n(14)\\n\\n(15)\\n\\nAR\\n\\n0.7554\\n0.6859\\n0.5249\\n0.7266\\n\\n\\x0c406 \\n\\nKang Zhang / Procedia Computer Science 00 (2019) 000–000\\n\\nKang Zhang  et al. / Procedia Computer Science 147 (2019) 400–406\\n\\n7\\n\\nFig. 5. Illustration of price prediction by our GAN and some compared models on PAICC.\\n\\nfactors from stock markets and optimize our model to learn the data distributions more accurately, so that we can\\nobtain a higher precision of trend or price prediction in stock market by our method.\\n\\nThis work was supported by the National Key R&D Program of China under Grant 2016YFC1401004, the Na-\\ntional Natural Science Foundation of China (NSFC) under Grant No. 61170312 and 61633021, the Science and\\nTechnology Program of Qingdao under Grant No. 17-3-3-20-nsh, the CERNET Innovation Project under Grant No.\\nNGII20170416, the State Key Laboratory of Software Engineering under Grant No. SKLSE2012-09-14, and the Fun-\\ndamental Research Funds for the Central Universities of China.\\n\\nAcknowledgements\\n\\nReferences\\n\\n[1] Areekul, P., Senjyu, T., Toyama, H., Yona, A., 2010. A hybrid arima and neural network model for short-term price forecasting in deregulated\\n\\nmarket. IEEE Transactions on Power Systems Pwrs .\\n\\n[2] Box, G.E.P., Jenkins, G.M., 1976. Time series analysis: Forecasting and control. Journal of Time 31, 238–242.\\n[3] Chandar, S.K., Sumathi, M., Sivanandam, S.N., 2016. Prediction of stock market price using hybrid of wavelet transform and artiﬁcial neural\\n\\nnetwork. Indian Journal of Science & Technology 9.\\n\\n[4] Ding, X., Zhang, Y., Liu, T., Duan, J., 2015. Deep learning for event-driven stock prediction, in: Proceedings of the Twenty-Fourth International\\n\\nJoint Conference on Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015, pp. 2327–2333.\\n\\n[5] Goodfellow, I.J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A.C., Bengio, Y., 2014. Generative adversar-\\nial nets, in: Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014,\\nDecember 8-13 2014, Montreal, Quebec, Canada, pp. 2672–2680.\\n\\n[6] Huang, S., Wang, H., 2006. Combining time-scale feature extractions with svms for stock index forecasting, in: Neural Information Processing,\\n\\n13th International Conference, ICONIP 2006, Hong Kong, China, October 3-6, 2006, Proceedings, Part III, pp. 390–399.\\n\\n[7] Li, B., Hoi, S.C.H., 2012. On-line portfolio selection with moving average reversion, in: Proceedings of the 29th International Conference on\\n\\nMachine Learning, ICML 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012.\\n\\n[8] Nevmyvaka, Y., Feng, Y., Kearns, M.J., 2006. Reinforcement learning for optimized trade execution, in: Machine Learning, Proceedings of\\n\\nthe Twenty-Third International Conference (ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006, pp. 673–680.\\n\\n[9] Pai, P.F., Lin, C.S., 2005. A hybrid arima and support vector machines model in stock price forecasting. Omega 33, 497–505.\\n[10] Rather, A.M., Agarwal, A., Sastry, V.N., 2015. Recurrent neural network and a hybrid model for prediction of stock returns. Expert Syst. Appl.\\n\\n42, 3234–3241.\\n\\n[11] Saad, E.W., Prokhorov, D.V., II, D.C.W., 1998. Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural\\n\\nnetworks. IEEE Trans. Neural Networks 9, 1456–1470.\\n\\n[12] Tsantekidis, A., Passalis, N., Tefas, A., Kanniainen, J., Gabbouj, M., Iosiﬁdis, A., 2017. Forecasting stock prices from the limit order book\\nusing convolutional neural networks, in: 19th IEEE Conference on Business Informatics, CBI 2017, Thessaloniki, Greece, July 24-27, 2017,\\nVolume 1: Conference Papers, pp. 7–12.\\n\\n\\x0c'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install pdfminer.six\n",
    "# from pdfminer.high_level import extract_text\n",
    "# text = extract_text('gan_prediction.pdf')\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read file, extract text and form tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def read_article(file_name):\n",
    "    text = extract_text(file_name) # extracts text from given pdf file\n",
    "    article = text.split(\". \")\n",
    "    sentences = []\n",
    "\n",
    "    for sentence in article:\n",
    "        print(sentence)\n",
    "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "    sentences.pop() \n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(file_name, top_n=5):\n",
    "    stop_words = stopwords.words('english')\n",
    "    summarize_text = []\n",
    "\n",
    "    # Step 1 - Read text anc split it\n",
    "    sentences =  read_article(file_name)\n",
    "\n",
    "    # Step 2 - Generate Similary Martix across sentences\n",
    "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
    "\n",
    "    # Step 3 - Rank sentences in similarity martix\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "    # Step 4 - Sort the rank and pick top sentences\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
    "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
    "\n",
    "    for i in range(top_n):\n",
    "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
    "\n",
    "    # Step 5 - Offcourse, output the summarize texr\n",
    "    print(\"Summarized Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      "v\n",
      "o\n",
      "N\n",
      "9\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "L\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "7\n",
      "v\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "\n",
      ".\n",
      "\n",
      "9\n",
      "0\n",
      "7\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "SEQ2SQL: GENERATING STRUCTURED QUERIES\n",
      "FROM NATURAL LANGUAGE USING REINFORCEMENT\n",
      "LEARNING\n",
      "\n",
      "Victor Zhong, Caiming Xiong, Richard Socher\n",
      "Salesforce Research\n",
      "Palo Alto, CA\n",
      "{vzhong,cxiong,rsocher}@salesforce.com\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Relational databases store a signiﬁcant amount of the worlds data\n",
      "However, ac-\n",
      "cessing this data currently requires users to understand a query language such as\n",
      "SQL\n",
      "We propose Seq2SQL, a deep neural network for translating natural lan-\n",
      "guage questions to corresponding SQL queries\n",
      "Our model uses rewards from in-\n",
      "the-loop query execution over the database to learn a policy to generate the query,\n",
      "which contains unordered parts that are less suitable for optimization via cross en-\n",
      "tropy loss\n",
      "Moreover, Seq2SQL leverages the structure of SQL to prune the space\n",
      "of generated queries and signiﬁcantly simplify the generation problem\n",
      "In addition\n",
      "to the model, we release WikiSQL, a dataset of 80654 hand-annotated examples\n",
      "of questions and SQL queries distributed across 24241 tables from Wikipedia that\n",
      "is an order of magnitude larger than comparable datasets\n",
      "By applying policy-\n",
      "based reinforcement learning with a query execution environment to WikiSQL,\n",
      "Seq2SQL outperforms a state-of-the-art semantic parser, improving execution ac-\n",
      "curacy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.\n",
      "\n",
      "1\n",
      "\n",
      "INTRODUCTION\n",
      "\n",
      "Relational databases store a vast amount of today’s information and provide the foundation of ap-\n",
      "plications such as medical records (Hillestad et al., 2005), ﬁnancial markets (Beck et al., 2000),\n",
      "and customer relations management (Ngai et al., 2009)\n",
      "However, accessing relational databases\n",
      "requires an understanding of query languages such as SQL, which, while powerful, is difﬁcult to\n",
      "master\n",
      "Natural language interfaces (NLI), a research area at the intersection of natural language\n",
      "processing and human-computer interactions, seeks to provide means for humans to interact with\n",
      "computers through the use of natural language (Androutsopoulos et al., 1995)\n",
      "We investigate one\n",
      "particular aspect of NLI applied to relational databases: translating natural language questions to\n",
      "SQL queries.\n",
      "Our main contributions in this work are two-fold\n",
      "First, we introduce Seq2SQL, a deep neural\n",
      "network for translating natural language questions to corresponding SQL queries\n",
      "Seq2SQL, shown\n",
      "in Figure 1, consists of three components that leverage the structure of SQL to prune the output\n",
      "space of generated queries\n",
      "Moreover, it uses policy-based reinforcement learning (RL) to generate\n",
      "the conditions of the query, which are unsuitable for optimization using cross entropy loss due\n",
      "to their unordered nature\n",
      "We train Seq2SQL using a mixed objective, combining cross entropy\n",
      "losses and RL rewards from in-the-loop query execution on a database\n",
      "These characteristics allow\n",
      "Seq2SQL to achieve state-of-the-art results on query generation.\n",
      "Next, we release WikiSQL, a corpus of 80654 hand-annotated instances of natural language ques-\n",
      "tions, SQL queries, and SQL tables extracted from 24241 HTML tables from Wikipedia\n",
      "Wik-\n",
      "iSQL is an order of magnitude larger than previous semantic parsing datasets that provide logi-\n",
      "cal forms along with natural language utterances\n",
      "We release the tables used in WikiSQL both in\n",
      "raw JSON format as well as in the form of a SQL database\n",
      "Along with WikiSQL, we release a\n",
      "query execution engine for the database used for in-the-loop query execution to learn the policy.\n",
      "On WikiSQL, Seq2SQL outperforms a previously state-of-the-art semantic parsing model by Dong\n",
      "\n",
      "1\n",
      "\n",
      "\f",
      "Figure 1: Seq2SQL takes as input a question and the columns of a table\n",
      "It generates the cor-\n",
      "responding SQL query, which, during training, is executed against a database\n",
      "The result of the\n",
      "execution is utilized as the reward to train the reinforcement learning algorithm.\n",
      "\n",
      "Figure 2: An example in WikiSQL\n",
      "The inputs consist of a table and a question\n",
      "The outputs consist\n",
      "of a ground truth SQL query and the corresponding result from execution.\n",
      "\n",
      "& Lapata (2016), which obtains 35.9% execution accuracy, as well as an augmented pointer net-\n",
      "work baseline, which obtains 53.3% execution accuracy\n",
      "By leveraging the inherent structure of\n",
      "SQL queries and applying policy gradient methods using reward signals from live query execution,\n",
      "Seq2SQL achieves state-of-the-art performance on WikiSQL, obtaining 59.4% execution accuracy.\n",
      "\n",
      "2 MODEL\n",
      "\n",
      "The WikiSQL task is to generate a SQL query from a natural language question and table schema.\n",
      "Our baseline model is the attentional sequence to sequence neural semantic parser proposed by Dong\n",
      "& Lapata (2016) that achieves state-of-the-art performance on a host of semantic parsing datasets\n",
      "without using hand-engineered grammar\n",
      "However, the output space of the softmax in their Seq2Seq\n",
      "model is unnecessarily large for this task\n",
      "In particular, we can limit the output space of the generated\n",
      "sequence to the union of the table schema, question utterance, and SQL key words\n",
      "The resulting\n",
      "model is similar to a pointer network (Vinyals et al., 2015) with augmented inputs\n",
      "We ﬁrst describe\n",
      "the augmented pointer network model, then address its limitations in our deﬁnition of Seq2SQL,\n",
      "particularly with respect to generating unordered query conditions.\n",
      "\n",
      "2.1 AUGMENTED POINTER NETWORK\n",
      "\n",
      "The augmented pointer network generates the SQL query token-by-token by selecting from an input\n",
      "sequence\n",
      "In our case, the input sequence is the concatenation of the column names, required for the\n",
      "selection column and the condition columns of the query, the question, required for the conditions\n",
      "of the query, and the limited vocabulary of the SQL language such as SELECT, COUNT etc\n",
      "In\n",
      "the example shown in Figure 2, the column name tokens consist of “Pick”, “#”, “CFL”, “Team”\n",
      "etc.; the question tokens consist of “How”, “many”, “CFL”, “teams” etc.; the SQL tokens consist of\n",
      "SELECT, WHERE, COUNT, MIN, MAX etc\n",
      "With this augmented input sequence, the pointer network\n",
      "can produce the SQL query by selecting exclusively from the input.\n",
      "Suppose we have a list of N table columns and a question such as in Figure 2, and want to produce\n",
      "the corresponding SQL query\n",
      "Let xc\n",
      "] denote the sequence of words in the\n",
      "name of the jth column, where xc\n",
      "j,i represents the ith word in the jth column and Tj represents the\n",
      "total number of words in the jth column\n",
      "Similarly, let xq and xs respectively denote the sequence\n",
      "of words in the question and the set of unique words in the SQL vocabulary.\n",
      "\n",
      "j = [xc\n",
      "\n",
      "j,2, ...xc\n",
      "\n",
      "j,1, xc\n",
      "\n",
      "j,Tj\n",
      "\n",
      "2\n",
      "\n",
      "Predicted resultsQuestion, schemaGround truth resultsSeq2SQLRewardGenerated SQLDatabase……………Wilfrid LaurierCaliforniaYorkYorkCollegeFrank Hoﬀman30Toronto ArgonautsDLCalgary StampedersL.P\n",
      "Ladouceur28Player29Ottawa RenegadesCFL Team27DBDTConnor HealyHamilton Tiger-CatsAnthony ForgonePick #OLPositionTable: CFLDraftQuestion:How many CFL teams are from York College?SELECT COUNT CFL Team FROM CFLDraft WHERE College = “York”SQL:2Result:\f",
      "We deﬁne the input sequence x as the concatenation of all the column names, the question, and the\n",
      "SQL vocabulary:\n",
      "\n",
      "x = [<col>; xc\n",
      "\n",
      "1; xc\n",
      "\n",
      "2; ...; xc\n",
      "\n",
      "N ; <sql>; xs; <question>; xq]\n",
      "\n",
      "(1)\n",
      "\n",
      "where [a; b] denotes the concatenation between the sequences a and b and we add sentinel tokens\n",
      "between neighbouring sequences to demarcate the boundaries.\n",
      "The network ﬁrst encodes x using a two-layer, bidirectional Long Short-Term Memory net-\n",
      "work (Hochreiter & Schmidhuber, 1997)\n",
      "The input to the encoder are the embeddings correspond-\n",
      "ing to words in the input sequence\n",
      "We denote the output of the encoder by henc, where henc\n",
      "is the\n",
      "state of the encoder corresponding to the tth word in the input sequence\n",
      "For brevity, we do not write\n",
      "out the LSTM equations, which are described by Hochreiter & Schmidhuber (1997)\n",
      "We then apply\n",
      "a pointer network similar to that proposed by Vinyals et al\n",
      "(2015) to the input encodings henc.\n",
      "The decoder network uses a two layer, unidirectional LSTM\n",
      "During each decoder step s, the decoder\n",
      "LSTM takes as input ys−1, the query token generated during the previous decoding step, and outputs\n",
      "the state gs\n",
      "Next, the decoder produces a scalar attention score αptr\n",
      "s,t for each position t of the input\n",
      "sequence:\n",
      "\n",
      "t\n",
      "\n",
      "s,t = W ptrtanh(cid:0)U ptrgs + V ptrht\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "αptr\n",
      "\n",
      "(2)\n",
      "\n",
      "We choose the input token with the highest score as the next token of the generated SQL query,\n",
      "ys = argmax(αptr\n",
      "\n",
      "s ).\n",
      "\n",
      "2.2 SEQ2SQL\n",
      "\n",
      "While the augmented pointer network can\n",
      "solve the SQL generation problem, it does\n",
      "not leverage the structure inherent in SQL.\n",
      "Typically, a SQL query such as that shown\n",
      "in Figure 3 consists of three components.\n",
      "The ﬁrst component is the aggregation op-\n",
      "erator,\n",
      "in this case COUNT, which pro-\n",
      "duces a summary of the rows selected by\n",
      "the query\n",
      "Alternatively the query may re-\n",
      "quest no summary statistics, in which case\n",
      "an aggregation operator is not provided.\n",
      "The second component is the SELECT\n",
      "column(s), in this case Engine, which\n",
      "identiﬁes the column(s) that are to be in-\n",
      "cluded in the returned results\n",
      "The third\n",
      "component is the WHERE clause of the query, in this case WHERE Driver = Val Musetti,\n",
      "which contains conditions by which to ﬁlter the rows\n",
      "Here, we keep rows in which the driver is\n",
      "“Val Musetti”.\n",
      "Seq2SQL, as shown in Figure 3, has three parts that correspond to the aggregation operator, the\n",
      "SELECT column, and the WHERE clause\n",
      "First, the network classiﬁes an aggregation operation\n",
      "for the query, with the addition of a null operation that corresponds to no aggregation\n",
      "Next, the\n",
      "network points to a column in the input table corresponding to the SELECT column\n",
      "Finally, the\n",
      "network generates the conditions for the query using a pointer network\n",
      "The ﬁrst two components\n",
      "are supervised using cross entropy loss, whereas the third generation component is trained using\n",
      "policy gradient to address the unordered nature of query conditions (we explain this in the subse-\n",
      "quent WHERE Clause section)\n",
      "Utilizing the structure of SQL allows Seq2SQL to further prune\n",
      "the output space of queries, which leads to higher performance than Seq2Seq and the augmented\n",
      "pointer network.\n",
      "\n",
      "Figure 3: The Seq2SQL model has three components,\n",
      "corresponding to the three parts of a SQL query (right).\n",
      "The input to the model are the question (top left) and\n",
      "the table column names (bottom left).\n",
      "\n",
      "Aggregation Operation\n",
      "The aggregation operation depends on the question\n",
      "For the example\n",
      "shown in Figure 3, the correct operator is COUNT because the question asks for “How many”\n",
      "To\n",
      "compute the aggregation operation, we ﬁrst compute the scalar attention score, αinp\n",
      ",\n",
      "t = W inphenc\n",
      "for each tth token in the input sequence\n",
      "We normalize the vector of scores αinp = [αinp\n",
      "2 , ...] to\n",
      "\n",
      "1 , αinp\n",
      "\n",
      "t\n",
      "\n",
      "3\n",
      "\n",
      "Seq2SQLCOUNTSELECTEngineWHERE Driver = Val MusettiHow many engine types did Val Musetti use?EntrantConstructorChassisEngineNoDriverAggregation classiﬁerSELECT column pointerWHERE clause pointer decoder\f",
      "produce a distribution over the input encodings, βinp = softmax(cid:0)αinp(cid:1)\n",
      "The input representation\n",
      "\n",
      "κagg is the sum over the input encodings henc weighted by the normalized scores βinp:\n",
      "\n",
      "κagg =\n",
      "\n",
      "βinp\n",
      "t henc\n",
      "\n",
      "t\n",
      "\n",
      "(3)\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "t\n",
      "\n",
      "Let αagg denote the scores over the aggregation operations such as COUNT, MIN, MAX, and the no-\n",
      "aggregation operation NULL\n",
      "We compute αagg by applying a multi-layer perceptron to the input\n",
      "representation κagg:\n",
      "(4)\n",
      "\n",
      "αagg = W agg tanh (V aggκagg + bagg) + cagg\n",
      "\n",
      "We apply the softmax function to obtain the distribution over the set of possible aggregation opera-\n",
      "tions βagg = softmax (αagg)\n",
      "We use cross entropy loss Lagg for the aggregation operation.\n",
      "\n",
      "SELECT Column\n",
      "The selection column depends on the table columns as well as the question.\n",
      "Namely, for the example in Figure 3, “How many engine types” indicates that we need to retrieve\n",
      "the “Engine” column\n",
      "SELECT column prediction is then a matching problem, solvable using a\n",
      "pointer: given the list of column representations and a question representation, we select the column\n",
      "that best matches the question.\n",
      "In order to produce the representations for the columns, we ﬁrst encode each column name with a\n",
      "LSTM\n",
      "The representation of a particular column j, ec\n",
      "\n",
      "j, is given by:\n",
      "\n",
      "j,t = LSTM(cid:0)emb(cid:0)xc\n",
      "\n",
      "(cid:1) , hc\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "j,t\n",
      "\n",
      "hc\n",
      "\n",
      "j,t−1\n",
      "\n",
      "(5)\n",
      "j,t denotes the tth encoder state of the jth column\n",
      "We take the last encoder state to be ec\n",
      "j,\n",
      "\n",
      "Here, hc\n",
      "column j’s representation.\n",
      "To construct a representation for the question, we compute another input representation κsel us-\n",
      "ing the same architecture as for κagg (Equation 3) but with untied weights\n",
      "Finally, we apply a\n",
      "multi-layer perceptron over the column representations, conditioned on the input representation, to\n",
      "compute the a score for each column j:\n",
      "\n",
      "j = hc\n",
      "ec\n",
      "\n",
      "j,Tj\n",
      "\n",
      "j = W sel tanh(cid:0)V selκsel + V cec\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "αsel\n",
      "\n",
      "columns βsel = softmax(cid:0)αsel(cid:1)\n",
      "For the example shown in Figure 3, the distribution is over the\n",
      "\n",
      "We normalize the scores with a softmax function to produce a distribution over the possible SELECT\n",
      "\n",
      "columns “Entrant”, “Constructor”, “Chassis”, “Engine”, “No”, and the ground truth SELECT col-\n",
      "umn “Driver”\n",
      "We train the SELECT network using cross entropy loss Lsel.\n",
      "\n",
      "j\n",
      "\n",
      "(6)\n",
      "\n",
      "WHERE Clause\n",
      "We can train the WHERE clause using a pointer decoder similar to that\n",
      "described in Section 2.1\n",
      "However,\n",
      "there is a limitation in using the cross entropy loss\n",
      "the WHERE conditions of a query can be swapped and the query\n",
      "to optimize the network:\n",
      "yield the same result.\n",
      "Suppose we have the question “which males are older than 18”\n",
      "and the queries SELECT name FROM insurance WHERE age > 18 AND gender =\n",
      "\"male\" and SELECT name FROM insurance WHERE gender = \"male\" AND age\n",
      "> 18\n",
      "Both queries obtain the correct execution result despite not having exact string match\n",
      "If\n",
      "the former is provided as the ground truth, using cross entropy loss to supervise the generation\n",
      "would then wrongly penalize the latter\n",
      "To address this problem, we apply reinforcement learning to\n",
      "learn a policy to directly optimize the expected correctness of the execution result (Equation 7).\n",
      "Instead of teacher forcing at each step of query generation, we sample from the output distribution to\n",
      "obtain the next token\n",
      "At the end of the generation procedure, we execute the generated SQL query\n",
      "against the database to obtain a reward\n",
      "Let y = [y1, y2, ..., yT ] denote the sequence of generated\n",
      "tokens in the WHERE clause\n",
      "Let q (y) denote the query generated by the model and qg denote the\n",
      "ground truth query corresponding to the question\n",
      "We deﬁne the reward R (q (y) , qg) as\n",
      "\n",
      "R (q (y) , qg) =\n",
      "\n",
      "if q (y) is not a valid SQL query\n",
      "if q (y) is a valid SQL query and executes to an incorrect result\n",
      "if q (y) is a valid SQL query and executes to the correct result\n",
      "\n",
      "(7)\n",
      "\n",
      "−2,\n",
      "\n",
      "−1,\n",
      "+1,\n",
      "\n",
      "4\n",
      "\n",
      "\f",
      "The loss, Lwhe = −Ey[R (q (y) , qg)], is the negative expected reward over possible WHERE clauses.\n",
      "We derive the policy gradient for Lwhe as shown by Sutton et al\n",
      "(2000) and Schulman et al\n",
      "(2015).\n",
      "\n",
      "∇Lwhe\n",
      "\n",
      "Θ\n",
      "\n",
      "(cid:0)Ey∼py [R (q (y) , qg)](cid:1)\n",
      "(cid:34)\n",
      "(cid:88)\n",
      "\n",
      "R (q (y) , qg)∇Θ\n",
      "\n",
      "= −∇Θ\n",
      "= −Ey∼py\n",
      "≈ −R (q (y) , qg)∇Θ\n",
      "\n",
      "(cid:88)\n",
      "\n",
      "t\n",
      "\n",
      "(log py (yt; Θ))\n",
      "\n",
      "t\n",
      "\n",
      "(cid:35)\n",
      "\n",
      "(log py (yt; Θ))\n",
      "\n",
      "(8)\n",
      "\n",
      "(9)\n",
      "\n",
      "(10)\n",
      "\n",
      "Here, py(yt) denotes the probability of choosing token yt during time step t\n",
      "In equation 10, we\n",
      "approximate the expected gradient using a single Monte-Carlo sample y.\n",
      "\n",
      "Mixed Objective Function\n",
      "We train the model using gradient descent to minimize the objective\n",
      "function L = Lagg + Lsel + Lwhe\n",
      "Consequently, the total gradient is the equally weighted sum of\n",
      "the gradients from the cross entropy loss in predicting the SELECT column, from the cross entropy\n",
      "loss in predicting the aggregation operation, and from policy learning.\n",
      "\n",
      "3 WIKISQL\n",
      "\n",
      "WikiSQL is a collection of questions, corre-\n",
      "sponding SQL queries, and SQL tables\n",
      "A sin-\n",
      "gle example in WikiSQL, shown in Figure 2,\n",
      "contains a table, a SQL query, and the natu-\n",
      "ral language question corresponding to the SQL\n",
      "query\n",
      "Table 1 shows how WikiSQL compares\n",
      "to related datasets\n",
      "Namely, WikiSQL is the\n",
      "largest hand-annotated semantic parsing dataset\n",
      "to date - it is an order of magnitude larger than\n",
      "other datasets that have logical forms, either in\n",
      "terms of the number of examples or the number\n",
      "of tables\n",
      "The queries in WikiSQL span over\n",
      "a large number of tables and hence presents an\n",
      "unique challenge: the model must be able to not only generalize to new queries, but to new table\n",
      "schema\n",
      "Finally, WikiSQL contains realistic data extracted from the web\n",
      "This is evident in the dis-\n",
      "tributions of the number of columns, the lengths of questions, and the length of queries, respectively\n",
      "shown in Figure 5\n",
      "Another indicator of the variety of questions in the dataset is the distribution of\n",
      "question types, shown in Figure 4.\n",
      "\n",
      "Figure 4: Distribution of questions in WikiSQL.\n",
      "\n",
      "Figure 5: Distribution of table, question, query sizes in WikiSQL.\n",
      "\n",
      "We collect WikiSQL by crowd-sourcing on Amazon Mechanical Turk in two phases\n",
      "First, a worker\n",
      "paraphrases a generated question for a table\n",
      "We form the generated question using a template,\n",
      "ﬁlled using a randomly generated SQL query\n",
      "We ensure the validity and complexity of the tables\n",
      "by keeping only those that are legitimate database tables and sufﬁciently large in the number of\n",
      "rows and columns\n",
      "Next, two other workers verify that the paraphrase has the same meaning as\n",
      "\n",
      "5\n",
      "\n",
      "\f",
      "Schema\n",
      "24241\n",
      "8\n",
      "141\n",
      "81*\n",
      "8\n",
      "2420\n",
      "2108\n",
      "\n",
      "the generated question\n",
      "We discard paraphrases that do not show enough variation, as measured\n",
      "by the character edit distance from the generated question, as well as those both workers deemed\n",
      "incorrect during veriﬁcation\n",
      "Section A of the Appendix contains more details on the collection\n",
      "of WikiSQL\n",
      "We make available examples of the interface used during the paraphrase phase and\n",
      "during the veriﬁcation phase in the supplementary materials\n",
      "The dataset is available for download\n",
      "at https://github.com/salesforce/WikiSQL.\n",
      "The tables, their paraphrases, and SQL queries\n",
      "are randomly slotted into train, dev, and test\n",
      "splits, such that each table is present in exactly\n",
      "one split\n",
      "In addition to the raw tables, queries,\n",
      "results, and natural utterances, we also release\n",
      "a corresponding SQL database and query exe-\n",
      "cution engine.\n",
      "\n",
      "Dataset\n",
      "WikiSQL\n",
      "Geoquery\n",
      "ATIS\n",
      "Freebase917\n",
      "Overnight\n",
      "WebQuestions\n",
      "WikiTableQuestions\n",
      "\n",
      "Size LF\n",
      "yes\n",
      "yes\n",
      "yes*\n",
      "yes\n",
      "yes\n",
      "no\n",
      "no\n",
      "\n",
      "80654\n",
      "880\n",
      "5871\n",
      "917\n",
      "26098\n",
      "5810\n",
      "22033\n",
      "\n",
      "result\n",
      "\n",
      "The datasets\n",
      "\n",
      "(Tang & Mooney,\n",
      "\n",
      "in the dataset, Nex\n",
      "\n",
      "3.1 EVALUATION\n",
      "Let N denote the total number of ex-\n",
      "amples\n",
      "the number\n",
      "of queries that, when executed,\n",
      "in\n",
      "the correct result, and Nlf\n",
      "the number of\n",
      "queries has exact\n",
      "string match with the\n",
      "ground truth query used to collect the para-\n",
      "phrase\n",
      "We evaluate using the execution\n",
      "accuracy metric Accex = Nex\n",
      "N .\n",
      "One\n",
      "downside of Accex is that\n",
      "is possible\n",
      "to construct a SQL query that does not\n",
      "correspond to the question but neverthe-\n",
      "less obtains the same result.\n",
      "For exam-\n",
      "ple, the two queries SELECT COUNT(name)\n",
      "WHERE SSN = 123 and SELECT COUNT(SSN) WHERE SSN = 123 produce the same re-\n",
      "sult if no two people with different names share the SSN 123\n",
      "Hence, we also use the logical form\n",
      "N \n",
      "However, as we showed in Section 2.2, Acclf incorrectly penalizes queries\n",
      "accuracy Acclf = Nlf\n",
      "that achieve the correct result but do not have exact string match with the ground truth query\n",
      "Due\n",
      "to these observations, we use both metrics to evaluate the models.\n",
      "\n",
      "Comparison between WikiSQL\n",
      "Table 1:\n",
      "are\n",
      "and existing datasets.\n",
      "GeoQuery880\n",
      "2001),\n",
      "ATIS (Price, 1990), Free917 (Cai & Yates,\n",
      "2013), Overnight (Wang et al., 2015), WebQues-\n",
      "tions (Berant et al., 2013), and WikiTableQues-\n",
      "tions (Pasupat & Liang, 2015)\n",
      "“Size” denotes\n",
      "the number of examples in the dataset.\n",
      "“LF”\n",
      "indicates whether it has annotated logical forms.\n",
      "“Schema” denotes the number of tables\n",
      "ATIS is\n",
      "presented as a slot ﬁlling task\n",
      "Each Freebase API\n",
      "page is counted as a separate domain.\n",
      "\n",
      "it\n",
      "\n",
      "4 EXPERIMENTS\n",
      "We tokenize the dataset using Stanford CoreNLP (Manning et al., 2014)\n",
      "We use the normalized\n",
      "tokens for training and revert into original gloss before outputting the query so that generated queries\n",
      "are executable on the database\n",
      "We use ﬁxed GloVe word embeddings (Pennington et al., 2014) and\n",
      "character n-gram embeddings (Hashimoto et al., 2016)\n",
      "Let wg\n",
      "x denote the GloVe embedding and wc\n",
      "x\n",
      "the character embedding for word x\n",
      "Here, wc\n",
      "x is the mean of the embeddings of all the character n-\n",
      "grams in x\n",
      "For words that have neither word nor character embeddings, we assign the zero vector.\n",
      "All networks are run for a maximum of 300 epochs with early stopping on dev split execution\n",
      "accuracy\n",
      "We train using ADAM (Kingma & Ba, 2014) and regularize using dropout (Srivastava\n",
      "et al., 2014)\n",
      "All recurrent layers have a hidden size of 200 units and are followed by a dropout of\n",
      "0.3\n",
      "We implement all models using PyTorch 1\n",
      "To train Seq2SQL, we ﬁrst train a version in which\n",
      "the WHERE clause is supervised via teacher forcing (i.e\n",
      "the policy is not learned from scratch) and\n",
      "then continue training using reinforcement learning\n",
      "In order to obtain the rewards described in\n",
      "Section 2.2, we use the query execution engine described in Section 3.\n",
      "\n",
      "4.1 RESULT\n",
      "We compare results against the attentional sequence to sequence neural semantic parser proposed\n",
      "by Dong & Lapata (2016)\n",
      "This model achieves state of the art results on a variety of semantic pars-\n",
      "ing datasets, outperforming a host of non-neural semantic parsers despite not using hand-engineered\n",
      "grammars\n",
      "To make this baseline even more competitive on our new dataset, we augment their input\n",
      "\n",
      "1https://pytorch.org\n",
      "\n",
      "6\n",
      "\n",
      "\f",
      "Model\n",
      "Baseline (Dong & Lapata, 2016)\n",
      "Aug Ptr Network\n",
      "Seq2SQL (no RL)\n",
      "Seq2SQL\n",
      "\n",
      "Dev Acclf Dev Accex\n",
      "23.3%\n",
      "44.1%\n",
      "48.2%\n",
      "49.5%\n",
      "\n",
      "37.0%\n",
      "53.8%\n",
      "58.1%\n",
      "60.8%\n",
      "\n",
      "Test Acclf\n",
      "23.4%\n",
      "43.3%\n",
      "47.4%\n",
      "48.3%\n",
      "\n",
      "Test Accex\n",
      "35.9%\n",
      "53.3%\n",
      "57.1%\n",
      "59.4%\n",
      "\n",
      "Table 2: Performance on WikiSQL\n",
      "Both metrics are deﬁned in Section 3.1\n",
      "For Seq2SQL (no RL),\n",
      "the WHERE clause is supervised via teacher forcing as opposed to reinforcement learning.\n",
      "with the table schema such that the model can generalize to new tables\n",
      "We describe this baseline in\n",
      "detail in Section 2 of the Appendix\n",
      "Table 2 compares the performance of the three models.\n",
      "Reducing the output space by utilizing the augmented pointer network improves upon the baseline\n",
      "by 17.4%\n",
      "Leveraging the structure of SQL queries leads to another improvement of 3.8%, as is\n",
      "shown by the performance of Seq2SQL without RL compared to the augmented pointer network.\n",
      "Finally, training using reinforcement learning based on rewards from in-the-loop query executions\n",
      "on a database leads to another performance increase of 2.3%, as is shown by the performance of the\n",
      "full Seq2SQL model.\n",
      "\n",
      "4.2 ANALYSIS\n",
      "Limiting the output space via pointer network leads to more accurate conditions\n",
      "Compared\n",
      "to the baseline, the augmented pointer network generates higher quality WHERE clause\n",
      "For ex-\n",
      "ample, for “in how many districts was a successor seated on march 4, 1850?”, the baseline gen-\n",
      "erates the condition successor seated = seated march 4 whereas Seq2SQL generates\n",
      "successor seated = seated march 4 1850\n",
      "Similarly, for “what’s doug battaglia’s\n",
      "pick number?”, the baseline generates Player = doug whereas Seq2SQL generates Player\n",
      "= doug battaglia\n",
      "The conditions tend to contain rare words (e.g\n",
      "“1850”), but the baseline\n",
      "is inclined to produce common words in the training corpus, such as “march” and “4” for date, or\n",
      "“doug” for name\n",
      "The pointer is less affected since it selects exclusively from the input.\n",
      "\n",
      "Model\n",
      "Aug Ptr Network\n",
      "Seq2SQL\n",
      "\n",
      "Incorporating structure reduces invalid\n",
      "queries\n",
      "Seq2SQL without RL directly pre-\n",
      "dicts selection and aggregation and reduces\n",
      "invalid SQL queries generated from 7.9% to\n",
      "4.8%\n",
      "A large quantity of invalid queries\n",
      "result from column names – the generated\n",
      "query refers to selection columns that are not present in the table\n",
      "This is particularly helpful when\n",
      "the column name contain many tokens, such as “Miles (km)”, which has 4 tokens\n",
      "Introducing a\n",
      "classiﬁer for the aggregation also reduces the error rate\n",
      "Table 3 shows that adding the aggregation\n",
      "classiﬁer improves the precision, recall, and F1 for predicting the COUNT operator\n",
      "For more queries\n",
      "produced by the different models, please see Section 3 of the Appendix.\n",
      "\n",
      "Precision Recall\n",
      "66.3%\n",
      "72.6%\n",
      "\n",
      "Table 3: Performance on the COUNT operator.\n",
      "\n",
      "64.4% 65.4%\n",
      "66.2% 69.2%\n",
      "\n",
      "F1\n",
      "\n",
      "RL generates higher quality WHERE clause that are ordered differently than ground truth.\n",
      "Training with policy-based RL obtains correct results in which the order of conditions is differs\n",
      "from the ground truth query\n",
      "For example, for “in what district was the democratic candidate ﬁrst\n",
      "elected in 1992?”, the ground truth conditions are First elected = 1992 AND Party =\n",
      "Democratic whereas Seq2SQL generates Party = Democratic AND First elected\n",
      "= 1992\n",
      "When Seq2SQL is correct and Seq2SQL without RL is not, the latter tends to produce an\n",
      "incorrect WHERE clause\n",
      "For example, for the rather complex question “what is the race name of the\n",
      "12th round trenton, new jersey race where a.j\n",
      "foyt had the pole position?”, Seq2SQL trained without\n",
      "RL generates WHERE rnd = 12 and track = a.j.\n",
      "foyt AND pole position =\n",
      "a.j\n",
      "foyt whereas Seq2SQL trained with RL correctly generates WHERE rnd = 12 AND\n",
      "pole position = a.j\n",
      "foyt.\n",
      "\n",
      "5 RELATED WORK\n",
      "Semantic Parsing.\n",
      "In semantic parsing for question answering (QA), natural language questions\n",
      "are parsed into logical forms that are then executed on a knowledge graph (Zelle & Mooney, 1996;\n",
      "\n",
      "7\n",
      "\n",
      "\f",
      "Wong & Mooney, 2007; Zettlemoyer & Collins, 2005; 2007)\n",
      "Other works in semantic parsing\n",
      "focus on learning parsers without relying on annotated logical forms by leveraging conversational\n",
      "logs (Artzi & Zettlemoyer, 2011), demonstrations (Artzi & Zettlemoyer, 2013), distant supervi-\n",
      "sion (Cai & Yates, 2013; Reddy et al., 2014), and question-answer pairs (Liang et al., 2011)\n",
      "Seman-\n",
      "tic parsing systems are typically constrained to a single schema and require hand-curated grammars\n",
      "to perform well2\n",
      "Pasupat & Liang (2015) addresses the single-schema limitation by proposing\n",
      "the ﬂoating parser, which generalizes to unseen web tables on the WikiTableQuestions task\n",
      "Our\n",
      "approach is similar in that it generalizes to new table schema\n",
      "However, we do not require access to\n",
      "table content, conversion of table to an additional graph, nor hand-engineered features/grammar.\n",
      "Semantic parsing datasets\n",
      "Previous semantic parsing systems were designed to answer complex\n",
      "and compositional questions over closed-domain, ﬁxed-schema datasets such as GeoQuery (Tang\n",
      "& Mooney, 2001) and ATIS (Price, 1990)\n",
      "Researchers also investigated QA over subsets of large-\n",
      "scale knowledge graphs such as DBPedia (Starc & Mladenic, 2017) and Freebase (Cai & Yates,\n",
      "2013; Berant et al., 2013)\n",
      "The dataset “Overnight” (Wang et al., 2015) uses a similar crowd-\n",
      "sourcing process to build a dataset of natural language question, logical form pairs, but has only 8\n",
      "domains\n",
      "WikiTableQuestions (Pasupat & Liang, 2015) is a collection of question and answers, also\n",
      "over a large quantity of tables extracted from Wikipedia\n",
      "However, it does not provide logical forms\n",
      "whereas WikiSQL does\n",
      "In addition, WikiSQL focuses on generating SQL queries for questions\n",
      "over relational database tables and only uses table content during evaluation.\n",
      "Representation learning for sequence generation\n",
      "Dong & Lapata (2016)’s attentional sequence\n",
      "to sequence neural semantic parser, which we use as the baseline, achieves state-of-the-art results\n",
      "on a variety of semantic parsing datasets despite not utilizing hand-engineered grammar\n",
      "Unlike\n",
      "their model, Seq2SQL uses pointer based generation akin to Vinyals et al\n",
      "(2015) to achieve higher\n",
      "performance, especially in generating queries with rare words and column names\n",
      "Pointer mod-\n",
      "els have also been successfully applied to tasks such as language modeling (Merity et al., 2017),\n",
      "summarization (Gu et al., 2016), combinatorial optimization (Bello et al., 2017), and question an-\n",
      "swering (Seo et al., 2017; Xiong et al., 2017)\n",
      "Other interesting neural semantic parsing models are\n",
      "the Neural Programmer (Neelakantan et al., 2017) and the Neural Enquirer (Yin et al., 2016)\n",
      "Mou\n",
      "et al\n",
      "(2017) proposed a distributed neural executor based on the Neural Enquirer, which efﬁciently\n",
      "executes queries and incorporates execution rewards in reinforcement learning\n",
      "Our approach is\n",
      "different in that we do not access table content, which may be unavailable due to privacy concerns.\n",
      "We also do not hand-engineer model architecture for query execution and instead leverage existing\n",
      "database engines to perform efﬁcient query execution\n",
      "Furthermore, in contrast to Dong & Lapata\n",
      "(2016) and Neelakantan et al\n",
      "(2017), we use policy-based RL in a fashion similar to Liang et al.\n",
      "(2017), Mou et al\n",
      "(2017), and Guu et al\n",
      "(2017), which helps Seq2SQL achieve state-of-the-art\n",
      "performance\n",
      "Unlike Mou et al\n",
      "(2017) and Yin et al\n",
      "(2016), we generalize across natural language\n",
      "questions and table schemas instead of across synthetic questions on a single schema.\n",
      "Natural language interface for databases\n",
      "One prominent works in natural language interfaces is\n",
      "PRECISE (Popescu et al., 2003), which translates questions to SQL queries and identiﬁes questions\n",
      "that it is not conﬁdent about\n",
      "Giordani & Moschitti (2012) translate questions to SQL by ﬁrst\n",
      "generating candidate queries from a grammar then ranking them using tree kernels\n",
      "Both of these\n",
      "approaches rely on high quality grammar and are not suitable for tasks that require generalization\n",
      "to new schema.\n",
      "Iyer et al\n",
      "(2017) also translate to SQL, but with a Seq2Seq model that is further\n",
      "improved with human feedback\n",
      "Seq2SQL outperforms Seq2Seq and uses reinforcement learning\n",
      "instead of human feedback during training.\n",
      "\n",
      "6 CONCLUSION\n",
      "We proposed Seq2SQL, a deep neural network for translating questions to SQL queries\n",
      "Our model\n",
      "leverages the structure of SQL queries to reduce the output space of the model\n",
      "To train Seq2SQL,\n",
      "we applied in-the-loop query execution to learn a policy for generating the conditions of the SQL\n",
      "query, which is unordered and unsuitable for optimization via cross entropy loss\n",
      "We also introduced\n",
      "WikiSQL, a dataset of questions and SQL queries that is an order of magnitude larger than compa-\n",
      "rable datasets\n",
      "Finally, we showed that Seq2SQL outperforms a state-of-the-art semantic parser\n",
      "on WikiSQL, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from\n",
      "23.4% to 48.3%.\n",
      "2For simplicity, we deﬁne table schema as the names of the columns in the table.\n",
      "\n",
      "8\n",
      "\n",
      "\f",
      "REFERENCES\n",
      "I\n",
      "Androutsopoulos, G.D\n",
      "Ritchie, and P\n",
      "Thanisch\n",
      "Natural language interfaces to databases - an\n",
      "\n",
      "Yoav Artzi and Luke S\n",
      "Zettlemoyer\n",
      "Bootstrapping semantic parsers from conversations.\n",
      "\n",
      "In\n",
      "\n",
      "introduction\n",
      "1995.\n",
      "\n",
      "EMNLP, 2011.\n",
      "\n",
      "Yoav Artzi and Luke S\n",
      "Zettlemoyer\n",
      "Weakly supervised learning of semantic parsers for mapping\n",
      "\n",
      "instructions to actions\n",
      "TACL, 1:49–62, 2013.\n",
      "\n",
      "Thorsten Beck, Asli Demirg¨uc¸-Kunt, and Ross Levine\n",
      "A new database on the structure and devel-\n",
      "\n",
      "opment of the ﬁnancial sector\n",
      "The World Bank Economic Review, 14(3):597–605, 2000.\n",
      "\n",
      "Irwan Bello, Hieu Pham, Quoc V\n",
      "Le, Mohammad Norouzi, and Samy Bengio\n",
      "Neural combinatorial\n",
      "\n",
      "optimization with reinforcement learning\n",
      "ICLR, 2017.\n",
      "\n",
      "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang\n",
      "Semantic parsing on freebase from\n",
      "\n",
      "question-answer pairs\n",
      "In EMNLP, 2013.\n",
      "\n",
      "Chandra Bhagavatula, Thanapon Noraset, and Doug Downey\n",
      "Methods for exploring and mining\n",
      "\n",
      "tables on wikipedia\n",
      "In IDEA@KDD, 2013.\n",
      "\n",
      "Qingqing Cai and Alexander Yates\n",
      "Large-scale semantic parsing via schema matching and lexicon\n",
      "\n",
      "extension\n",
      "In ACL, 2013.\n",
      "\n",
      "Li Dong and Mirella Lapata\n",
      "Language to logical form with neural attention\n",
      "ACL, 2016.\n",
      "\n",
      "Alessandra Giordani and Alessandro Moschitti\n",
      "Translating questions to SQL queries with genera-\n",
      "\n",
      "tive parsers discriminatively reranked\n",
      "In COLING, 2012.\n",
      "\n",
      "Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O\n",
      "K\n",
      "Li.\n",
      "\n",
      "Incorporating copying mechanism in\n",
      "\n",
      "sequence-to-sequence learning\n",
      "ACL, 2016.\n",
      "\n",
      "Kelvin Guu, Panupong Pasupat, Evan Zheran Liu, and Percy Liang\n",
      "From language to programs:\n",
      "\n",
      "Bridging reinforcement learning and maximum marginal likelihood\n",
      "In ACL, 2017.\n",
      "\n",
      "Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher\n",
      "A Joint Many-Task\n",
      "\n",
      "Model: Growing a Neural Network for Multiple NLP Tasks\n",
      "arXiv, cs.CL 1611.01587, 2016.\n",
      "\n",
      "Richard Hillestad, James Bigelow, Anthony Bower, Federico Girosi, Robin Meili, Richard Scoville,\n",
      "and Roger Taylor\n",
      "Can electronic medical record systems transform health care? potential health\n",
      "beneﬁts, savings, and costs\n",
      "Health affairs, 24(5):1103–1117, 2005.\n",
      "\n",
      "Sepp Hochreiter and Jurgen Schmidhuber\n",
      "Long short-term memory\n",
      "Neural computation, 1997.\n",
      "\n",
      "Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant Krishnamurthy, and Luke Zettlemoyer.\n",
      "\n",
      "Learning a neural semantic parser from user feedback\n",
      "In ACL, 2017.\n",
      "\n",
      "Diederik P\n",
      "Kingma and Jimmy Ba\n",
      "Adam: A method for stochastic optimization.\n",
      "\n",
      "arXiv,\n",
      "\n",
      "abs/1412.6980, 2014.\n",
      "\n",
      "Chen Liang, Jonathan Berant, Quoc V\n",
      "Le, Ken Forbus, and Ni Lao\n",
      "Neural symbolic machines:\n",
      "\n",
      "Learning semantic parsers on freebase with weak supervision\n",
      "In ACL, 2017.\n",
      "\n",
      "Percy Liang, Michael I\n",
      "Jordan, and Dan Klein\n",
      "Learning dependency-based compositional seman-\n",
      "\n",
      "tics\n",
      "Computational Linguistics, 39:389–446, 2011.\n",
      "\n",
      "Christopher D\n",
      "Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J\n",
      "Bethard, and David\n",
      "McClosky\n",
      "The Stanford CoreNLP natural language processing toolkit\n",
      "In Association for Com-\n",
      "putational Linguistics (ACL) System Demonstrations, pp\n",
      "55–60, 2014.\n",
      "\n",
      "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher\n",
      "Pointer sentinel mixture\n",
      "\n",
      "models\n",
      "ICLR, 2017.\n",
      "\n",
      "9\n",
      "\n",
      "\f",
      "Lili Mou, Zhengdong Lu, Hang Li, and Zhi Jin\n",
      "Coupling distributed and symbolic execution for\n",
      "\n",
      "natural language queries\n",
      "In ICML, 2017.\n",
      "\n",
      "Arvind Neelakantan, Quoc V\n",
      "Le, Mart´ın Abadi, Andrew McCallum, and Dario Amodei\n",
      "Learning\n",
      "\n",
      "a natural language interface with neural programmer\n",
      "In ICLR, 2017.\n",
      "\n",
      "Eric WT Ngai, Li Xiu, and Dorothy CK Chau\n",
      "Application of data mining techniques in customer\n",
      "relationship management: A literature review and classiﬁcation\n",
      "Expert systems with applications,\n",
      "36(2):2592–2602, 2009.\n",
      "\n",
      "Panupong Pasupat and Percy Liang\n",
      "Compositional semantic parsing on semi-structured tables\n",
      "In\n",
      "\n",
      "ACL, 2015.\n",
      "\n",
      "Jeffrey Pennington, Richard Socher, and Christopher D\n",
      "Manning\n",
      "Glove: Global vectors for word\n",
      "\n",
      "representation\n",
      "In EMNLP, 2014.\n",
      "\n",
      "Ana-Maria Popescu, Oren Etzioni, and Henry Kautz\n",
      "Towards a theory of natural language interfaces\n",
      "to databases\n",
      "In Proceedings of the 8th International Conference on Intelligent User Interfaces,\n",
      "pp\n",
      "149–157\n",
      "ACM, 2003.\n",
      "\n",
      "Patti J\n",
      "Price\n",
      "Evaluation of spoken language systems: The ATIS domain\n",
      "1990.\n",
      "Siva Reddy, Mirella Lapata, and Mark Steedman\n",
      "Large-scale semantic parsing without question-\n",
      "\n",
      "answer pairs\n",
      "TACL, 2:377–392, 2014.\n",
      "\n",
      "John Schulman, Nicolas Heess, Theophane Weber, and Pieter Abbeel\n",
      "Gradient estimation using\n",
      "\n",
      "stochastic computation graphs\n",
      "In NIPS, 2015.\n",
      "\n",
      "Min Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi\n",
      "Bidirectional attention\n",
      "\n",
      "ﬂow for machine comprehension\n",
      "ICLR, 2017.\n",
      "\n",
      "Nitish Srivastava, Geoffrey E\n",
      "Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\n",
      "Dropout: a simple way to prevent neural networks from overﬁtting\n",
      "Journal of Machine Learning\n",
      "Research, 15:1929–1958, 2014.\n",
      "\n",
      "Janez Starc and Dunja Mladenic\n",
      "Joint learning of ontology and semantic parser from text\n",
      "Intelli-\n",
      "\n",
      "gent Data Analysis, 21:19–38, 2017.\n",
      "\n",
      "Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour\n",
      "Policy gradient\n",
      "methods for reinforcement learning with function approximation\n",
      "In Advances in neural informa-\n",
      "tion processing systems, pp\n",
      "1057–1063, 2000.\n",
      "\n",
      "Lappoon R\n",
      "Tang and Raymond J\n",
      "Mooney\n",
      "Using multiple clause constructors in inductive logic\n",
      "\n",
      "programming for semantic parsing\n",
      "In ECML, 2001.\n",
      "\n",
      "Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly\n",
      "Pointer networks\n",
      "In NIPS, 2015.\n",
      "Yushi Wang, Jonathan Berant, and Percy Liang\n",
      "Building a semantic parser overnight.\n",
      "\n",
      "In ACL,\n",
      "\n",
      "2015.\n",
      "\n",
      "Yuk Wah Wong and Raymond J\n",
      "Mooney\n",
      "Learning synchronous grammars for semantic parsing\n",
      "\n",
      "with lambda calculus\n",
      "In ACL, 2007.\n",
      "\n",
      "Caiming Xiong, Victor Zhong, and Richard Socher\n",
      "Dynamic coattention networks for question\n",
      "\n",
      "Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao\n",
      "Neural enquirer: Learning to query tables.\n",
      "\n",
      "answering\n",
      "ICLR, 2017.\n",
      "\n",
      "In ACL, 2016.\n",
      "\n",
      "John M\n",
      "Zelle and Raymond J\n",
      "Mooney\n",
      "Learning to parse database queries using inductive logic\n",
      "\n",
      "programming\n",
      "In AAAI/IAAI, Vol\n",
      "2, 1996.\n",
      "\n",
      "Luke S\n",
      "Zettlemoyer and Michael Collins\n",
      "Learning to map sentences to logical form: Structured\n",
      "In Uncertainty in Artiﬁcial Intelligence,\n",
      "\n",
      "classiﬁcation with probabilistic categorial grammars.\n",
      "2005.\n",
      "\n",
      "Luke S\n",
      "Zettlemoyer and Michael Collins\n",
      "Online learning of relaxed ccg grammars for parsing to\n",
      "\n",
      "logical form\n",
      "In EMNLP-CoNLL, 2007.\n",
      "\n",
      "10\n",
      "\n",
      "\f",
      "A COLLECTION OF WIKISQL\n",
      "\n",
      "WikiSQL is collected in a paraphrase phases as well as a veriﬁcation phase.\n",
      "In the paraphrase\n",
      "phase, we use tables extracted from Wikipedia by Bhagavatula et al\n",
      "(2013) and remove small tables\n",
      "according to the following criteria:\n",
      "\n",
      "• the number of cells in each row is not the same\n",
      "• the content in a cell exceed 50 characters\n",
      "• a header cell is empty\n",
      "• the table has less than 5 rows or 5 columns\n",
      "• over 40% of the cells of a row contain identical content\n",
      "\n",
      "We also remove the last row of a table because a large quantity of HTML tables tend to have sum-\n",
      "mary statistics in the last row, and hence the last row does not adhere to the table schema deﬁned by\n",
      "the header row.\n",
      "For each of the table that passes the above criteria, we randomly generate 6 SQL queries according\n",
      "to the following rules:\n",
      "\n",
      "• the query follows the format SELECT agg op agg col from table where\n",
      "\n",
      "cond1 col cond1 op cond1 AND cond2 col cond2 op cond2 ...\n",
      "\n",
      "• the aggregation operator agg op can be empty or COUNT\n",
      "In the event that the aggregation\n",
      "\n",
      "column agg col is numeric, agg op can additionally be one of MAX and MIN\n",
      "\n",
      "• the condition operator cond op is =\n",
      "In the event that the corresponding condition column\n",
      "\n",
      "cond col is numeric, cond op can additionally be one of > and <\n",
      "\n",
      "• the condition cond can be any possible value present in the table under the corresponding\n",
      "cond col\n",
      "In the event that cond col is numerical, cond can be any numerical value\n",
      "sampled from the range from the minimum value in the column to the maximum value in\n",
      "the column.\n",
      "\n",
      "We only generate queries that produce a non-empty result set\n",
      "To enforce succinct queries, we\n",
      "remove conditions from the generated queries if doing so does not change the execution result.\n",
      "For each query, we generate a crude question using a template and obtain a human paraphrase via\n",
      "crowdsourcing on Amazon Mechanical Turk\n",
      "In each Amazon Mechanical Turk HIT, a worker is\n",
      "shown the ﬁrst 4 rows of the table as well as its generated questions and asked to paraphrase each\n",
      "question.\n",
      "After obtaining natural language utterances from the paraphrase phase, we give each question-\n",
      "paraphrase pair to two other workers in the veriﬁcation phase to verify that the paraphrase and\n",
      "the original question contain the same meaning.\n",
      "We then ﬁlter the initial collection of paraphrases using the following criteria:\n",
      "\n",
      "• the paraphrase must be deemed correct by at least one worker during the veriﬁcation phrase\n",
      "• the paraphrase must be sufﬁciently different from the generated question, with a character-\n",
      "\n",
      "level edit distance greater than 10\n",
      "\n",
      "B ATTENTIONAL SEQ2SEQ NEURAL SEMANTIC PARSER BASELINE\n",
      "\n",
      "We employ the attentional sequence to sequence model for the baseline\n",
      "This model by Dong &\n",
      "Lapata (2016) achieves state of the art results on a variety of semantic parsing datasets despite not\n",
      "using hand-engineered grammar\n",
      "We implement a variant using OpenNMT and a global attention\n",
      "encoder-decoder architecture (with input feeding) described by Luong et al.\n",
      "\n",
      "11\n",
      "\n",
      "\f",
      "We use the same two-layer, bidirectional, stacked LSTM encoder as described previously\n",
      "The\n",
      "decoder is almost identical to that described by Equation 2 of the paper, with the sole difference\n",
      "coming from input feeding.\n",
      "\n",
      "gs = LSTM(cid:0)(cid:2)emb (ys−1) ; κdec\n",
      "\n",
      "s−1\n",
      "\n",
      "(cid:3) , gs−1\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "where κdec\n",
      "as\n",
      "\n",
      "s\n",
      "\n",
      "is the attentional context over the input sequence during the sth decoding step, computed\n",
      "\n",
      "(cid:0)W dechenc\n",
      "\n",
      "t\n",
      "\n",
      "s = softmax(cid:0)αdec\n",
      "\n",
      "s\n",
      "\n",
      "βdec\n",
      "\n",
      "(cid:1)\n",
      "\n",
      "αdec\n",
      "s,t = hdec\n",
      "\n",
      "s\n",
      "\n",
      "(cid:1)(cid:124)\n",
      "(cid:88)\n",
      "\n",
      "t\n",
      "\n",
      "κs =\n",
      "\n",
      "βs,thenc\n",
      "\n",
      "t\n",
      "\n",
      "To produce the output token during the sth decoder step, the concatenation of the decoder state and\n",
      "the attention context is given to a ﬁnal linear layer to produce a distribution αdec over words in the\n",
      "target vocabulary\n",
      "\n",
      "αdec = softmax(cid:0)U dec[hdec\n",
      "\n",
      "; κdec\n",
      "\n",
      "s\n",
      "\n",
      "s\n",
      "\n",
      "](cid:1)\n",
      "\n",
      "During training, teacher forcing is used\n",
      "During inference, a beam size of 5 is used and generated\n",
      "unknown words are replaced by the input words with the highest attention weight.\n",
      "\n",
      "(11)\n",
      "\n",
      "(12)\n",
      "\n",
      "(13)\n",
      "\n",
      "(14)\n",
      "\n",
      "C PREDICTIONS BY SEQ2SQL\n",
      "\n",
      "Q\n",
      "P\n",
      "S’\n",
      "S\n",
      "G\n",
      "\n",
      "Q\n",
      "P\n",
      "S’\n",
      "S\n",
      "G\n",
      "\n",
      "Q\n",
      "P\n",
      "S’\n",
      "S\n",
      "G\n",
      "\n",
      "Q\n",
      "P\n",
      "S’\n",
      "S\n",
      "G\n",
      "\n",
      "Q\n",
      "P\n",
      "S’\n",
      "S\n",
      "G\n",
      "\n",
      "Q\n",
      "P\n",
      "S’\n",
      "S\n",
      "G\n",
      "\n",
      "when connecticut & villanova are the regular season winner how many tournament venues (city) are there?\n",
      "SELECT COUNT tournament player (city) WHERE regular season winner city ) = connecticut & villanova\n",
      "SELECT COUNT tournament venue (city) WHERE tournament winner = connecticut & villanova\n",
      "SELECT COUNT tournament venue (city) WHERE regular season winner = connecticut & villanova\n",
      "SELECT COUNT tournament venue (city) WHERE regular season winner = connecticut & villanova\n",
      "\n",
      "what are the aggregate scores of those races where the ﬁrst leg results are 0-1?\n",
      "SELECT aggregate WHERE 1st .\n",
      "SELECT COUNT agg.\n",
      "SELECT agg.\n",
      "SELECT agg.\n",
      "\n",
      "score WHERE 1st leg = 0-1\n",
      "score WHERE 1st leg = 0-1\n",
      "\n",
      "score WHERE 1st leg = 0-1\n",
      "\n",
      "= 0-1\n",
      "\n",
      "what is the race name of the 12th round trenton, new jersey race where a.j\n",
      "foyt had the pole position?\n",
      "SELECT race name WHERE location = 12th AND round position = a.j.\n",
      "SELECT race name WHERE rnd = 12 AND track = a.j\n",
      "foyt AND pole position = a.j.\n",
      "SELECT race name WHERE rnd = 12 AND pole position = a.j\n",
      "foyt\n",
      "SELECT race name WHERE rnd = 12 AND pole position = a.j\n",
      "foyt\n",
      "\n",
      "foyt, new jersey AND\n",
      "\n",
      "foyt\n",
      "\n",
      "what city is on 89.9?\n",
      "SELECT city WHERE frequency = 89.9\n",
      "SELECT city of license WHERE frequency = 89.9\n",
      "SELECT city of license WHERE frequency = 89.9\n",
      "SELECT city of license WHERE frequency = 89.9\n",
      "\n",
      "how many voters from the bronx voted for the socialist party?\n",
      "SELECT MIN % party = socialist\n",
      "SELECT COUNT the bronx where the bronx = socialist\n",
      "SELECT COUNT the bronx WHERE the bronx = socialist\n",
      "SELECT the bronx WHERE party = socialist\n",
      "\n",
      "in what year did a plymouth vehicle win on february 9 ?\n",
      "SELECT MIN year (km) WHERE date = february 9 AND race time = plymouth 9\n",
      "SELECT year (km) WHERE date = plymouth 9 AND race time = february 9\n",
      "SELECT year (km) WHERE date = plymouth 9 AND race time= february 9\n",
      "SELECT year (km) WHERE manufacturer = plymouth AND date = february 9\n",
      "\n",
      "Table 4: Examples predictions by the models on the dev split\n",
      "Q denotes the natural language\n",
      "question and G denotes the corresponding ground truth query\n",
      "P, S’, and S denote, respectively, the\n",
      "queries produced by the Augmented Pointer Network, Seq2SQL without reinforcement learning,\n",
      "Seq2SQL\n",
      "We omit the FROM table part of the query for succinctness.\n",
      "\n",
      "12\n",
      "\n",
      "\f",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are  [(0.009775413283603927, ['By', 'leveraging', 'the', 'inherent', 'structure', 'of\\nSQL', 'queries', 'and', 'applying', 'policy', 'gradient', 'methods', 'using', 'reward', 'signals', 'from', 'live', 'query', 'execution,\\nSeq2SQL', 'achieves', 'state-of-the-art', 'performance', 'on', 'WikiSQL,', 'obtaining', '59.4%', 'execution', 'accuracy.\\n\\n2', 'MODEL\\n\\nThe', 'WikiSQL', 'task', 'is', 'to', 'generate', 'a', 'SQL', 'query', 'from', 'a', 'natural', 'language', 'question', 'and', 'table', 'schema.\\nOur', 'baseline', 'model', 'is', 'the', 'attentional', 'sequence', 'to', 'sequence', 'neural', 'semantic', 'parser', 'proposed', 'by', 'Dong\\n&', 'Lapata', '(2016)', 'that', 'achieves', 'state-of-the-art', 'performance', 'on', 'a', 'host', 'of', 'semantic', 'parsing', 'datasets\\nwithout', 'using', 'hand-engineered', 'grammar']), (0.007833918231926527, ['With', 'this', 'augmented', 'input', 'sequence,', 'the', 'pointer', 'network\\ncan', 'produce', 'the', 'SQL', 'query', 'by', 'selecting', 'exclusively', 'from', 'the', 'input.\\nSuppose', 'we', 'have', 'a', 'list', 'of', 'N', 'table', 'columns', 'and', 'a', 'question', 'such', 'as', 'in', 'Figure', '2,', 'and', 'want', 'to', 'produce\\nthe', 'corresponding', 'SQL', 'query']), (0.00750994581874336, ['Next,', 'the', 'decoder', 'produces', 'a', 'scalar', 'attention', 'score', 'αptr\\ns,t', 'for', 'each', 'position', 't', 'of', 'the', 'input\\nsequence:\\n\\nt\\n\\ns,t', '=', 'W', 'ptrtanh(cid:0)U', 'ptrgs', '+', 'V', 'ptrht\\n\\n(cid:1)\\n\\nαptr\\n\\n(2)\\n\\nWe', 'choose', 'the', 'input', 'token', 'with', 'the', 'highest', 'score', 'as', 'the', 'next', 'token', 'of', 'the', 'generated', 'SQL', 'query,\\nys', '=', 'argmax(αptr\\n\\ns', ').\\n\\n2.2', 'SEQ2SQL\\n\\nWhile', 'the', 'augmented', 'pointer', 'network', 'can\\nsolve', 'the', 'SQL', 'generation', 'problem,', 'it', 'does\\nnot', 'leverage', 'the', 'structure', 'inherent', 'in', 'SQL.\\nTypically,', 'a', 'SQL', 'query', 'such', 'as', 'that', 'shown\\nin', 'Figure', '3', 'consists', 'of', 'three', 'components.\\nThe', 'ﬁrst', 'component', 'is', 'the', 'aggregation', 'op-\\nerator,\\nin', 'this', 'case', 'COUNT,', 'which', 'pro-\\nduces', 'a', 'summary', 'of', 'the', 'rows', 'selected', 'by\\nthe', 'query']), (0.007231223067216413, ['We', 'form', 'the', 'generated', 'question', 'using', 'a', 'template,\\nﬁlled', 'using', 'a', 'randomly', 'generated', 'SQL', 'query']), (0.006992502495990472, ['Along', 'with', 'WikiSQL,', 'we', 'release', 'a\\nquery', 'execution', 'engine', 'for', 'the', 'database', 'used', 'for', 'in-the-loop', 'query', 'execution', 'to', 'learn', 'the', 'policy.\\nOn', 'WikiSQL,', 'Seq2SQL', 'outperforms', 'a', 'previously', 'state-of-the-art', 'semantic', 'parsing', 'model', 'by', 'Dong\\n\\n1\\n\\n\\x0cFigure', '1:', 'Seq2SQL', 'takes', 'as', 'input', 'a', 'question', 'and', 'the', 'columns', 'of', 'a', 'table']), (0.006925445226640938, ['Leveraging', 'the', 'structure', 'of', 'SQL', 'queries', 'leads', 'to', 'another', 'improvement', 'of', '3.8%,', 'as', 'is\\nshown', 'by', 'the', 'performance', 'of', 'Seq2SQL', 'without', 'RL', 'compared', 'to', 'the', 'augmented', 'pointer', 'network.\\nFinally,', 'training', 'using', 'reinforcement', 'learning', 'based', 'on', 'rewards', 'from', 'in-the-loop', 'query', 'executions\\non', 'a', 'database', 'leads', 'to', 'another', 'performance', 'increase', 'of', '2.3%,', 'as', 'is', 'shown', 'by', 'the', 'performance', 'of', 'the\\nfull', 'Seq2SQL', 'model.\\n\\n4.2', 'ANALYSIS\\nLimiting', 'the', 'output', 'space', 'via', 'pointer', 'network', 'leads', 'to', 'more', 'accurate', 'conditions']), (0.006875293546499729, ['Utilizing', 'the', 'structure', 'of', 'SQL', 'allows', 'Seq2SQL', 'to', 'further', 'prune\\nthe', 'output', 'space', 'of', 'queries,', 'which', 'leads', 'to', 'higher', 'performance', 'than', 'Seq2Seq', 'and', 'the', 'augmented\\npointer', 'network.\\n\\nFigure', '3:', 'The', 'Seq2SQL', 'model', 'has', 'three', 'components,\\ncorresponding', 'to', 'the', 'three', 'parts', 'of', 'a', 'SQL', 'query', '(right).\\nThe', 'input', 'to', 'the', 'model', 'are', 'the', 'question', '(top', 'left)', 'and\\nthe', 'table', 'column', 'names', '(bottom', 'left).\\n\\nAggregation', 'Operation']), (0.0066713475235238495, ['In', 'addition,', 'WikiSQL', 'focuses', 'on', 'generating', 'SQL', 'queries', 'for', 'questions\\nover', 'relational', 'database', 'tables', 'and', 'only', 'uses', 'table', 'content', 'during', 'evaluation.\\nRepresentation', 'learning', 'for', 'sequence', 'generation']), (0.006490590323980405, ['We', 'propose', 'Seq2SQL,', 'a', 'deep', 'neural', 'network', 'for', 'translating', 'natural', 'lan-\\nguage', 'questions', 'to', 'corresponding', 'SQL', 'queries']), (0.0064099626276541855, ['Finally,', 'the\\nnetwork', 'generates', 'the', 'conditions', 'for', 'the', 'query', 'using', 'a', 'pointer', 'network']), (0.006233485453459825, ['Other', 'interesting', 'neural', 'semantic', 'parsing', 'models', 'are\\nthe', 'Neural', 'Programmer', '(Neelakantan', 'et', 'al.,', '2017)', 'and', 'the', 'Neural', 'Enquirer', '(Yin', 'et', 'al.,', '2016)']), (0.006177287325716855, ['By', 'applying', 'policy-\\nbased', 'reinforcement', 'learning', 'with', 'a', 'query', 'execution', 'environment', 'to', 'WikiSQL,\\nSeq2SQL', 'outperforms', 'a', 'state-of-the-art', 'semantic', 'parser,', 'improving', 'execution', 'ac-\\ncuracy', 'from', '35.9%', 'to', '59.4%', 'and', 'logical', 'form', 'accuracy', 'from', '23.4%', 'to', '48.3%.\\n\\n1\\n\\nINTRODUCTION\\n\\nRelational', 'databases', 'store', 'a', 'vast', 'amount', 'of', 'today’s', 'information', 'and', 'provide', 'the', 'foundation', 'of', 'ap-\\nplications', 'such', 'as', 'medical', 'records', '(Hillestad', 'et', 'al.,', '2005),', 'ﬁnancial', 'markets', '(Beck', 'et', 'al.,', '2000),\\nand', 'customer', 'relations', 'management', '(Ngai', 'et', 'al.,', '2009)']), (0.006112446822884964, ['First,', 'we', 'introduce', 'Seq2SQL,', 'a', 'deep', 'neural\\nnetwork', 'for', 'translating', 'natural', 'language', 'questions', 'to', 'corresponding', 'SQL', 'queries']), (0.006106464824986034, ['These', 'characteristics', 'allow\\nSeq2SQL', 'to', 'achieve', 'state-of-the-art', 'results', 'on', 'query', 'generation.\\nNext,', 'we', 'release', 'WikiSQL,', 'a', 'corpus', 'of', '80654', 'hand-annotated', 'instances', 'of', 'natural', 'language', 'ques-\\ntions,', 'SQL', 'queries,', 'and', 'SQL', 'tables', 'extracted', 'from', '24241', 'HTML', 'tables', 'from', 'Wikipedia']), (0.0060850122719830054, ['One', 'prominent', 'works', 'in', 'natural', 'language', 'interfaces', 'is\\nPRECISE', '(Popescu', 'et', 'al.,', '2003),', 'which', 'translates', 'questions', 'to', 'SQL', 'queries', 'and', 'identiﬁes', 'questions\\nthat', 'it', 'is', 'not', 'conﬁdent', 'about']), (0.006014637441008875, ['Seq2SQL', 'outperforms', 'Seq2Seq', 'and', 'uses', 'reinforcement', 'learning\\ninstead', 'of', 'human', 'feedback', 'during', 'training.\\n\\n6', 'CONCLUSION\\nWe', 'proposed', 'Seq2SQL,', 'a', 'deep', 'neural', 'network', 'for', 'translating', 'questions', 'to', 'SQL', 'queries']), (0.006001756032184053, ['In', 'order', 'to', 'obtain', 'the', 'rewards', 'described', 'in\\nSection', '2.2,', 'we', 'use', 'the', 'query', 'execution', 'engine', 'described', 'in', 'Section', '3.\\n\\n4.1', 'RESULT\\nWe', 'compare', 'results', 'against', 'the', 'attentional', 'sequence', 'to', 'sequence', 'neural', 'semantic', 'parser', 'proposed\\nby', 'Dong', '&', 'Lapata', '(2016)']), (0.005986424741743987, ['Dong', '&', 'Lapata', '(2016)’s', 'attentional', 'sequence\\nto', 'sequence', 'neural', 'semantic', 'parser,', 'which', 'we', 'use', 'as', 'the', 'baseline,', 'achieves', 'state-of-the-art', 'results\\non', 'a', 'variety', 'of', 'semantic', 'parsing', 'datasets', 'despite', 'not', 'utilizing', 'hand-engineered', 'grammar']), (0.005672525653657547, ['In', 'each', 'Amazon', 'Mechanical', 'Turk', 'HIT,', 'a', 'worker', 'is\\nshown', 'the', 'ﬁrst', '4', 'rows', 'of', 'the', 'table', 'as', 'well', 'as', 'its', 'generated', 'questions', 'and', 'asked', 'to', 'paraphrase', 'each\\nquestion.\\nAfter', 'obtaining', 'natural', 'language', 'utterances', 'from', 'the', 'paraphrase', 'phase,', 'we', 'give', 'each', 'question-\\nparaphrase', 'pair', 'to', 'two', 'other', 'workers', 'in', 'the', 'veriﬁcation', 'phase', 'to', 'verify', 'that', 'the', 'paraphrase', 'and\\nthe', 'original', 'question', 'contain', 'the', 'same', 'meaning.\\nWe', 'then', 'ﬁlter', 'the', 'initial', 'collection', 'of', 'paraphrases', 'using', 'the', 'following', 'criteria:\\n\\n•', 'the', 'paraphrase', 'must', 'be', 'deemed', 'correct', 'by', 'at', 'least', 'one', 'worker', 'during', 'the', 'veriﬁcation', 'phrase\\n•', 'the', 'paraphrase', 'must', 'be', 'sufﬁciently', 'different', 'from', 'the', 'generated', 'question,', 'with', 'a', 'character-\\n\\nlevel', 'edit', 'distance', 'greater', 'than', '10\\n\\nB', 'ATTENTIONAL', 'SEQ2SEQ', 'NEURAL', 'SEMANTIC', 'PARSER', 'BASELINE\\n\\nWe', 'employ', 'the', 'attentional', 'sequence', 'to', 'sequence', 'model', 'for', 'the', 'baseline']), (0.0056457742383482, ['Pointer', 'mod-\\nels', 'have', 'also', 'been', 'successfully', 'applied', 'to', 'tasks', 'such', 'as', 'language', 'modeling', '(Merity', 'et', 'al.,', '2017),\\nsummarization', '(Gu', 'et', 'al.,', '2016),', 'combinatorial', 'optimization', '(Bello', 'et', 'al.,', '2017),', 'and', 'question', 'an-\\nswering', '(Seo', 'et', 'al.,', '2017;', 'Xiong', 'et', 'al.,', '2017)']), (0.0056422365367499346, ['We', 'evaluate', 'using', 'the', 'execution\\naccuracy', 'metric', 'Accex', '=', 'Nex\\nN', '.\\nOne\\ndownside', 'of', 'Accex', 'is', 'that\\nis', 'possible\\nto', 'construct', 'a', 'SQL', 'query', 'that', 'does', 'not\\ncorrespond', 'to', 'the', 'question', 'but', 'neverthe-\\nless', 'obtains', 'the', 'same', 'result.\\nFor', 'exam-\\nple,', 'the', 'two', 'queries', 'SELECT', 'COUNT(name)\\nWHERE', 'SSN', '=', '123', 'and', 'SELECT', 'COUNT(SSN)', 'WHERE', 'SSN', '=', '123', 'produce', 'the', 'same', 're-\\nsult', 'if', 'no', 'two', 'people', 'with', 'different', 'names', 'share', 'the', 'SSN', '123']), (0.005613084268135247, ['We', 'ﬁrst', 'describe\\nthe', 'augmented', 'pointer', 'network', 'model,', 'then', 'address', 'its', 'limitations', 'in', 'our', 'deﬁnition', 'of', 'Seq2SQL,\\nparticularly', 'with', 'respect', 'to', 'generating', 'unordered', 'query', 'conditions.\\n\\n2.1', 'AUGMENTED', 'POINTER', 'NETWORK\\n\\nThe', 'augmented', 'pointer', 'network', 'generates', 'the', 'SQL', 'query', 'token-by-token', 'by', 'selecting', 'from', 'an', 'input\\nsequence']), (0.005588510997507114, ['Let', 'q', '(y)', 'denote', 'the', 'query', 'generated', 'by', 'the', 'model', 'and', 'qg', 'denote', 'the\\nground', 'truth', 'query', 'corresponding', 'to', 'the', 'question']), (0.005583619899551793, ['We', 'train', 'Seq2SQL', 'using', 'a', 'mixed', 'objective,', 'combining', 'cross', 'entropy\\nlosses', 'and', 'RL', 'rewards', 'from', 'in-the-loop', 'query', 'execution', 'on', 'a', 'database']), (0.005546750414519003, ['In', 'addition', 'to', 'the', 'raw', 'tables,', 'queries,\\nresults,', 'and', 'natural', 'utterances,', 'we', 'also', 'release\\na', 'corresponding', 'SQL', 'database', 'and', 'query', 'exe-\\ncution', 'engine.\\n\\nDataset\\nWikiSQL\\nGeoquery\\nATIS\\nFreebase917\\nOvernight\\nWebQuestions\\nWikiTableQuestions\\n\\nSize', 'LF\\nyes\\nyes\\nyes*\\nyes\\nyes\\nno\\nno\\n\\n80654\\n880\\n5871\\n917\\n26098\\n5810\\n22033\\n\\nresult\\n\\nThe', 'datasets\\n\\n(Tang', '&', 'Mooney,\\n\\nin', 'the', 'dataset,', 'Nex\\n\\n3.1', 'EVALUATION\\nLet', 'N', 'denote', 'the', 'total', 'number', 'of', 'ex-\\namples\\nthe', 'number\\nof', 'queries', 'that,', 'when', 'executed,\\nin\\nthe', 'correct', 'result,', 'and', 'Nlf\\nthe', 'number', 'of\\nqueries', 'has', 'exact\\nstring', 'match', 'with', 'the\\nground', 'truth', 'query', 'used', 'to', 'collect', 'the', 'para-\\nphrase']), (0.005525318953657118, ['Neural', 'enquirer:', 'Learning', 'to', 'query', 'tables.\\n\\nanswering']), (0.005511949924031277, ['foyt.\\n\\n5', 'RELATED', 'WORK\\nSemantic', 'Parsing.\\nIn', 'semantic', 'parsing', 'for', 'question', 'answering', '(QA),', 'natural', 'language', 'questions\\nare', 'parsed', 'into', 'logical', 'forms', 'that', 'are', 'then', 'executed', 'on', 'a', 'knowledge', 'graph', '(Zelle', '&', 'Mooney,', '1996;\\n\\n7\\n\\n\\x0cWong', '&', 'Mooney,', '2007;', 'Zettlemoyer', '&', 'Collins,', '2005;', '2007)']), (0.005498417079847656, ['Learning', 'to', 'parse', 'database', 'queries', 'using', 'inductive', 'logic\\n\\nprogramming']), (0.005334407894487353, ['Giordani', '&', 'Moschitti', '(2012)', 'translate', 'questions', 'to', 'SQL', 'by', 'ﬁrst\\ngenerating', 'candidate', 'queries', 'from', 'a', 'grammar', 'then', 'ranking', 'them', 'using', 'tree', 'kernels']), (0.005311516425516833, ['We', 'deﬁne', 'the', 'reward', 'R', '(q', '(y)', ',', 'qg)', 'as\\n\\nR', '(q', '(y)', ',', 'qg)', '=\\n\\nif', 'q', '(y)', 'is', 'not', 'a', 'valid', 'SQL', 'query\\nif', 'q', '(y)', 'is', 'a', 'valid', 'SQL', 'query', 'and', 'executes', 'to', 'an', 'incorrect', 'result\\nif', 'q', '(y)', 'is', 'a', 'valid', 'SQL', 'query', 'and', 'executes', 'to', 'the', 'correct', 'result\\n\\n(7)\\n\\n\\uf8f1\\uf8f2\\uf8f3−2,\\n\\n−1,\\n+1,\\n\\n4\\n\\n\\x0cThe', 'loss,', 'Lwhe', '=', '−Ey[R', '(q', '(y)', ',', 'qg)],', 'is', 'the', 'negative', 'expected', 'reward', 'over', 'possible', 'WHERE', 'clauses.\\nWe', 'derive', 'the', 'policy', 'gradient', 'for', 'Lwhe', 'as', 'shown', 'by', 'Sutton', 'et', 'al']), (0.005292678119280557, ['(2017),', 'we', 'use', 'policy-based', 'RL', 'in', 'a', 'fashion', 'similar', 'to', 'Liang', 'et', 'al.\\n(2017),', 'Mou', 'et', 'al']), (0.005289891949265312, ['Other', 'works', 'in', 'semantic', 'parsing\\nfocus', 'on', 'learning', 'parsers', 'without', 'relying', 'on', 'annotated', 'logical', 'forms', 'by', 'leveraging', 'conversational\\nlogs', '(Artzi', '&', 'Zettlemoyer,', '2011),', 'demonstrations', '(Artzi', '&', 'Zettlemoyer,', '2013),', 'distant', 'supervi-\\nsion', '(Cai', '&', 'Yates,', '2013;', 'Reddy', 'et', 'al.,', '2014),', 'and', 'question-answer', 'pairs', '(Liang', 'et', 'al.,', '2011)']), (0.0052562552636207256, ['Using', 'multiple', 'clause', 'constructors', 'in', 'inductive', 'logic\\n\\nprogramming', 'for', 'semantic', 'parsing']), (0.005254779277754864, ['Finally,', 'we', 'apply', 'a\\nmulti-layer', 'perceptron', 'over', 'the', 'column', 'representations,', 'conditioned', 'on', 'the', 'input', 'representation,', 'to\\ncompute', 'the', 'a', 'score', 'for', 'each', 'column', 'j:\\n\\nj', '=', 'hc\\nec\\n\\nj,Tj\\n\\nj', '=', 'W', 'sel', 'tanh(cid:0)V', 'selκsel', '+', 'V', 'cec\\n\\n(cid:1)\\n\\nαsel\\n\\ncolumns', 'βsel', '=', 'softmax(cid:0)αsel(cid:1)']), (0.005232139729814754, ['Ladouceur28Player29Ottawa', 'RenegadesCFL', 'Team27DBDTConnor', 'HealyHamilton', 'Tiger-CatsAnthony', 'ForgonePick', '#OLPositionTable:', 'CFLDraftQuestion:How', 'many', 'CFL', 'teams', 'are', 'from', 'York', 'College?SELECT', 'COUNT', 'CFL', 'Team', 'FROM', 'CFLDraft', 'WHERE', 'College', '=', '“York”SQL:2Result:\\x0cWe', 'deﬁne', 'the', 'input', 'sequence', 'x', 'as', 'the', 'concatenation', 'of', 'all', 'the', 'column', 'names,', 'the', 'question,', 'and', 'the\\nSQL', 'vocabulary:\\n\\nx', '=', '[<col>;', 'xc\\n\\n1;', 'xc\\n\\n2;', '...;', 'xc\\n\\nN', ';', '<sql>;', 'xs;', '<question>;', 'xq]\\n\\n(1)\\n\\nwhere', '[a;', 'b]', 'denotes', 'the', 'concatenation', 'between', 'the', 'sequences', 'a', 'and', 'b', 'and', 'we', 'add', 'sentinel', 'tokens\\nbetween', 'neighbouring', 'sequences', 'to', 'demarcate', 'the', 'boundaries.\\nThe', 'network', 'ﬁrst', 'encodes', 'x', 'using', 'a', 'two-layer,', 'bidirectional', 'Long', 'Short-Term', 'Memory', 'net-\\nwork', '(Hochreiter', '&', 'Schmidhuber,', '1997)']), (0.005216476379701112, ['We', 'then', 'apply\\na', 'pointer', 'network', 'similar', 'to', 'that', 'proposed', 'by', 'Vinyals', 'et', 'al']), (0.005188461163054099, ['(2017)', 'proposed', 'a', 'distributed', 'neural', 'executor', 'based', 'on', 'the', 'Neural', 'Enquirer,', 'which', 'efﬁciently\\nexecutes', 'queries', 'and', 'incorporates', 'execution', 'rewards', 'in', 'reinforcement', 'learning']), (0.005146217625996635, ['Seq2SQL', 'without', 'RL', 'directly', 'pre-\\ndicts', 'selection', 'and', 'aggregation', 'and', 'reduces\\ninvalid', 'SQL', 'queries', 'generated', 'from', '7.9%', 'to\\n4.8%']), (0.005106717989298263, ['foyt', 'had', 'the', 'pole', 'position?\\nSELECT', 'race', 'name', 'WHERE', 'location', '=', '12th', 'AND', 'round', 'position', '=', 'a.j.\\nSELECT', 'race', 'name', 'WHERE', 'rnd', '=', '12', 'AND', 'track', '=', 'a.j']), (0.00508551524285216, ['We', 'normalize', 'the', 'vector', 'of', 'scores', 'αinp', '=', '[αinp\\n2', ',', '...]', 'to\\n\\n1', ',', 'αinp\\n\\nt\\n\\n3\\n\\nSeq2SQLCOUNTSELECTEngineWHERE', 'Driver', '=', 'Val', 'MusettiHow', 'many', 'engine', 'types', 'did', 'Val', 'Musetti', 'use?EntrantConstructorChassisEngineNoDriverAggregation', 'classiﬁerSELECT', 'column', 'pointerWHERE', 'clause', 'pointer', 'decoder\\x0cproduce', 'a', 'distribution', 'over', 'the', 'input', 'encodings,', 'βinp', '=', 'softmax(cid:0)αinp(cid:1)']), (0.005054918440949777, ['The', 'outputs', 'consist\\nof', 'a', 'ground', 'truth', 'SQL', 'query', 'and', 'the', 'corresponding', 'result', 'from', 'execution.\\n\\n&', 'Lapata', '(2016),', 'which', 'obtains', '35.9%', 'execution', 'accuracy,', 'as', 'well', 'as', 'an', 'augmented', 'pointer', 'net-\\nwork', 'baseline,', 'which', 'obtains', '53.3%', 'execution', 'accuracy']), (0.005035487603138569, ['A', 'sin-\\ngle', 'example', 'in', 'WikiSQL,', 'shown', 'in', 'Figure', '2,\\ncontains', 'a', 'table,', 'a', 'SQL', 'query,', 'and', 'the', 'natu-\\nral', 'language', 'question', 'corresponding', 'to', 'the', 'SQL\\nquery']), (0.005018856008831471, ['The', 'resulting\\nmodel', 'is', 'similar', 'to', 'a', 'pointer', 'network', '(Vinyals', 'et', 'al.,', '2015)', 'with', 'augmented', 'inputs']), (0.005002918851792839, ['foyt', 'whereas', 'Seq2SQL', 'trained', 'with', 'RL', 'correctly', 'generates', 'WHERE', 'rnd', '=', '12', 'AND\\npole', 'position', '=', 'a.j']), (0.004972702773551622, ['Wik-\\niSQL', 'is', 'an', 'order', 'of', 'magnitude', 'larger', 'than', 'previous', 'semantic', 'parsing', 'datasets', 'that', 'provide', 'logi-\\ncal', 'forms', 'along', 'with', 'natural', 'language', 'utterances']), (0.004906067778238758, ['Our', 'model\\nleverages', 'the', 'structure', 'of', 'SQL', 'queries', 'to', 'reduce', 'the', 'output', 'space', 'of', 'the', 'model']), (0.004905974998482492, ['The\\ndecoder', 'is', 'almost', 'identical', 'to', 'that', 'described', 'by', 'Equation', '2', 'of', 'the', 'paper,', 'with', 'the', 'sole', 'difference\\ncoming', 'from', 'input', 'feeding.\\n\\ngs', '=', 'LSTM(cid:0)(cid:2)emb', '(ys−1)', ';', 'κdec\\n\\ns−1\\n\\n(cid:3)', ',', 'gs−1\\n\\n(cid:1)\\n\\nwhere', 'κdec\\nas\\n\\ns\\n\\nis', 'the', 'attentional', 'context', 'over', 'the', 'input', 'sequence', 'during', 'the', 'sth', 'decoding', 'step,', 'computed\\n\\n(cid:0)W', 'dechenc\\n\\nt\\n\\ns', '=', 'softmax(cid:0)αdec\\n\\ns\\n\\nβdec\\n\\n(cid:1)\\n\\nαdec\\ns,t', '=', 'hdec\\n\\ns\\n\\n(cid:1)(cid:124)\\n(cid:88)\\n\\nt\\n\\nκs', '=\\n\\nβs,thenc\\n\\nt\\n\\nTo', 'produce', 'the', 'output', 'token', 'during', 'the', 'sth', 'decoder', 'step,', 'the', 'concatenation', 'of', 'the', 'decoder', 'state', 'and\\nthe', 'attention', 'context', 'is', 'given', 'to', 'a', 'ﬁnal', 'linear', 'layer', 'to', 'produce', 'a', 'distribution', 'αdec', 'over', 'words', 'in', 'the\\ntarget', 'vocabulary\\n\\nαdec', '=', 'softmax(cid:0)U', 'dec[hdec\\n\\n;', 'κdec\\n\\ns\\n\\ns\\n\\n](cid:1)\\n\\nDuring', 'training,', 'teacher', 'forcing', 'is', 'used']), (0.0049032934795305615, ['Unlike\\ntheir', 'model,', 'Seq2SQL', 'uses', 'pointer', 'based', 'generation', 'akin', 'to', 'Vinyals', 'et', 'al']), (0.004879512508700271, ['For', 'Seq2SQL', '(no', 'RL),\\nthe', 'WHERE', 'clause', 'is', 'supervised', 'via', 'teacher', 'forcing', 'as', 'opposed', 'to', 'reinforcement', 'learning.\\nwith', 'the', 'table', 'schema', 'such', 'that', 'the', 'model', 'can', 'generalize', 'to', 'new', 'tables']), (0.004874534398919935, ['The', 'dataset', '“Overnight”', '(Wang', 'et', 'al.,', '2015)', 'uses', 'a', 'similar', 'crowd-\\nsourcing', 'process', 'to', 'build', 'a', 'dataset', 'of', 'natural', 'language', 'question,', 'logical', 'form', 'pairs,', 'but', 'has', 'only', '8\\ndomains']), (0.0048708873213956705, ['We', 'train', 'the', 'SELECT', 'network', 'using', 'cross', 'entropy', 'loss', 'Lsel.\\n\\nj\\n\\n(6)\\n\\nWHERE', 'Clause']), (0.004859390328530574, ['We', 'can', 'train', 'the', 'WHERE', 'clause', 'using', 'a', 'pointer', 'decoder', 'similar', 'to', 'that\\ndescribed', 'in', 'Section', '2.1']), (0.00484859257476666, ['foyt', 'AND', 'pole', 'position', '=', 'a.j.\\nSELECT', 'race', 'name', 'WHERE', 'rnd', '=', '12', 'AND', 'pole', 'position', '=', 'a.j']), (0.004836334967170985, ['However,\\nthere', 'is', 'a', 'limitation', 'in', 'using', 'the', 'cross', 'entropy', 'loss\\nthe', 'WHERE', 'conditions', 'of', 'a', 'query', 'can', 'be', 'swapped', 'and', 'the', 'query\\nto', 'optimize', 'the', 'network:\\nyield', 'the', 'same', 'result.\\nSuppose', 'we', 'have', 'the', 'question', '“which', 'males', 'are', 'older', 'than', '18”\\nand', 'the', 'queries', 'SELECT', 'name', 'FROM', 'insurance', 'WHERE', 'age', '>', '18', 'AND', 'gender', '=\\n\"male\"', 'and', 'SELECT', 'name', 'FROM', 'insurance', 'WHERE', 'gender', '=', '\"male\"', 'AND', 'age\\n>', '18']), (0.004832845703795158, ['We', 'also', 'introduced\\nWikiSQL,', 'a', 'dataset', 'of', 'questions', 'and', 'SQL', 'queries', 'that', 'is', 'an', 'order', 'of', 'magnitude', 'larger', 'than', 'compa-\\nrable', 'datasets']), (0.004809728892007076, ['In', 'our', 'case,', 'the', 'input', 'sequence', 'is', 'the', 'concatenation', 'of', 'the', 'column', 'names,', 'required', 'for', 'the\\nselection', 'column', 'and', 'the', 'condition', 'columns', 'of', 'the', 'query,', 'the', 'question,', 'required', 'for', 'the', 'conditions\\nof', 'the', 'query,', 'and', 'the', 'limited', 'vocabulary', 'of', 'the', 'SQL', 'language', 'such', 'as', 'SELECT,', 'COUNT', 'etc']), (0.004767972157136028, ['Neural', 'combinatorial\\n\\noptimization', 'with', 'reinforcement', 'learning']), (0.004763897498455799, ['Natural', 'language', 'interfaces', '(NLI),', 'a', 'research', 'area', 'at', 'the', 'intersection', 'of', 'natural', 'language\\nprocessing', 'and', 'human-computer', 'interactions,', 'seeks', 'to', 'provide', 'means', 'for', 'humans', 'to', 'interact', 'with\\ncomputers', 'through', 'the', 'use', 'of', 'natural', 'language', '(Androutsopoulos', 'et', 'al.,', '1995)']), (0.004759412603249678, ['To', 'enforce', 'succinct', 'queries,', 'we\\nremove', 'conditions', 'from', 'the', 'generated', 'queries', 'if', 'doing', 'so', 'does', 'not', 'change', 'the', 'execution', 'result.\\nFor', 'each', 'query,', 'we', 'generate', 'a', 'crude', 'question', 'using', 'a', 'template', 'and', 'obtain', 'a', 'human', 'paraphrase', 'via\\ncrowdsourcing', 'on', 'Amazon', 'Mechanical', 'Turk']), (0.0047457583139331894, ['foyt\\nSELECT', 'race', 'name', 'WHERE', 'rnd', '=', '12', 'AND', 'pole', 'position', '=', 'a.j']), (0.004735329869527605, ['Language', 'to', 'logical', 'form', 'with', 'neural', 'attention']), (0.0046862811288590255, ['Compositional', 'semantic', 'parsing', 'on', 'semi-structured', 'tables']), (0.004675292839729131, ['Next,', 'the\\nnetwork', 'points', 'to', 'a', 'column', 'in', 'the', 'input', 'table', 'corresponding', 'to', 'the', 'SELECT', 'column']), (0.00465199540527688, ['This', 'model', 'achieves', 'state', 'of', 'the', 'art', 'results', 'on', 'a', 'variety', 'of', 'semantic', 'pars-\\ning', 'datasets,', 'outperforming', 'a', 'host', 'of', 'non-neural', 'semantic', 'parsers', 'despite', 'not', 'using', 'hand-engineered\\ngrammars']), (0.004637262796036665, ['Moreover,', 'it', 'uses', 'policy-based', 'reinforcement', 'learning', '(RL)', 'to', 'generate\\nthe', 'conditions', 'of', 'the', 'query,', 'which', 'are', 'unsuitable', 'for', 'optimization', 'using', 'cross', 'entropy', 'loss', 'due\\nto', 'their', 'unordered', 'nature']), (0.004627503980269407, ['Moreover,', 'Seq2SQL', 'leverages', 'the', 'structure', 'of', 'SQL', 'to', 'prune', 'the', 'space\\nof', 'generated', 'queries', 'and', 'signiﬁcantly', 'simplify', 'the', 'generation', 'problem']), (0.004588988972298243, ['In', 'particular,', 'we', 'can', 'limit', 'the', 'output', 'space', 'of', 'the', 'generated\\nsequence', 'to', 'the', 'union', 'of', 'the', 'table', 'schema,', 'question', 'utterance,', 'and', 'SQL', 'key', 'words']), (0.004580463733769763, ['Learning\\n\\na', 'natural', 'language', 'interface', 'with', 'neural', 'programmer']), (0.00456815021112581, ['In', 'addition\\nto', 'the', 'model,', 'we', 'release', 'WikiSQL,', 'a', 'dataset', 'of', '80654', 'hand-annotated', 'examples\\nof', 'questions', 'and', 'SQL', 'queries', 'distributed', 'across', '24241', 'tables', 'from', 'Wikipedia', 'that\\nis', 'an', 'order', 'of', 'magnitude', 'larger', 'than', 'comparable', 'datasets']), (0.004496113468256853, ['Consequently,', 'the', 'total', 'gradient', 'is', 'the', 'equally', 'weighted', 'sum', 'of\\nthe', 'gradients', 'from', 'the', 'cross', 'entropy', 'loss', 'in', 'predicting', 'the', 'SELECT', 'column,', 'from', 'the', 'cross', 'entropy\\nloss', 'in', 'predicting', 'the', 'aggregation', 'operation,', 'and', 'from', 'policy', 'learning.\\n\\n3', 'WIKISQL\\n\\nWikiSQL', 'is', 'a', 'collection', 'of', 'questions,', 'corre-\\nsponding', 'SQL', 'queries,', 'and', 'SQL', 'tables']), (0.004468448788611815, ['Translating', 'questions', 'to', 'SQL', 'queries', 'with', 'genera-\\n\\ntive', 'parsers', 'discriminatively', 'reranked']), (0.004445134239939029, ['In', 'ACL,', '2007.\\n\\nCaiming', 'Xiong,', 'Victor', 'Zhong,', 'and', 'Richard', 'Socher']), (0.004428680420381598, ['We', 'release', 'the', 'tables', 'used', 'in', 'WikiSQL', 'both', 'in\\nraw', 'JSON', 'format', 'as', 'well', 'as', 'in', 'the', 'form', 'of', 'a', 'SQL', 'database']), (0.004428175373614809, ['Finally,', 'we', 'showed', 'that', 'Seq2SQL', 'outperforms', 'a', 'state-of-the-art', 'semantic', 'parser\\non', 'WikiSQL,', 'improving', 'execution', 'accuracy', 'from', '35.9%', 'to', '59.4%', 'and', 'logical', 'form', 'accuracy', 'from\\n23.4%', 'to', '48.3%.\\n2For', 'simplicity,', 'we', 'deﬁne', 'table', 'schema', 'as', 'the', 'names', 'of', 'the', 'columns', 'in', 'the', 'table.\\n\\n8\\n\\n\\x0cREFERENCES\\nI']), (0.004423786599698736, ['We', 'train', 'using', 'ADAM', '(Kingma', '&', 'Ba,', '2014)', 'and', 'regularize', 'using', 'dropout', '(Srivastava\\net', 'al.,', '2014)']), (0.004418050936146366, ['The', 'ﬁrst', 'two', 'components\\nare', 'supervised', 'using', 'cross', 'entropy', 'loss,', 'whereas', 'the', 'third', 'generation', 'component', 'is', 'trained', 'using\\npolicy', 'gradient', 'to', 'address', 'the', 'unordered', 'nature', 'of', 'query', 'conditions', '(we', 'explain', 'this', 'in', 'the', 'subse-\\nquent', 'WHERE', 'Clause', 'section)']), (0.004402195819477176, ['foyt', 'had', 'the', 'pole', 'position?”,', 'Seq2SQL', 'trained', 'without\\nRL', 'generates', 'WHERE', 'rnd', '=', '12', 'and', 'track', '=', 'a.j.\\nfoyt', 'AND', 'pole', 'position', '=\\na.j']), (0.004401211917857495, ['Our', 'model', 'uses', 'rewards', 'from', 'in-\\nthe-loop', 'query', 'execution', 'over', 'the', 'database', 'to', 'learn', 'a', 'policy', 'to', 'generate', 'the', 'query,\\nwhich', 'contains', 'unordered', 'parts', 'that', 'are', 'less', 'suitable', 'for', 'optimization', 'via', 'cross', 'en-\\ntropy', 'loss']), (0.004370585295064926, ['First,', 'a', 'worker\\nparaphrases', 'a', 'generated', 'question', 'for', 'a', 'table']), (0.004361182484191204, ['Our', 'approach', 'is\\ndifferent', 'in', 'that', 'we', 'do', 'not', 'access', 'table', 'content,', 'which', 'may', 'be', 'unavailable', 'due', 'to', 'privacy', 'concerns.\\nWe', 'also', 'do', 'not', 'hand-engineer', 'model', 'architecture', 'for', 'query', 'execution', 'and', 'instead', 'leverage', 'existing\\ndatabase', 'engines', 'to', 'perform', 'efﬁcient', 'query', 'execution']), (0.004360545471379361, ['Namely,', 'WikiSQL', 'is', 'the\\nlargest', 'hand-annotated', 'semantic', 'parsing', 'dataset\\nto', 'date', '-', 'it', 'is', 'an', 'order', 'of', 'magnitude', 'larger', 'than\\nother', 'datasets', 'that', 'have', 'logical', 'forms,', 'either', 'in\\nterms', 'of', 'the', 'number', 'of', 'examples', 'or', 'the', 'number\\nof', 'tables']), (0.004346113853032705, ['(2013)', 'and', 'remove', 'small', 'tables\\naccording', 'to', 'the', 'following', 'criteria:\\n\\n•', 'the', 'number', 'of', 'cells', 'in', 'each', 'row', 'is', 'not', 'the', 'same\\n•', 'the', 'content', 'in', 'a', 'cell', 'exceed', '50', 'characters\\n•', 'a', 'header', 'cell', 'is', 'empty\\n•', 'the', 'table', 'has', 'less', 'than', '5', 'rows', 'or', '5', 'columns\\n•', 'over', '40%', 'of', 'the', 'cells', 'of', 'a', 'row', 'contain', 'identical', 'content\\n\\nWe', 'also', 'remove', 'the', 'last', 'row', 'of', 'a', 'table', 'because', 'a', 'large', 'quantity', 'of', 'HTML', 'tables', 'tend', 'to', 'have', 'sum-\\nmary', 'statistics', 'in', 'the', 'last', 'row,', 'and', 'hence', 'the', 'last', 'row', 'does', 'not', 'adhere', 'to', 'the', 'table', 'schema', 'deﬁned', 'by\\nthe', 'header', 'row.\\nFor', 'each', 'of', 'the', 'table', 'that', 'passes', 'the', 'above', 'criteria,', 'we', 'randomly', 'generate', '6', 'SQL', 'queries', 'according\\nto', 'the', 'following', 'rules:\\n\\n•', 'the', 'query', 'follows', 'the', 'format', 'SELECT', 'agg', 'op', 'agg', 'col', 'from', 'table', 'where\\n\\ncond1', 'col', 'cond1', 'op', 'cond1', 'AND', 'cond2', 'col', 'cond2', 'op', 'cond2', '...\\n\\n•', 'the', 'aggregation', 'operator', 'agg', 'op', 'can', 'be', 'empty', 'or', 'COUNT']), (0.004331553820250596, ['ICLR,', '2017.\\n\\nIn', 'ACL,', '2016.\\n\\nJohn', 'M']), (0.00432244254436732, ['During', 'inference,', 'a', 'beam', 'size', 'of', '5', 'is', 'used', 'and', 'generated\\nunknown', 'words', 'are', 'replaced', 'by', 'the', 'input', 'words', 'with', 'the', 'highest', 'attention', 'weight.\\n\\n(11)\\n\\n(12)\\n\\n(13)\\n\\n(14)\\n\\nC', 'PREDICTIONS', 'BY', 'SEQ2SQL\\n\\nQ\\nP\\nS’\\nS\\nG\\n\\nQ\\nP\\nS’\\nS\\nG\\n\\nQ\\nP\\nS’\\nS\\nG\\n\\nQ\\nP\\nS’\\nS\\nG\\n\\nQ\\nP\\nS’\\nS\\nG\\n\\nQ\\nP\\nS’\\nS\\nG\\n\\nwhen', 'connecticut', '&', 'villanova', 'are', 'the', 'regular', 'season', 'winner', 'how', 'many', 'tournament', 'venues', '(city)', 'are', 'there?\\nSELECT', 'COUNT', 'tournament', 'player', '(city)', 'WHERE', 'regular', 'season', 'winner', 'city', ')', '=', 'connecticut', '&', 'villanova\\nSELECT', 'COUNT', 'tournament', 'venue', '(city)', 'WHERE', 'tournament', 'winner', '=', 'connecticut', '&', 'villanova\\nSELECT', 'COUNT', 'tournament', 'venue', '(city)', 'WHERE', 'regular', 'season', 'winner', '=', 'connecticut', '&', 'villanova\\nSELECT', 'COUNT', 'tournament', 'venue', '(city)', 'WHERE', 'regular', 'season', 'winner', '=', 'connecticut', '&', 'villanova\\n\\nwhat', 'are', 'the', 'aggregate', 'scores', 'of', 'those', 'races', 'where', 'the', 'ﬁrst', 'leg', 'results', 'are', '0-1?\\nSELECT', 'aggregate', 'WHERE', '1st', '.\\nSELECT', 'COUNT', 'agg.\\nSELECT', 'agg.\\nSELECT', 'agg.\\n\\nscore', 'WHERE', '1st', 'leg', '=', '0-1\\nscore', 'WHERE', '1st', 'leg', '=', '0-1\\n\\nscore', 'WHERE', '1st', 'leg', '=', '0-1\\n\\n=', '0-1\\n\\nwhat', 'is', 'the', 'race', 'name', 'of', 'the', '12th', 'round', 'trenton,', 'new', 'jersey', 'race', 'where', 'a.j']), (0.004306234774569988, ['Seq2SQL,', 'shown\\nin', 'Figure', '1,', 'consists', 'of', 'three', 'components', 'that', 'leverage', 'the', 'structure', 'of', 'SQL', 'to', 'prune', 'the', 'output\\nspace', 'of', 'generated', 'queries']), (0.004296618755260912, ['Due\\nto', 'these', 'observations,', 'we', 'use', 'both', 'metrics', 'to', 'evaluate', 'the', 'models.\\n\\nComparison', 'between', 'WikiSQL\\nTable', '1:\\nare\\nand', 'existing', 'datasets.\\nGeoQuery880\\n2001),\\nATIS', '(Price,', '1990),', 'Free917', '(Cai', '&', 'Yates,\\n2013),', 'Overnight', '(Wang', 'et', 'al.,', '2015),', 'WebQues-\\ntions', '(Berant', 'et', 'al.,', '2013),', 'and', 'WikiTableQues-\\ntions', '(Pasupat', '&', 'Liang,', '2015)']), (0.004264041255778413, ['To', 'train', 'Seq2SQL,\\nwe', 'applied', 'in-the-loop', 'query', 'execution', 'to', 'learn', 'a', 'policy', 'for', 'generating', 'the', 'conditions', 'of', 'the', 'SQL\\nquery,', 'which', 'is', 'unordered', 'and', 'unsuitable', 'for', 'optimization', 'via', 'cross', 'entropy', 'loss']), (0.004233027372769959, ['foyt\\n\\nfoyt,', 'new', 'jersey', 'AND\\n\\nfoyt\\n\\nwhat', 'city', 'is', 'on', '89.9?\\nSELECT', 'city', 'WHERE', 'frequency', '=', '89.9\\nSELECT', 'city', 'of', 'license', 'WHERE', 'frequency', '=', '89.9\\nSELECT', 'city', 'of', 'license', 'WHERE', 'frequency', '=', '89.9\\nSELECT', 'city', 'of', 'license', 'WHERE', 'frequency', '=', '89.9\\n\\nhow', 'many', 'voters', 'from', 'the', 'bronx', 'voted', 'for', 'the', 'socialist', 'party?\\nSELECT', 'MIN', '%', 'party', '=', 'socialist\\nSELECT', 'COUNT', 'the', 'bronx', 'where', 'the', 'bronx', '=', 'socialist\\nSELECT', 'COUNT', 'the', 'bronx', 'WHERE', 'the', 'bronx', '=', 'socialist\\nSELECT', 'the', 'bronx', 'WHERE', 'party', '=', 'socialist\\n\\nin', 'what', 'year', 'did', 'a', 'plymouth', 'vehicle', 'win', 'on', 'february', '9', '?\\nSELECT', 'MIN', 'year', '(km)', 'WHERE', 'date', '=', 'february', '9', 'AND', 'race', 'time', '=', 'plymouth', '9\\nSELECT', 'year', '(km)', 'WHERE', 'date', '=', 'plymouth', '9', 'AND', 'race', 'time', '=', 'february', '9\\nSELECT', 'year', '(km)', 'WHERE', 'date', '=', 'plymouth', '9', 'AND', 'race', 'time=', 'february', '9\\nSELECT', 'year', '(km)', 'WHERE', 'manufacturer', '=', 'plymouth', 'AND', 'date', '=', 'february', '9\\n\\nTable', '4:', 'Examples', 'predictions', 'by', 'the', 'models', 'on', 'the', 'dev', 'split']), (0.004222151127787727, ['However,', 'as', 'we', 'showed', 'in', 'Section', '2.2,', 'Acclf', 'incorrectly', 'penalizes', 'queries\\naccuracy', 'Acclf', '=', 'Nlf\\nthat', 'achieve', 'the', 'correct', 'result', 'but', 'do', 'not', 'have', 'exact', 'string', 'match', 'with', 'the', 'ground', 'truth', 'query']), (0.0042203294966797495, ['In', 'ACL,', '2017.\\n\\nDiederik', 'P']), (0.004212094598469097, ['We', 'implement', 'a', 'variant', 'using', 'OpenNMT', 'and', 'a', 'global', 'attention\\nencoder-decoder', 'architecture', '(with', 'input', 'feeding)', 'described', 'by', 'Luong', 'et', 'al.\\n\\n11\\n\\n\\x0cWe', 'use', 'the', 'same', 'two-layer,', 'bidirectional,', 'stacked', 'LSTM', 'encoder', 'as', 'described', 'previously']), (0.00418805065699449, ['Furthermore,', 'in', 'contrast', 'to', 'Dong', '&', 'Lapata\\n(2016)', 'and', 'Neelakantan', 'et', 'al']), (0.004164743949310736, ['To', 'address', 'this', 'problem,', 'we', 'apply', 'reinforcement', 'learning', 'to\\nlearn', 'a', 'policy', 'to', 'directly', 'optimize', 'the', 'expected', 'correctness', 'of', 'the', 'execution', 'result', '(Equation', '7).\\nInstead', 'of', 'teacher', 'forcing', 'at', 'each', 'step', 'of', 'query', 'generation,', 'we', 'sample', 'from', 'the', 'output', 'distribution', 'to\\nobtain', 'the', 'next', 'token']), (0.00414899733676631, ['7\\n1\\n0\\n2\\n\\n', '\\n\\nv\\no\\nN\\n9\\n\\n', '\\n\\n', '\\n', '\\n]\\nL\\nC\\n.\\ns\\nc\\n[\\n', '\\n', '\\n\\n7\\nv\\n3\\n0\\n1\\n0\\n0\\n\\n.\\n\\n9\\n0\\n7\\n1\\n:\\nv\\ni\\nX\\nr\\na\\n\\nSEQ2SQL:', 'GENERATING', 'STRUCTURED', 'QUERIES\\nFROM', 'NATURAL', 'LANGUAGE', 'USING', 'REINFORCEMENT\\nLEARNING\\n\\nVictor', 'Zhong,', 'Caiming', 'Xiong,', 'Richard', 'Socher\\nSalesforce', 'Research\\nPalo', 'Alto,', 'CA\\n{vzhong,cxiong,rsocher}@salesforce.com\\n\\nABSTRACT\\n\\nRelational', 'databases', 'store', 'a', 'signiﬁcant', 'amount', 'of', 'the', 'worlds', 'data']), (0.004144030542610908, ['Joint', 'learning', 'of', 'ontology', 'and', 'semantic', 'parser', 'from', 'text']), (0.004136360962847615, ['We', 'use', 'ﬁxed', 'GloVe', 'word', 'embeddings', '(Pennington', 'et', 'al.,', '2014)', 'and\\ncharacter', 'n-gram', 'embeddings', '(Hashimoto', 'et', 'al.,', '2016)']), (0.004129170723645393, ['Similarly,', 'let', 'xq', 'and', 'xs', 'respectively', 'denote', 'the', 'sequence\\nof', 'words', 'in', 'the', 'question', 'and', 'the', 'set', 'of', 'unique', 'words', 'in', 'the', 'SQL', 'vocabulary.\\n\\nj', '=', '[xc\\n\\nj,2,', '...xc\\n\\nj,1,', 'xc\\n\\nj,Tj\\n\\n2\\n\\nPredicted', 'resultsQuestion,', 'schemaGround', 'truth', 'resultsSeq2SQLRewardGenerated', 'SQLDatabase……………Wilfrid', 'LaurierCaliforniaYorkYorkCollegeFrank', 'Hoﬀman30Toronto', 'ArgonautsDLCalgary', 'StampedersL.P']), (0.004110988915011183, ['This', 'model', 'by', 'Dong', '&\\nLapata', '(2016)', 'achieves', 'state', 'of', 'the', 'art', 'results', 'on', 'a', 'variety', 'of', 'semantic', 'parsing', 'datasets', 'despite', 'not\\nusing', 'hand-engineered', 'grammar']), (0.004105892241967278, ['the', 'policy', 'is', 'not', 'learned', 'from', 'scratch)', 'and\\nthen', 'continue', 'training', 'using', 'reinforcement', 'learning']), (0.0040933300280591375, ['We', 'train', 'the', 'model', 'using', 'gradient', 'descent', 'to', 'minimize', 'the', 'objective\\nfunction', 'L', '=', 'Lagg', '+', 'Lsel', '+', 'Lwhe']), (0.004090941918777454, ['ICLR,', '2017.\\n\\nJonathan', 'Berant,', 'Andrew', 'Chou,', 'Roy', 'Frostig,', 'and', 'Percy', 'Liang']), (0.004078300402739949, ['(2017),', 'and', 'Guu', 'et', 'al']), (0.004076878165297981, ['From', 'language', 'to', 'programs:\\n\\nBridging', 'reinforcement', 'learning', 'and', 'maximum', 'marginal', 'likelihood']), (0.004055663367850838, ['SELECT', 'column', 'prediction', 'is', 'then', 'a', 'matching', 'problem,', 'solvable', 'using', 'a\\npointer:', 'given', 'the', 'list', 'of', 'column', 'representations', 'and', 'a', 'question', 'representation,', 'we', 'select', 'the', 'column\\nthat', 'best', 'matches', 'the', 'question.\\nIn', 'order', 'to', 'produce', 'the', 'representations', 'for', 'the', 'columns,', 'we', 'ﬁrst', 'encode', 'each', 'column', 'name', 'with', 'a\\nLSTM']), (0.004052770872123127, ['Coupling', 'distributed', 'and', 'symbolic', 'execution', 'for\\n\\nnatural', 'language', 'queries']), (0.0040517661981719815, ['Weakly', 'supervised', 'learning', 'of', 'semantic', 'parsers', 'for', 'mapping\\n\\ninstructions', 'to', 'actions']), (0.004032085226732807, ['(2017)', 'and', 'Yin', 'et', 'al']), (0.004022052221501203, ['In', 'ACL,', '2017.\\n\\nKazuma', 'Hashimoto,', 'Caiming', 'Xiong,', 'Yoshimasa', 'Tsuruoka,', 'and', 'Richard', 'Socher']), (0.0039929834891329445, ['To\\ncompute', 'the', 'aggregation', 'operation,', 'we', 'ﬁrst', 'compute', 'the', 'scalar', 'attention', 'score,', 'αinp\\n,\\nt', '=', 'W', 'inphenc\\nfor', 'each', 'tth', 'token', 'in', 'the', 'input', 'sequence']), (0.003985229050241304, ['In', 'EMNLP-CoNLL,', '2007.\\n\\n10\\n\\n\\x0cA', 'COLLECTION', 'OF', 'WIKISQL\\n\\nWikiSQL', 'is', 'collected', 'in', 'a', 'paraphrase', 'phases', 'as', 'well', 'as', 'a', 'veriﬁcation', 'phase.\\nIn', 'the', 'paraphrase\\nphase,', 'we', 'use', 'tables', 'extracted', 'from', 'Wikipedia', 'by', 'Bhagavatula', 'et', 'al']), (0.003955425858516555, ['Neural', 'computation,', '1997.\\n\\nSrinivasan', 'Iyer,', 'Ioannis', 'Konstas,', 'Alvin', 'Cheung,', 'Jayant', 'Krishnamurthy,', 'and', 'Luke', 'Zettlemoyer.\\n\\nLearning', 'a', 'neural', 'semantic', 'parser', 'from', 'user', 'feedback']), (0.003935533441198124, ['Semantic', 'parsing', 'on', 'freebase', 'from\\n\\nquestion-answer', 'pairs']), (0.0039347764116305734, ['Unlike', 'Mou', 'et', 'al']), (0.003916636954325438, ['However,', 'ac-\\ncessing', 'this', 'data', 'currently', 'requires', 'users', 'to', 'understand', 'a', 'query', 'language', 'such', 'as\\nSQL']), (0.003914622481554992, ['WikiTableQuestions', '(Pasupat', '&', 'Liang,', '2015)', 'is', 'a', 'collection', 'of', 'question', 'and', 'answers,', 'also\\nover', 'a', 'large', 'quantity', 'of', 'tables', 'extracted', 'from', 'Wikipedia']), (0.0039006336414147184, ['Table', '2', 'compares', 'the', 'performance', 'of', 'the', 'three', 'models.\\nReducing', 'the', 'output', 'space', 'by', 'utilizing', 'the', 'augmented', 'pointer', 'network', 'improves', 'upon', 'the', 'baseline\\nby', '17.4%']), (0.00389684554801023, ['In', 'ACL,', '2017.\\n\\nPercy', 'Liang,', 'Michael', 'I']), (0.0038655505240475477, ['The', 'representation', 'of', 'a', 'particular', 'column', 'j,', 'ec\\n\\nj,', 'is', 'given', 'by:\\n\\nj,t', '=', 'LSTM(cid:0)emb(cid:0)xc\\n\\n(cid:1)', ',', 'hc\\n\\n(cid:1)\\n\\nj,t\\n\\nhc\\n\\nj,t−1\\n\\n(5)\\nj,t', 'denotes', 'the', 'tth', 'encoder', 'state', 'of', 'the', 'jth', 'column']), (0.0038623819528992758, ['The', 'inputs', 'consist', 'of', 'a', 'table', 'and', 'a', 'question']), (0.0038551348899299102, ['(2000)', 'and', 'Schulman', 'et', 'al']), (0.003833133936764877, ['Building', 'a', 'semantic', 'parser', 'overnight.\\n\\nIn', 'ACL,\\n\\n2015.\\n\\nYuk', 'Wah', 'Wong', 'and', 'Raymond', 'J']), (0.0038308984243425872, ['Each', 'Freebase', 'API\\npage', 'is', 'counted', 'as', 'a', 'separate', 'domain.\\n\\nit\\n\\n4', 'EXPERIMENTS\\nWe', 'tokenize', 'the', 'dataset', 'using', 'Stanford', 'CoreNLP', '(Manning', 'et', 'al.,', '2014)']), (0.0038123423892419002, ['Q', 'denotes', 'the', 'natural', 'language\\nquestion', 'and', 'G', 'denotes', 'the', 'corresponding', 'ground', 'truth', 'query']), (0.0038090911769424925, ['Large-scale', 'semantic', 'parsing', 'without', 'question-\\n\\nanswer', 'pairs']), (0.0037994697564034986, ['Neural', 'symbolic', 'machines:\\n\\nLearning', 'semantic', 'parsers', 'on', 'freebase', 'with', 'weak', 'supervision']), (0.003786738127484583, ['Compared\\nto', 'the', 'baseline,', 'the', 'augmented', 'pointer', 'network', 'generates', 'higher', 'quality', 'WHERE', 'clause']), (0.0037784057398941393, ['(2016),', 'we', 'generalize', 'across', 'natural', 'language\\nquestions', 'and', 'table', 'schemas', 'instead', 'of', 'across', 'synthetic', 'questions', 'on', 'a', 'single', 'schema.\\nNatural', 'language', 'interface', 'for', 'databases']), (0.003777496568854768, ['Large-scale', 'semantic', 'parsing', 'via', 'schema', 'matching', 'and', 'lexicon\\n\\nextension']), (0.0037570432538042084, ['We', 'use', 'cross', 'entropy', 'loss', 'Lagg', 'for', 'the', 'aggregation', 'operation.\\n\\nSELECT', 'Column']), (0.003738295927223666, ['For', 'more', 'queries\\nproduced', 'by', 'the', 'different', 'models,', 'please', 'see', 'Section', '3', 'of', 'the', 'Appendix.\\n\\nPrecision', 'Recall\\n66.3%\\n72.6%\\n\\nTable', '3:', 'Performance', 'on', 'the', 'COUNT', 'operator.\\n\\n64.4%', '65.4%\\n66.2%', '69.2%\\n\\nF1\\n\\nRL', 'generates', 'higher', 'quality', 'WHERE', 'clause', 'that', 'are', 'ordered', 'differently', 'than', 'ground', 'truth.\\nTraining', 'with', 'policy-based', 'RL', 'obtains', 'correct', 'results', 'in', 'which', 'the', 'order', 'of', 'conditions', 'is', 'differs\\nfrom', 'the', 'ground', 'truth', 'query']), (0.003728837176668082, ['The', 'queries', 'in', 'WikiSQL', 'span', 'over\\na', 'large', 'number', 'of', 'tables', 'and', 'hence', 'presents', 'an\\nunique', 'challenge:', 'the', 'model', 'must', 'be', 'able', 'to', 'not', 'only', 'generalize', 'to', 'new', 'queries,', 'but', 'to', 'new', 'table\\nschema']), (0.003690373449013092, ['The', 'third\\ncomponent', 'is', 'the', 'WHERE', 'clause', 'of', 'the', 'query,', 'in', 'this', 'case', 'WHERE', 'Driver', '=', 'Val', 'Musetti,\\nwhich', 'contains', 'conditions', 'by', 'which', 'to', 'ﬁlter', 'the', 'rows']), (0.0036871087665771847, ['At', 'the', 'end', 'of', 'the', 'generation', 'procedure,', 'we', 'execute', 'the', 'generated', 'SQL', 'query\\nagainst', 'the', 'database', 'to', 'obtain', 'a', 'reward']), (0.0036745715455615393, ['It', 'generates', 'the', 'cor-\\nresponding', 'SQL', 'query,', 'which,', 'during', 'training,', 'is', 'executed', 'against', 'a', 'database']), (0.003665805850248165, ['Similarly,', 'for', '“what’s', 'doug', 'battaglia’s\\npick', 'number?”,', 'the', 'baseline', 'generates', 'Player', '=', 'doug', 'whereas', 'Seq2SQL', 'generates', 'Player\\n=', 'doug', 'battaglia']), (0.003662151238452989, ['The', 'result', 'of', 'the\\nexecution', 'is', 'utilized', 'as', 'the', 'reward', 'to', 'train', 'the', 'reinforcement', 'learning', 'algorithm.\\n\\nFigure', '2:', 'An', 'example', 'in', 'WikiSQL']), (0.0036535641427868997, ['Natural', 'language', 'interfaces', 'to', 'databases', '-', 'an\\n\\nYoav', 'Artzi', 'and', 'Luke', 'S']), (0.0036414996803088495, ['(2015)', 'to', 'achieve', 'higher\\nperformance,', 'especially', 'in', 'generating', 'queries', 'with', 'rare', 'words', 'and', 'column', 'names']), (0.0036404796515265526, ['Let', 'y', '=', '[y1,', 'y2,', '...,', 'yT', ']', 'denote', 'the', 'sequence', 'of', 'generated\\ntokens', 'in', 'the', 'WHERE', 'clause']), (0.0036386074612465716, ['Learning', 'synchronous', 'grammars', 'for', 'semantic', 'parsing\\n\\nwith', 'lambda', 'calculus']), (0.0036295487088985673, ['The', 'selection', 'column', 'depends', 'on', 'the', 'table', 'columns', 'as', 'well', 'as', 'the', 'question.\\nNamely,', 'for', 'the', 'example', 'in', 'Figure', '3,', '“How', 'many', 'engine', 'types”', 'indicates', 'that', 'we', 'need', 'to', 'retrieve\\nthe', '“Engine”', 'column']), (0.003624694304843967, ['For', 'example,', 'for', '“in', 'what', 'district', 'was', 'the', 'democratic', 'candidate', 'ﬁrst\\nelected', 'in', '1992?”,', 'the', 'ground', 'truth', 'conditions', 'are', 'First', 'elected', '=', '1992', 'AND', 'Party', '=\\nDemocratic', 'whereas', 'Seq2SQL', 'generates', 'Party', '=', 'Democratic', 'AND', 'First', 'elected\\n=', '1992']), (0.003607099865811621, ['Previous', 'semantic', 'parsing', 'systems', 'were', 'designed', 'to', 'answer', 'complex\\nand', 'compositional', 'questions', 'over', 'closed-domain,', 'ﬁxed-schema', 'datasets', 'such', 'as', 'GeoQuery', '(Tang\\n&', 'Mooney,', '2001)', 'and', 'ATIS', '(Price,', '1990)']), (0.0035550605029752314, ['Researchers', 'also', 'investigated', 'QA', 'over', 'subsets', 'of', 'large-\\nscale', 'knowledge', 'graphs', 'such', 'as', 'DBPedia', '(Starc', '&', 'Mladenic,', '2017)', 'and', 'Freebase', '(Cai', '&', 'Yates,\\n2013;', 'Berant', 'et', 'al.,', '2013)']), (0.0035467281432878164, ['TACL,', '2:377–392,', '2014.\\n\\nJohn', 'Schulman,', 'Nicolas', 'Heess,', 'Theophane', 'Weber,', 'and', 'Pieter', 'Abbeel']), (0.0035467281432878164, ['TACL,', '1:49–62,', '2013.\\n\\nThorsten', 'Beck,', 'Asli', 'Demirg¨uc¸-Kunt,', 'and', 'Ross', 'Levine']), (0.0035467281432878164, ['Mooney']), (0.0035467281432878164, ['Mooney']), (0.0035467281432878164, ['Mooney']), (0.0035212466811899083, ['We', 'use', 'the', 'normalized\\ntokens', 'for', 'training', 'and', 'revert', 'into', 'original', 'gloss', 'before', 'outputting', 'the', 'query', 'so', 'that', 'generated', 'queries\\nare', 'executable', 'on', 'the', 'database']), (0.0035208249141992576, ['We', 'compute', 'αagg', 'by', 'applying', 'a', 'multi-layer', 'perceptron', 'to', 'the', 'input\\nrepresentation', 'κagg:\\n(4)\\n\\nαagg', '=', 'W', 'agg', 'tanh', '(V', 'aggκagg', '+', 'bagg)', '+', 'cagg\\n\\nWe', 'apply', 'the', 'softmax', 'function', 'to', 'obtain', 'the', 'distribution', 'over', 'the', 'set', 'of', 'possible', 'aggregation', 'opera-\\ntions', 'βagg', '=', 'softmax', '(αagg)']), (0.0035071377128255035, ['In', 'ACL,', '2013.\\n\\nLi', 'Dong', 'and', 'Mirella', 'Lapata']), (0.003490834261243671, ['The', 'input', 'to', 'the', 'encoder', 'are', 'the', 'embeddings', 'correspond-\\ning', 'to', 'words', 'in', 'the', 'input', 'sequence']), (0.0034530473544935056, ['Zettlemoyer', 'and', 'Michael', 'Collins']), (0.003453047354493505, ['Zettlemoyer', 'and', 'Michael', 'Collins']), (0.003395224649635419, ['During', 'each', 'decoder', 'step', 's,', 'the', 'decoder\\nLSTM', 'takes', 'as', 'input', 'ys−1,', 'the', 'query', 'token', 'generated', 'during', 'the', 'previous', 'decoding', 'step,', 'and', 'outputs\\nthe', 'state', 'gs']), (0.003392624932785671, ['ACL,', '2016.\\n\\nKelvin', 'Guu,', 'Panupong', 'Pasupat,', 'Evan', 'Zheran', 'Liu,', 'and', 'Percy', 'Liang']), (0.003341454030568228, ['Another', 'indicator', 'of', 'the', 'variety', 'of', 'questions', 'in', 'the', 'dataset', 'is', 'the', 'distribution', 'of\\nquestion', 'types,', 'shown', 'in', 'Figure', '4.\\n\\nFigure', '4:', 'Distribution', 'of', 'questions', 'in', 'WikiSQL.\\n\\nFigure', '5:', 'Distribution', 'of', 'table,', 'question,', 'query', 'sizes', 'in', 'WikiSQL.\\n\\nWe', 'collect', 'WikiSQL', 'by', 'crowd-sourcing', 'on', 'Amazon', 'Mechanical', 'Turk', 'in', 'two', 'phases']), (0.0033249270601560417, ['In', 'NIPS,', '2015.\\nYushi', 'Wang,', 'Jonathan', 'Berant,', 'and', 'Percy', 'Liang']), (0.0033226091073876706, ['The', 'Stanford', 'CoreNLP', 'natural', 'language', 'processing', 'toolkit']), (0.0033200222146914584, ['In\\nthe', 'example', 'shown', 'in', 'Figure', '2,', 'the', 'column', 'name', 'tokens', 'consist', 'of', '“Pick”,', '“#”,', '“CFL”,', '“Team”\\netc.;', 'the', 'question', 'tokens', 'consist', 'of', '“How”,', '“many”,', '“CFL”,', '“teams”', 'etc.;', 'the', 'SQL', 'tokens', 'consist', 'of\\nSELECT,', 'WHERE,', 'COUNT,', 'MIN,', 'MAX', 'etc']), (0.0033013879505895604, ['ICLR,', '2017.\\n\\n9\\n\\n\\x0cLili', 'Mou,', 'Zhengdong', 'Lu,', 'Hang', 'Li,', 'and', 'Zhi', 'Jin']), (0.003297828851436449, ['Towards', 'a', 'theory', 'of', 'natural', 'language', 'interfaces\\nto', 'databases']), (0.003277368194271783, ['When', 'Seq2SQL', 'is', 'correct', 'and', 'Seq2SQL', 'without', 'RL', 'is', 'not,', 'the', 'latter', 'tends', 'to', 'produce', 'an\\nincorrect', 'WHERE', 'clause']), (0.0032143952988256024, ['Both', 'of', 'these\\napproaches', 'rely', 'on', 'high', 'quality', 'grammar', 'and', 'are', 'not', 'suitable', 'for', 'tasks', 'that', 'require', 'generalization\\nto', 'new', 'schema.\\nIyer', 'et', 'al']), (0.0032027282502479102, ['Policy', 'gradient\\nmethods', 'for', 'reinforcement', 'learning', 'with', 'function', 'approximation']), (0.0031887975563852787, ['Both', 'queries', 'obtain', 'the', 'correct', 'execution', 'result', 'despite', 'not', 'having', 'exact', 'string', 'match']), (0.0031652527740707046, ['Table', '1', 'shows', 'how', 'WikiSQL', 'compares\\nto', 'related', 'datasets']), (0.0031443875728371772, ['The', 'aggregation', 'operation', 'depends', 'on', 'the', 'question']), (0.003138069145656456, ['Pointer', 'networks']), (0.0031091183329694304, ['We', 'denote', 'the', 'output', 'of', 'the', 'encoder', 'by', 'henc,', 'where', 'henc\\nis', 'the\\nstate', 'of', 'the', 'encoder', 'corresponding', 'to', 'the', 'tth', 'word', 'in', 'the', 'input', 'sequence']), (0.0031056948321547535, ['Zettlemoyer']), (0.0031056948321547535, ['Zettlemoyer']), (0.0031034266857899607, ['Online', 'learning', 'of', 'relaxed', 'ccg', 'grammars', 'for', 'parsing', 'to\\n\\nlogical', 'form']), (0.0030942462836980288, ['Let', 'xc\\n]', 'denote', 'the', 'sequence', 'of', 'words', 'in', 'the\\nname', 'of', 'the', 'jth', 'column,', 'where', 'xc\\nj,i', 'represents', 'the', 'ith', 'word', 'in', 'the', 'jth', 'column', 'and', 'Tj', 'represents', 'the\\ntotal', 'number', 'of', 'words', 'in', 'the', 'jth', 'column']), (0.003088950321013438, ['Our\\napproach', 'is', 'similar', 'in', 'that', 'it', 'generalizes', 'to', 'new', 'table', 'schema']), (0.0030848048220220485, ['P,', 'S’,', 'and', 'S', 'denote,', 'respectively,', 'the\\nqueries', 'produced', 'by', 'the', 'Augmented', 'Pointer', 'Network,', 'Seq2SQL', 'without', 'reinforcement', 'learning,\\nSeq2SQL']), (0.003075721448203781, ['Bootstrapping', 'semantic', 'parsers', 'from', 'conversations.\\n\\nIn\\n\\nintroduction']), (0.0030749885691775756, ['Pasupat', '&', 'Liang', '(2015)', 'addresses', 'the', 'single-schema', 'limitation', 'by', 'proposing\\nthe', 'ﬂoating', 'parser,', 'which', 'generalizes', 'to', 'unseen', 'web', 'tables', 'on', 'the', 'WikiTableQuestions', 'task']), (0.0030653664405583697, ['However,', 'we', 'do', 'not', 'require', 'access', 'to\\ntable', 'content,', 'conversion', 'of', 'table', 'to', 'an', 'additional', 'graph,', 'nor', 'hand-engineered', 'features/grammar.\\nSemantic', 'parsing', 'datasets']), (0.002994971627892857, ['A', 'large', 'quantity', 'of', 'invalid', 'queries\\nresult', 'from', 'column', 'names', '–', 'the', 'generated\\nquery', 'refers', 'to', 'selection', 'columns', 'that', 'are', 'not', 'present', 'in', 'the', 'table']), (0.0029812707710352208, ['The', 'dataset', 'is', 'available', 'for', 'download\\nat', 'https://github.com/salesforce/WikiSQL.\\nThe', 'tables,', 'their', 'paraphrases,', 'and', 'SQL', 'queries\\nare', 'randomly', 'slotted', 'into', 'train,', 'dev,', 'and', 'test\\nsplits,', 'such', 'that', 'each', 'table', 'is', 'present', 'in', 'exactly\\none', 'split']), (0.0028246571520551813, ['Next,', 'two', 'other', 'workers', 'verify', 'that', 'the', 'paraphrase', 'has', 'the', 'same', 'meaning', 'as\\n\\n5\\n\\n\\x0cSchema\\n24241\\n8\\n141\\n81*\\n8\\n2420\\n2108\\n\\nthe', 'generated', 'question']), (0.0028124128143872883, ['Le,', 'Mart´ın', 'Abadi,', 'Andrew', 'McCallum,', 'and', 'Dario', 'Amodei']), (0.002753491942119649, ['We', 'investigate', 'one\\nparticular', 'aspect', 'of', 'NLI', 'applied', 'to', 'relational', 'databases:', 'translating', 'natural', 'language', 'questions', 'to\\nSQL', 'queries.\\nOur', 'main', 'contributions', 'in', 'this', 'work', 'are', 'two-fold']), (0.0027457537371090874, ['Zelle', 'and', 'Raymond', 'J']), (0.0027457537371090874, ['Tang', 'and', 'Raymond', 'J']), (0.0027306406505999063, ['(2015)', 'to', 'the', 'input', 'encodings', 'henc.\\nThe', 'decoder', 'network', 'uses', 'a', 'two', 'layer,', 'unidirectional', 'LSTM']), (0.0027014204610947582, ['ACL,', '2016.\\n\\nAlessandra', 'Giordani', 'and', 'Alessandro', 'Moschitti']), (0.002678487737458088, ['For', 'the', 'example\\nshown', 'in', 'Figure', '3,', 'the', 'correct', 'operator', 'is', 'COUNT', 'because', 'the', 'question', 'asks', 'for', '“How', 'many”']), (0.0026765628235892527, ['If\\nthe', 'former', 'is', 'provided', 'as', 'the', 'ground', 'truth,', 'using', 'cross', 'entropy', 'loss', 'to', 'supervise', 'the', 'generation\\nwould', 'then', 'wrongly', 'penalize', 'the', 'latter']), (0.0026701067352465253, ['In', 'the', 'event', 'that', 'cond', 'col', 'is', 'numerical,', 'cond', 'can', 'be', 'any', 'numerical', 'value\\nsampled', 'from', 'the', 'range', 'from', 'the', 'minimum', 'value', 'in', 'the', 'column', 'to', 'the', 'maximum', 'value', 'in\\nthe', 'column.\\n\\nWe', 'only', 'generate', 'queries', 'that', 'produce', 'a', 'non-empty', 'result', 'set']), (0.0026662082000903143, ['First,', 'the', 'network', 'classiﬁes', 'an', 'aggregation', 'operation\\nfor', 'the', 'query,', 'with', 'the', 'addition', 'of', 'a', 'null', 'operation', 'that', 'corresponds', 'to', 'no', 'aggregation']), (0.002633656699957698, ['Adam:', 'A', 'method', 'for', 'stochastic', 'optimization.\\n\\narXiv,\\n\\nabs/1412.6980,', '2014.\\n\\nChen', 'Liang,', 'Jonathan', 'Berant,', 'Quoc', 'V']), (0.0026237392401341785, ['A', 'Joint', 'Many-Task\\n\\nModel:', 'Growing', 'a', 'Neural', 'Network', 'for', 'Multiple', 'NLP', 'Tasks']), (0.0026078414789561057, ['Le,', 'Mohammad', 'Norouzi,', 'and', 'Samy', 'Bengio']), (0.0026078414789561057, ['Le,', 'Ken', 'Forbus,', 'and', 'Ni', 'Lao']), (0.00260346246150139, ['For', 'ex-\\nample,', 'for', '“in', 'how', 'many', 'districts', 'was', 'a', 'successor', 'seated', 'on', 'march', '4,', '1850?”,', 'the', 'baseline', 'gen-\\nerates', 'the', 'condition', 'successor', 'seated', '=', 'seated', 'march', '4', 'whereas', 'Seq2SQL', 'generates\\nsuccessor', 'seated', '=', 'seated', 'march', '4', '1850']), (0.0025877904088377557, ['However,', 'it', 'does', 'not', 'provide', 'logical', 'forms\\nwhereas', 'WikiSQL', 'does']), (0.0025699425542935034, ['In', 'the', 'event', 'that', 'the', 'aggregation\\n\\ncolumn', 'agg', 'col', 'is', 'numeric,', 'agg', 'op', 'can', 'additionally', 'be', 'one', 'of', 'MAX', 'and', 'MIN\\n\\n•', 'the', 'condition', 'operator', 'cond', 'op', 'is', '=']), (0.0025638075635667024, ['55–60,', '2014.\\n\\nStephen', 'Merity,', 'Caiming', 'Xiong,', 'James', 'Bradbury,', 'and', 'Richard', 'Socher']), (0.0025595173351612294, ['Expert', 'systems', 'with', 'applications,\\n36(2):2592–2602,', '2009.\\n\\nPanupong', 'Pasupat', 'and', 'Percy', 'Liang']), (0.0025142806793877186, ['To', 'make', 'this', 'baseline', 'even', 'more', 'competitive', 'on', 'our', 'new', 'dataset,', 'we', 'augment', 'their', 'input\\n\\n1https://pytorch.org\\n\\n6\\n\\n\\x0cModel\\nBaseline', '(Dong', '&', 'Lapata,', '2016)\\nAug', 'Ptr', 'Network\\nSeq2SQL', '(no', 'RL)\\nSeq2SQL\\n\\nDev', 'Acclf', 'Dev', 'Accex\\n23.3%\\n44.1%\\n48.2%\\n49.5%\\n\\n37.0%\\n53.8%\\n58.1%\\n60.8%\\n\\nTest', 'Acclf\\n23.4%\\n43.3%\\n47.4%\\n48.3%\\n\\nTest', 'Accex\\n35.9%\\n53.3%\\n57.1%\\n59.4%\\n\\nTable', '2:', 'Performance', 'on', 'WikiSQL']), (0.002510099041717407, ['Learning', 'dependency-based', 'compositional', 'seman-\\n\\ntics']), (0.002502118847744727, ['We', 'implement', 'all', 'models', 'using', 'PyTorch', '1']), (0.002482352934137626, ['Here,', 'we', 'keep', 'rows', 'in', 'which', 'the', 'driver', 'is\\n“Val', 'Musetti”.\\nSeq2SQL,', 'as', 'shown', 'in', 'Figure', '3,', 'has', 'three', 'parts', 'that', 'correspond', 'to', 'the', 'aggregation', 'operator,', 'the\\nSELECT', 'column,', 'and', 'the', 'WHERE', 'clause']), (0.0024589854752574826, ['In', 'Advances', 'in', 'neural', 'informa-\\ntion', 'processing', 'systems,', 'pp']), (0.0024545392032099657, ['Dynamic', 'coattention', 'networks', 'for', 'question\\n\\nPengcheng', 'Yin,', 'Zhengdong', 'Lu,', 'Hang', 'Li,', 'and', 'Ben', 'Kao']), (0.0024348593360763895, ['Finally,', 'WikiSQL', 'contains', 'realistic', 'data', 'extracted', 'from', 'the', 'web']), (0.0024154603409595812, ['In', 'COLING,', '2012.\\n\\nJiatao', 'Gu,', 'Zhengdong', 'Lu,', 'Hang', 'Li,', 'and', 'Victor', 'O']), (0.002414777785657356, ['For', 'example,', 'for', 'the', 'rather', 'complex', 'question', '“what', 'is', 'the', 'race', 'name', 'of', 'the\\n12th', 'round', 'trenton,', 'new', 'jersey', 'race', 'where', 'a.j']), (0.002404975359211554, ['The', 'input', 'representation\\n\\nκagg', 'is', 'the', 'sum', 'over', 'the', 'input', 'encodings', 'henc', 'weighted', 'by', 'the', 'normalized', 'scores', 'βinp:\\n\\nκagg', '=\\n\\nβinp\\nt', 'henc\\n\\nt\\n\\n(3)\\n\\n(cid:88)\\n\\nt\\n\\nLet', 'αagg', 'denote', 'the', 'scores', 'over', 'the', 'aggregation', 'operations', 'such', 'as', 'COUNT,', 'MIN,', 'MAX,', 'and', 'the', 'no-\\naggregation', 'operation', 'NULL']), (0.0023905915697773183, ['For', 'the', 'example', 'shown', 'in', 'Figure', '3,', 'the', 'distribution', 'is', 'over', 'the\\n\\nWe', 'normalize', 'the', 'scores', 'with', 'a', 'softmax', 'function', 'to', 'produce', 'a', 'distribution', 'over', 'the', 'possible', 'SELECT\\n\\ncolumns', '“Entrant”,', '“Constructor”,', '“Chassis”,', '“Engine”,', '“No”,', 'and', 'the', 'ground', 'truth', 'SELECT', 'col-\\numn', '“Driver”']), (0.002387757951695117, ['To', 'train', 'Seq2SQL,', 'we', 'ﬁrst', 'train', 'a', 'version', 'in', 'which\\nthe', 'WHERE', 'clause', 'is', 'supervised', 'via', 'teacher', 'forcing', '(i.e']), (0.0023527788836687664, ['Alternatively', 'the', 'query', 'may', 're-\\nquest', 'no', 'summary', 'statistics,', 'in', 'which', 'case\\nan', 'aggregation', 'operator', 'is', 'not', 'provided.\\nThe', 'second', 'component', 'is', 'the', 'SELECT\\ncolumn(s),', 'in', 'this', 'case', 'Engine,', 'which\\nidentiﬁes', 'the', 'column(s)', 'that', 'are', 'to', 'be', 'in-\\ncluded', 'in', 'the', 'returned', 'results']), (0.0023392116288890093, ['We', 'ensure', 'the', 'validity', 'and', 'complexity', 'of', 'the', 'tables\\nby', 'keeping', 'only', 'those', 'that', 'are', 'legitimate', 'database', 'tables', 'and', 'sufﬁciently', 'large', 'in', 'the', 'number', 'of\\nrows', 'and', 'columns']), (0.0023367416120453626, ['In', 'the', 'event', 'that', 'the', 'corresponding', 'condition', 'column\\n\\ncond', 'col', 'is', 'numeric,', 'cond', 'op', 'can', 'additionally', 'be', 'one', 'of', '>', 'and', '<\\n\\n•', 'the', 'condition', 'cond', 'can', 'be', 'any', 'possible', 'value', 'present', 'in', 'the', 'table', 'under', 'the', 'corresponding\\ncond', 'col']), (0.0023322590363205075, ['“Size”', 'denotes\\nthe', 'number', 'of', 'examples', 'in', 'the', 'dataset.\\n“LF”\\nindicates', 'whether', 'it', 'has', 'annotated', 'logical', 'forms.\\n“Schema”', 'denotes', 'the', 'number', 'of', 'tables']), (0.0023131141986807435, ['In', 'equation', '10,', 'we\\napproximate', 'the', 'expected', 'gradient', 'using', 'a', 'single', 'Monte-Carlo', 'sample', 'y.\\n\\nMixed', 'Objective', 'Function']), (0.0022977968797372734, ['However,', 'accessing', 'relational', 'databases\\nrequires', 'an', 'understanding', 'of', 'query', 'languages', 'such', 'as', 'SQL,', 'which,', 'while', 'powerful,', 'is', 'difﬁcult', 'to\\nmaster']), (0.002283218464366521, ['Evaluation', 'of', 'spoken', 'language', 'systems:', 'The', 'ATIS', 'domain']), (0.0022742181006139045, ['Bidirectional', 'attention\\n\\nﬂow', 'for', 'machine', 'comprehension']), (0.0022699259135903114, ['Section', 'A', 'of', 'the', 'Appendix', 'contains', 'more', 'details', 'on', 'the', 'collection\\nof', 'WikiSQL']), (0.002248373903870462, ['For', 'brevity,', 'we', 'do', 'not', 'write\\nout', 'the', 'LSTM', 'equations,', 'which', 'are', 'described', 'by', 'Hochreiter', '&', 'Schmidhuber', '(1997)']), (0.0022095151728359587, ['Pointer', 'sentinel', 'mixture\\n\\nmodels']), (0.0021330703543978877, ['Li.\\n\\nIncorporating', 'copying', 'mechanism', 'in\\n\\nsequence-to-sequence', 'learning']), (0.0021258542865337715, ['Hence,', 'we', 'also', 'use', 'the', 'logical', 'form\\nN', '']), (0.0021066693585143677, ['Seman-\\ntic', 'parsing', 'systems', 'are', 'typically', 'constrained', 'to', 'a', 'single', 'schema', 'and', 'require', 'hand-curated', 'grammars\\nto', 'perform', 'well2']), (0.002104375110993782, ['A', 'new', 'database', 'on', 'the', 'structure', 'and', 'devel-\\n\\nopment', 'of', 'the', 'ﬁnancial', 'sector']), (0.0020901793217819064, ['However,', 'the', 'output', 'space', 'of', 'the', 'softmax', 'in', 'their', 'Seq2Seq\\nmodel', 'is', 'unnecessarily', 'large', 'for', 'this', 'task']), (0.0020793861303611935, ['Mou\\net', 'al']), (0.0020729606675673543, ['Table', '3', 'shows', 'that', 'adding', 'the', 'aggregation\\nclassiﬁer', 'improves', 'the', 'precision,', 'recall,', 'and', 'F1', 'for', 'predicting', 'the', 'COUNT', 'operator']), (0.0020523248112626516, ['Learning', 'to', 'map', 'sentences', 'to', 'logical', 'form:', 'Structured\\nIn', 'Uncertainty', 'in', 'Artiﬁcial', 'Intelligence,\\n\\nclassiﬁcation', 'with', 'probabilistic', 'categorial', 'grammars.\\n2005.\\n\\nLuke', 'S']), (0.002050814473709187, ['This', 'is', 'particularly', 'helpful', 'when\\nthe', 'column', 'name', 'contain', 'many', 'tokens,', 'such', 'as', '“Miles', '(km)”,', 'which', 'has', '4', 'tokens']), (0.0020374161806601026, ['ACM,', '2003.\\n\\nPatti', 'J']), (0.0020117826078661955, ['(2017),', 'which', 'helps', 'Seq2SQL', 'achieve', 'state-of-the-art\\nperformance']), (0.0019986786927664517, ['Journal', 'of', 'Machine', 'Learning\\nResearch,', '15:1929–1958,', '2014.\\n\\nJanez', 'Starc', 'and', 'Dunja', 'Mladenic']), (0.001924616102086944, ['(2017)', 'also', 'translate', 'to', 'SQL,', 'but', 'with', 'a', 'Seq2Seq', 'model', 'that', 'is', 'further\\nimproved', 'with', 'human', 'feedback']), (0.0018873506147626918, ['The', 'pointer', 'is', 'less', 'affected', 'since', 'it', 'selects', 'exclusively', 'from', 'the', 'input.\\n\\nModel\\nAug', 'Ptr', 'Network\\nSeq2SQL\\n\\nIncorporating', 'structure', 'reduces', 'invalid\\nqueries']), (0.0018680760891257215, ['ICLR,', '2017.\\n\\nNitish', 'Srivastava,', 'Geoffrey', 'E']), (0.0018378248887408444, ['The', 'conditions', 'tend', 'to', 'contain', 'rare', 'words', '(e.g']), (0.0018266683137572166, ['We', 'discard', 'paraphrases', 'that', 'do', 'not', 'show', 'enough', 'variation,', 'as', 'measured\\nby', 'the', 'character', 'edit', 'distance', 'from', 'the', 'generated', 'question,', 'as', 'well', 'as', 'those', 'both', 'workers', 'deemed\\nincorrect', 'during', 'veriﬁcation']), (0.0017733800537235332, ['In', 'ICML,', '2017.\\n\\nArvind', 'Neelakantan,', 'Quoc', 'V']), (0.0017616948148887574, ['Hinton,', 'Alex', 'Krizhevsky,', 'Ilya', 'Sutskever,', 'and', 'Ruslan', 'Salakhutdinov.\\nDropout:', 'a', 'simple', 'way', 'to', 'prevent', 'neural', 'networks', 'from', 'overﬁtting']), (0.0017327009202777251, ['For', 'words', 'that', 'have', 'neither', 'word', 'nor', 'character', 'embeddings,', 'we', 'assign', 'the', 'zero', 'vector.\\nAll', 'networks', 'are', 'run', 'for', 'a', 'maximum', 'of', '300', 'epochs', 'with', 'early', 'stopping', 'on', 'dev', 'split', 'execution\\naccuracy']), (0.0017077346848860492, ['Introducing', 'a\\nclassiﬁer', 'for', 'the', 'aggregation', 'also', 'reduces', 'the', 'error', 'rate']), (0.0016832553355149338, ['We', 'take', 'the', 'last', 'encoder', 'state', 'to', 'be', 'ec\\nj,\\n\\nHere,', 'hc\\ncolumn', 'j’s', 'representation.\\nTo', 'construct', 'a', 'representation', 'for', 'the', 'question,', 'we', 'compute', 'another', 'input', 'representation', 'κsel', 'us-\\ning', 'the', 'same', 'architecture', 'as', 'for', 'κagg', '(Equation', '3)', 'but', 'with', 'untied', 'weights']), (0.0016332270820620103, ['“1850”),', 'but', 'the', 'baseline\\nis', 'inclined', 'to', 'produce', 'common', 'words', 'in', 'the', 'training', 'corpus,', 'such', 'as', '“march”', 'and', '“4”', 'for', 'date,', 'or\\n“doug”', 'for', 'name']), (0.0015999341552307797, ['In', 'ICLR,', '2017.\\n\\nEric', 'WT', 'Ngai,', 'Li', 'Xiu,', 'and', 'Dorothy', 'CK', 'Chau']), (0.0015950604802648922, ['Let', 'wg\\nx', 'denote', 'the', 'GloVe', 'embedding', 'and', 'wc\\nx\\nthe', 'character', 'embedding', 'for', 'word', 'x']), (0.0015913516835149243, ['In', 'EMNLP,', '2013.\\n\\nChandra', 'Bhagavatula,', 'Thanapon', 'Noraset,', 'and', 'Doug', 'Downey']), (0.0015855920750360258, ['Intelli-\\n\\ngent', 'Data', 'Analysis,', '21:19–38,', '2017.\\n\\nRichard', 'S', 'Sutton,', 'David', 'A', 'McAllester,', 'Satinder', 'P', 'Singh,', 'and', 'Yishay', 'Mansour']), (0.0015644158316618691, ['Ritchie,', 'and', 'P']), (0.0015330094875566171, ['The', 'World', 'Bank', 'Economic', 'Review,', '14(3):597–605,', '2000.\\n\\nIrwan', 'Bello,', 'Hieu', 'Pham,', 'Quoc', 'V']), (0.0015109231396223484, ['Manning,', 'Mihai', 'Surdeanu,', 'John', 'Bauer,', 'Jenny', 'Finkel,', 'Steven', 'J']), (0.001498196989206455, ['We', 'describe', 'this', 'baseline', 'in\\ndetail', 'in', 'Section', '2', 'of', 'the', 'Appendix']), (0.00148240950586844, ['This', 'is', 'evident', 'in', 'the', 'dis-\\ntributions', 'of', 'the', 'number', 'of', 'columns,', 'the', 'lengths', 'of', 'questions,', 'and', 'the', 'length', 'of', 'queries,', 'respectively\\nshown', 'in', 'Figure', '5']), (0.0014811198101437712, ['In\\n\\nACL,', '2015.\\n\\nJeffrey', 'Pennington,', 'Richard', 'Socher,', 'and', 'Christopher', 'D']), (0.0013026714192102716, ['arXiv,', 'cs.CL', '1611.01587,', '2016.\\n\\nRichard', 'Hillestad,', 'James', 'Bigelow,', 'Anthony', 'Bower,', 'Federico', 'Girosi,', 'Robin', 'Meili,', 'Richard', 'Scoville,\\nand', 'Roger', 'Taylor']), (0.0012460974995105185, ['We', 'make', 'available', 'examples', 'of', 'the', 'interface', 'used', 'during', 'the', 'paraphrase', 'phase', 'and\\nduring', 'the', 'veriﬁcation', 'phase', 'in', 'the', 'supplementary', 'materials']), (0.0012402987984207615, ['(2015).\\n\\n∇Lwhe\\n\\nΘ\\n\\n(cid:0)Ey∼py', '[R', '(q', '(y)', ',', 'qg)](cid:1)\\n(cid:34)\\n(cid:88)\\n\\nR', '(q', '(y)', ',', 'qg)∇Θ\\n\\n=', '−∇Θ\\n=', '−Ey∼py\\n≈', '−R', '(q', '(y)', ',', 'qg)∇Θ\\n\\n(cid:88)\\n\\nt\\n\\n(log', 'py', '(yt;', 'Θ))\\n\\nt\\n\\n(cid:35)\\n\\n(log', 'py', '(yt;', 'Θ))\\n\\n(8)\\n\\n(9)\\n\\n(10)\\n\\nHere,', 'py(yt)', 'denotes', 'the', 'probability', 'of', 'choosing', 'token', 'yt', 'during', 'time', 'step', 't']), (0.0012058627070639396, ['Both', 'metrics', 'are', 'deﬁned', 'in', 'Section', '3.1']), (0.001169514573557362, ['Here,', 'wc\\nx', 'is', 'the', 'mean', 'of', 'the', 'embeddings', 'of', 'all', 'the', 'character', 'n-\\ngrams', 'in', 'x']), (0.001113636287547825, ['Can', 'electronic', 'medical', 'record', 'systems', 'transform', 'health', 'care?', 'potential', 'health\\nbeneﬁts,', 'savings,', 'and', 'costs']), (0.0010482340284388703, ['In', 'EMNLP,', '2014.\\n\\nAna-Maria', 'Popescu,', 'Oren', 'Etzioni,', 'and', 'Henry', 'Kautz']), (0.0010181721669032789, ['Health', 'affairs,', '24(5):1103–1117,', '2005.\\n\\nSepp', 'Hochreiter', 'and', 'Jurgen', 'Schmidhuber']), (0.0009562672913831169, ['ATIS', 'is\\npresented', 'as', 'a', 'slot', 'ﬁlling', 'task']), (0.0009256461431998602, ['Application', 'of', 'data', 'mining', 'techniques', 'in', 'customer\\nrelationship', 'management:', 'A', 'literature', 'review', 'and', 'classiﬁcation']), (0.0008795613746006691, ['Gradient', 'estimation', 'using\\n\\nstochastic', 'computation', 'graphs']), (0.0008355706695349921, ['1995.\\n\\nEMNLP,', '2011.\\n\\nYoav', 'Artzi', 'and', 'Luke', 'S']), (0.0008314934346346988, ['1990.\\nSiva', 'Reddy,', 'Mirella', 'Lapata,', 'and', 'Mark', 'Steedman']), (0.0008268331731125467, ['Methods', 'for', 'exploring', 'and', 'mining\\n\\ntables', 'on', 'wikipedia']), (0.0007641072542258539, ['In', 'NIPS,', '2015.\\n\\nMin', 'Joon', 'Seo,', 'Aniruddha', 'Kembhavi,', 'Ali', 'Farhadi,', 'and', 'Hannaneh', 'Hajishirzi']), (0.0006699911301913848, ['2,', '1996.\\n\\nLuke', 'S']), (0.0006659251993314041, ['Long', 'short-term', 'memory']), (0.0006594553307324054, ['In', 'Association', 'for', 'Com-\\nputational', 'Linguistics', '(ACL)', 'System', 'Demonstrations,', 'pp']), (0.000598506906337315, ['Glove:', 'Global', 'vectors', 'for', 'word\\n\\nrepresentation']), (0.0005918033482206586, ['All', 'recurrent', 'layers', 'have', 'a', 'hidden', 'size', 'of', '200', 'units', 'and', 'are', 'followed', 'by', 'a', 'dropout', 'of\\n0.3']), (0.0005915464323326957, ['In', 'Proceedings', 'of', 'the', '8th', 'International', 'Conference', 'on', 'Intelligent', 'User', 'Interfaces,\\npp']), (0.0005628995563398885, ['1057–1063,', '2000.\\n\\nLappoon', 'R']), (0.0005320092214931727, ['Thanisch']), (0.0005320092214931727, ['Price']), (0.0005320092214931727, ['Manning']), (0.0005320092214931727, ['Kingma', 'and', 'Jimmy', 'Ba']), (0.0005320092214931727, ['K']), (0.0005320092214931727, ['Jordan,', 'and', 'Dan', 'Klein']), (0.0005320092214931727, ['In', 'IDEA@KDD,', '2013.\\n\\nQingqing', 'Cai', 'and', 'Alexander', 'Yates']), (0.0005320092214931727, ['In', 'ECML,', '2001.\\n\\nOriol', 'Vinyals,', 'Meire', 'Fortunato,', 'and', 'Navdeep', 'Jaitly']), (0.0005320092214931727, ['In', 'AAAI/IAAI,', 'Vol']), (0.0005320092214931727, ['Computational', 'Linguistics,', '39:389–446,', '2011.\\n\\nChristopher', 'D']), (0.0005320092214931727, ['Bethard,', 'and', 'David\\nMcClosky']), (0.0005320092214931727, ['Androutsopoulos,', 'G.D']), (0.0005320092214931727, ['149–157'])]\n",
      "Summarized Text: \n",
      " By leveraging the inherent structure of\n",
      "SQL queries and applying policy gradient methods using reward signals from live query execution,\n",
      "Seq2SQL achieves state-of-the-art performance on WikiSQL, obtaining 59.4% execution accuracy.\n",
      "\n",
      "2 MODEL\n",
      "\n",
      "The WikiSQL task is to generate a SQL query from a natural language question and table schema.\n",
      "Our baseline model is the attentional sequence to sequence neural semantic parser proposed by Dong\n",
      "& Lapata (2016) that achieves state-of-the-art performance on a host of semantic parsing datasets\n",
      "without using hand-engineered grammar. With this augmented input sequence, the pointer network\n",
      "can produce the SQL query by selecting exclusively from the input.\n",
      "Suppose we have a list of N table columns and a question such as in Figure 2, and want to produce\n",
      "the corresponding SQL query\n"
     ]
    }
   ],
   "source": [
    "generate_summary( \"seq2sql.pdf\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
